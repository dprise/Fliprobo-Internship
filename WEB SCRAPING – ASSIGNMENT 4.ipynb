{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries for web scraping:-\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    "You need to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) Name\n",
    "\n",
    "C) Artist\n",
    "\n",
    "D) Upload date\n",
    "\n",
    "E) Views\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Rank_list=[]\n",
    "Name_list=[]\n",
    "Artist_list=[]\n",
    "Upload_date=[]\n",
    "Views_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Rank_list\n",
    "rank1=driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[1]\")\n",
    "for i in rank1:\n",
    "    try:\n",
    "        Rank_list.append(i.text.replace(\".\",\"\"))\n",
    "    except:\n",
    "        Rank_list.append(\"-\")\n",
    "Rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Baby Shark Dance\"[22]',\n",
       " '\"Despacito\"[24]',\n",
       " '\"Johny Johny Yes Papa\"[25]',\n",
       " '\"Shape of You\"[26]',\n",
       " '\"See You Again\"[27]',\n",
       " '\"Masha and the Bear – Recipe for Disaster\"[30]',\n",
       " '\"Uptown Funk\"[31]',\n",
       " '\"Bath Song\"[32]',\n",
       " '\"Learning Colors – Colorful Eggs on a Farm\"[33]',\n",
       " '\"Gangnam Style\"[34]',\n",
       " '\"Phonics Song with Two Words\"[36]',\n",
       " '\"Sugar\"[37]',\n",
       " '\"Sorry\"[38]',\n",
       " '\"Dame Tu Cosita\"[39]',\n",
       " '\"Roar\"[40]',\n",
       " '\"Counting Stars\"[41]',\n",
       " '\"Thinking Out Loud\"[42]',\n",
       " '\"Dark Horse\"[43]',\n",
       " '\"Wheels on the Bus\"[44]',\n",
       " '\"Faded\"[45]',\n",
       " '\"Shake It Off\"[46]',\n",
       " '\"Lean On\"[47]',\n",
       " '\"Girls Like You\"[48]',\n",
       " '\"Bailando\"[49]',\n",
       " '\"Let Her Go\"[50]',\n",
       " '\"Mi Gente\"[51]',\n",
       " '\"Perfect\"[52]',\n",
       " '\"Waka Waka (This Time for Africa)\"[53]',\n",
       " '\"Hello\"[54]',\n",
       " '\"Axel F\"[55]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Name_list\n",
    "name1=driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[2]\")\n",
    "for i in name1:\n",
    "    try:\n",
    "        Name_list.append(i.text)\n",
    "    except:\n",
    "        Name_list.append(\"-\")\n",
    "Name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Pinkfong Kids' Songs & Stories\",\n",
       " 'Luis Fonsi',\n",
       " 'LooLoo Kids',\n",
       " 'Ed Sheeran',\n",
       " 'Wiz Khalifa',\n",
       " 'Get Movies',\n",
       " 'Mark Ronson',\n",
       " 'Cocomelon – Nursery Rhymes',\n",
       " 'Miroshka TV',\n",
       " 'Psy',\n",
       " 'ChuChu TV',\n",
       " 'Maroon 5',\n",
       " 'Justin Bieber',\n",
       " 'El Chombo',\n",
       " 'Katy Perry',\n",
       " 'OneRepublic',\n",
       " 'Ed Sheeran',\n",
       " 'Katy Perry',\n",
       " 'Cocomelon – Nursery Rhymes',\n",
       " 'Alan Walker',\n",
       " 'Taylor Swift',\n",
       " 'Major Lazer',\n",
       " 'Maroon 5',\n",
       " 'Enrique Iglesias',\n",
       " 'Passenger',\n",
       " 'J Balvin',\n",
       " 'Ed Sheeran',\n",
       " 'Shakira',\n",
       " 'Adele',\n",
       " 'Crazy Frog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Artist_list\n",
    "art1=driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[3]\")\n",
    "for i in art1:\n",
    "    try:\n",
    "        Artist_list.append(i.text)\n",
    "    except:\n",
    "        Artist_list.append(\"-\")\n",
    "Artist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['June 17, 2016',\n",
       " 'January 12, 2017',\n",
       " 'October 8, 2016',\n",
       " 'January 30, 2017',\n",
       " 'April 6, 2015',\n",
       " 'January 31, 2012',\n",
       " 'November 19, 2014',\n",
       " 'May 2, 2018',\n",
       " 'February 27, 2018',\n",
       " 'July 15, 2012',\n",
       " 'March 6, 2014',\n",
       " 'January 14, 2015',\n",
       " 'October 22, 2015',\n",
       " 'April 5, 2018',\n",
       " 'September 5, 2013',\n",
       " 'May 31, 2013',\n",
       " 'October 7, 2014',\n",
       " 'February 20, 2014',\n",
       " 'May 24, 2018',\n",
       " 'December 3, 2015',\n",
       " 'August 18, 2014',\n",
       " 'March 22, 2015',\n",
       " 'May 31, 2018',\n",
       " 'April 11, 2014',\n",
       " 'July 25, 2012',\n",
       " 'June 29, 2017',\n",
       " 'November 9, 2017',\n",
       " 'June 4, 2010',\n",
       " 'October 22, 2015',\n",
       " 'June 16, 2009']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Upload_date\n",
    "upd1=driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[5]\")\n",
    "for i in upd1:\n",
    "    try:\n",
    "        Upload_date.append(i.text)\n",
    "    except:\n",
    "        Upload_date.append(\"-\")\n",
    "Upload_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8.81',\n",
       " '7.40',\n",
       " '5.47',\n",
       " '5.35',\n",
       " '5.14',\n",
       " '4.44',\n",
       " '4.20',\n",
       " '4.16',\n",
       " '4.16',\n",
       " '4.09',\n",
       " '3.93',\n",
       " '3.49',\n",
       " '3.44',\n",
       " '3.39',\n",
       " '3.37',\n",
       " '3.32',\n",
       " '3.27',\n",
       " '3.09',\n",
       " '3.08',\n",
       " '3.08',\n",
       " '3.07',\n",
       " '3.05',\n",
       " '3.05',\n",
       " '3.05',\n",
       " '3.01',\n",
       " '2.93',\n",
       " '2.87',\n",
       " '2.85',\n",
       " '2.85',\n",
       " '2.84']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Views_list\n",
    "v1=driver.find_elements_by_xpath(\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[4]\")\n",
    "for i in v1:\n",
    "    try:\n",
    "        Views_list.append(i.text)\n",
    "    except:\n",
    "        Views_list.append(\"-\")\n",
    "Views_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Baby Shark Dance\"[22]</td>\n",
       "      <td>Pinkfong Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>8.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Despacito\"[24]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[25]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>5.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Shape of You\"[26]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"See You Again\"[27]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[30]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>\"Uptown Funk\"[31]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>\"Bath Song\"[32]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[33]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>\"Gangnam Style\"[34]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>\"Phonics Song with Two Words\"[36]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>\"Sugar\"[37]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>\"Sorry\"[38]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>\"Dame Tu Cosita\"[39]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>\"Roar\"[40]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>\"Counting Stars\"[41]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>\"Thinking Out Loud\"[42]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>\"Dark Horse\"[43]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>\"Wheels on the Bus\"[44]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>\"Faded\"[45]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Shake It Off\"[46]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>August 18, 2014</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>\"Lean On\"[47]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>\"Girls Like You\"[48]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>\"Bailando\"[49]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>\"Let Her Go\"[50]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>\"Mi Gente\"[51]</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>June 29, 2017</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>\"Perfect\"[52]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[53]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>\"Hello\"[54]</td>\n",
       "      <td>Adele</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>\"Axel F\"[55]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                       Video Name  \\\n",
       "0     1                           \"Baby Shark Dance\"[22]   \n",
       "1     2                                  \"Despacito\"[24]   \n",
       "2     3                       \"Johny Johny Yes Papa\"[25]   \n",
       "3     4                               \"Shape of You\"[26]   \n",
       "4     5                              \"See You Again\"[27]   \n",
       "5     6   \"Masha and the Bear – Recipe for Disaster\"[30]   \n",
       "6     7                                \"Uptown Funk\"[31]   \n",
       "7     8                                  \"Bath Song\"[32]   \n",
       "8     9  \"Learning Colors – Colorful Eggs on a Farm\"[33]   \n",
       "9    10                              \"Gangnam Style\"[34]   \n",
       "10   11                \"Phonics Song with Two Words\"[36]   \n",
       "11   12                                      \"Sugar\"[37]   \n",
       "12   13                                      \"Sorry\"[38]   \n",
       "13   14                             \"Dame Tu Cosita\"[39]   \n",
       "14   15                                       \"Roar\"[40]   \n",
       "15   16                             \"Counting Stars\"[41]   \n",
       "16   17                          \"Thinking Out Loud\"[42]   \n",
       "17   18                                 \"Dark Horse\"[43]   \n",
       "18   19                          \"Wheels on the Bus\"[44]   \n",
       "19   20                                      \"Faded\"[45]   \n",
       "20   21                               \"Shake It Off\"[46]   \n",
       "21   22                                    \"Lean On\"[47]   \n",
       "22   23                             \"Girls Like You\"[48]   \n",
       "23   24                                   \"Bailando\"[49]   \n",
       "24   25                                 \"Let Her Go\"[50]   \n",
       "25   26                                   \"Mi Gente\"[51]   \n",
       "26   27                                    \"Perfect\"[52]   \n",
       "27   28           \"Waka Waka (This Time for Africa)\"[53]   \n",
       "28   29                                      \"Hello\"[54]   \n",
       "29   30                                     \"Axel F\"[55]   \n",
       "\n",
       "                            Artist        Upload Date Views (Billions)  \n",
       "0   Pinkfong Kids' Songs & Stories      June 17, 2016             8.81  \n",
       "1                       Luis Fonsi   January 12, 2017             7.40  \n",
       "2                      LooLoo Kids    October 8, 2016             5.47  \n",
       "3                       Ed Sheeran   January 30, 2017             5.35  \n",
       "4                      Wiz Khalifa      April 6, 2015             5.14  \n",
       "5                       Get Movies   January 31, 2012             4.44  \n",
       "6                      Mark Ronson  November 19, 2014             4.20  \n",
       "7       Cocomelon – Nursery Rhymes        May 2, 2018             4.16  \n",
       "8                      Miroshka TV  February 27, 2018             4.16  \n",
       "9                              Psy      July 15, 2012             4.09  \n",
       "10                       ChuChu TV      March 6, 2014             3.93  \n",
       "11                        Maroon 5   January 14, 2015             3.49  \n",
       "12                   Justin Bieber   October 22, 2015             3.44  \n",
       "13                       El Chombo      April 5, 2018             3.39  \n",
       "14                      Katy Perry  September 5, 2013             3.37  \n",
       "15                     OneRepublic       May 31, 2013             3.32  \n",
       "16                      Ed Sheeran    October 7, 2014             3.27  \n",
       "17                      Katy Perry  February 20, 2014             3.09  \n",
       "18      Cocomelon – Nursery Rhymes       May 24, 2018             3.08  \n",
       "19                     Alan Walker   December 3, 2015             3.08  \n",
       "20                    Taylor Swift    August 18, 2014             3.07  \n",
       "21                     Major Lazer     March 22, 2015             3.05  \n",
       "22                        Maroon 5       May 31, 2018             3.05  \n",
       "23                Enrique Iglesias     April 11, 2014             3.05  \n",
       "24                       Passenger      July 25, 2012             3.01  \n",
       "25                        J Balvin      June 29, 2017             2.93  \n",
       "26                      Ed Sheeran   November 9, 2017             2.87  \n",
       "27                         Shakira       June 4, 2010             2.85  \n",
       "28                           Adele   October 22, 2015             2.85  \n",
       "29                      Crazy Frog      June 16, 2009             2.84  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "You_tube=pd.DataFrame({})\n",
    "You_tube['Rank']=Rank_list[0:30]\n",
    "You_tube['Video Name']=Name_list[0:30]\n",
    "You_tube['Artist']=Artist_list[0:30]\n",
    "You_tube['Upload Date']=Upload_date[0:30]\n",
    "You_tube['Views (Billions)']=Views_list[0:30]\n",
    "You_tube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************************Completed***********************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "\n",
    "A) Match title (I.e. 1st ODI)\n",
    "\n",
    "B) Series\n",
    "\n",
    "C) Place\n",
    "\n",
    "D) Date\n",
    "\n",
    "E) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code.\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening \"https://www.bcci.tv/\"\n",
    "url=\"https://www.bcci.tv/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"ad1021bae87555021968566197c07d1c\", element=\"a3236373-1202-4272-8d36-0911f16b96df\")>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for International tab\n",
    "search_int=driver.find_element_by_xpath('//div[@class=\"navigation__drop-down drop-down drop-down--reveal-on-hover\"]')\n",
    "search_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking International tab\n",
    "search_int.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"ad1021bae87555021968566197c07d1c\", element=\"60a84a66-b28b-4dfb-a286-e7977581ccc8\")>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for Fixture Link\n",
    "search_fix=driver.find_element_by_xpath('//a[@class=\"navigation__link navigation__link--in-drop-down\"]')\n",
    "search_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking Fixture Link\n",
    "search_fix.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Match_title=[]\n",
    "Series_info=[]\n",
    "Place_det=[]\n",
    "Date_info=[]\n",
    "Time_info=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Final',\n",
       " '1st ODI',\n",
       " '2nd ODI',\n",
       " '3rd ODI',\n",
       " '1st T20I',\n",
       " '2nd T20I',\n",
       " '3rd T20I',\n",
       " '1st Test',\n",
       " '2nd Test',\n",
       " '3rd Test',\n",
       " '4th Test',\n",
       " '5th Test']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Match_title\n",
    "mtitle=driver.find_elements_by_xpath('//strong[@class=\"fixture__name fixture__name--with-margin\"]')\n",
    "for i in mtitle:\n",
    "    try:\n",
    "        Match_title.append(i.text)\n",
    "    except:\n",
    "        Match_title.append(\"-\")\n",
    "Match_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICC WORLD TEST CHAMPIONSHIP',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'SRI LANKA V INDIA 2021',\n",
       " 'ENGLAND V INDIA 2021',\n",
       " 'ENGLAND V INDIA 2021',\n",
       " 'ENGLAND V INDIA 2021',\n",
       " 'ENGLAND V INDIA 2021',\n",
       " 'ENGLAND V INDIA 2021']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Series\n",
    "series1=driver.find_elements_by_xpath('//span[@class=\"u-unskewed-text fixture__tournament-label u-truncated\"]')\n",
    "for i in series1:\n",
    "    try:\n",
    "        Series_info.append(i.text)\n",
    "    except:\n",
    "        Series_info.append(\"-\")\n",
    "Series_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Ageas Bowl, Southampton',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'Trent Bridge, Nottingham',\n",
       " \"Lord's, London\",\n",
       " 'Headingley, Leeds',\n",
       " 'The Oval, London',\n",
       " 'Old Trafford, Manchester']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Place\n",
    "p1=driver.find_elements_by_xpath('//p[@class=\"fixture__additional-info\"]/span')\n",
    "for i in p1:\n",
    "    try:\n",
    "        Place_det.append(i.text)\n",
    "    except:\n",
    "        Place_det.append(\"-\")\n",
    "Place_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18 JUNE',\n",
       " '13 JULY',\n",
       " '16 JULY',\n",
       " '18 JULY',\n",
       " '21 JULY',\n",
       " '23 JULY',\n",
       " '25 JULY',\n",
       " '04 AUGUST',\n",
       " '12 AUGUST',\n",
       " '25 AUGUST',\n",
       " '02 SEPTEMBER',\n",
       " '10 SEPTEMBER']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Date\n",
    "date1=driver.find_elements_by_xpath(\"//div[@class='fixture__datetime desktop-only']/div/span\")\n",
    "month=driver.find_elements_by_xpath(\"//div[@class='fixture__datetime desktop-only']/div/div/span[1]\")\n",
    "j=len(date1)\n",
    "for i in range(j):\n",
    "    try:\n",
    "        Date_info.append(date1[i].text+\" \"+month[i].text)\n",
    "    except:\n",
    "        Date_info.append(\"-\")\n",
    "Date_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15:00 IST',\n",
       " '14:30 IST',\n",
       " '14:30 IST',\n",
       " '14:30 IST',\n",
       " '19:00 IST',\n",
       " '19:00 IST',\n",
       " '19:00 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Time\n",
    "t1=driver.find_elements_by_xpath(\"//div[@class='fixture__datetime desktop-only']/div/div/span[2]\")\n",
    "for i in t1:\n",
    "    try:\n",
    "        Time_info.append(i.text)\n",
    "    except:\n",
    "        Time_info.append(\"-\")\n",
    "Time_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final</td>\n",
       "      <td>ICC WORLD TEST CHAMPIONSHIP</td>\n",
       "      <td>The Ageas Bowl, Southampton</td>\n",
       "      <td>18 JUNE</td>\n",
       "      <td>15:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>13 JULY</td>\n",
       "      <td>14:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>16 JULY</td>\n",
       "      <td>14:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>18 JULY</td>\n",
       "      <td>14:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>21 JULY</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>23 JULY</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>SRI LANKA V INDIA 2021</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>25 JULY</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>ENGLAND V INDIA 2021</td>\n",
       "      <td>Trent Bridge, Nottingham</td>\n",
       "      <td>04 AUGUST</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>ENGLAND V INDIA 2021</td>\n",
       "      <td>Lord's, London</td>\n",
       "      <td>12 AUGUST</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3rd Test</td>\n",
       "      <td>ENGLAND V INDIA 2021</td>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>25 AUGUST</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4th Test</td>\n",
       "      <td>ENGLAND V INDIA 2021</td>\n",
       "      <td>The Oval, London</td>\n",
       "      <td>02 SEPTEMBER</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5th Test</td>\n",
       "      <td>ENGLAND V INDIA 2021</td>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>10 SEPTEMBER</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match title                       Series                         Place  \\\n",
       "0        Final  ICC WORLD TEST CHAMPIONSHIP   The Ageas Bowl, Southampton   \n",
       "1      1st ODI       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "2      2nd ODI       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "3      3rd ODI       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "4     1st T20I       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "5     2nd T20I       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "6     3rd T20I       SRI LANKA V INDIA 2021  R Premadasa Stadium, Colombo   \n",
       "7     1st Test         ENGLAND V INDIA 2021      Trent Bridge, Nottingham   \n",
       "8     2nd Test         ENGLAND V INDIA 2021                Lord's, London   \n",
       "9     3rd Test         ENGLAND V INDIA 2021             Headingley, Leeds   \n",
       "10    4th Test         ENGLAND V INDIA 2021              The Oval, London   \n",
       "11    5th Test         ENGLAND V INDIA 2021      Old Trafford, Manchester   \n",
       "\n",
       "            Date       Time  \n",
       "0        18 JUNE  15:00 IST  \n",
       "1        13 JULY  14:30 IST  \n",
       "2        16 JULY  14:30 IST  \n",
       "3        18 JULY  14:30 IST  \n",
       "4        21 JULY  19:00 IST  \n",
       "5        23 JULY  19:00 IST  \n",
       "6        25 JULY  19:00 IST  \n",
       "7      04 AUGUST  15:30 IST  \n",
       "8      12 AUGUST  15:30 IST  \n",
       "9      25 AUGUST  15:30 IST  \n",
       "10  02 SEPTEMBER  15:30 IST  \n",
       "11  10 SEPTEMBER  15:30 IST  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "bcci=pd.DataFrame({})\n",
    "bcci['Match title']=Match_title\n",
    "bcci['Series']=Series_info\n",
    "bcci['Place']=Place_det\n",
    "bcci['Date']=Date_info\n",
    "bcci['Time']=Time_info\n",
    "bcci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************************************************************************************************************************************Completed****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/.\n",
    "You need to find following details:\n",
    "    \n",
    "A) Name\n",
    "\n",
    "B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code.\n",
    "    \n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening \"https://www.guru99.com/\"\n",
    "url=\"https://www.guru99.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"eff774a82051cea5701fc7da64dc943a\", element=\"4e13afd5-7165-4a14-88fe-bece0d203ce6\")>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for Selenium\n",
    "selenium1=driver.find_element_by_xpath(\"//div[@class='srch']/span[8]/a\")\n",
    "selenium1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking selenium button\n",
    "selenium1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"eff774a82051cea5701fc7da64dc943a\", element=\"9eb785e4-13ef-469d-a536-42e83e131056\")>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for Selenium exception handling Tutorial\n",
    "selenium2=driver.find_element_by_xpath(\"//table[@class='table']/tbody/tr[34]/td/a\")\n",
    "selenium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking selenium exception handling Tutorial link\n",
    "selenium2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Name_list=[]\n",
    "Description_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exception name',\n",
       " 'ElementNotVisibleException',\n",
       " 'ElementNotSelectableException',\n",
       " 'NoSuchElementException',\n",
       " 'NoSuchFrameException',\n",
       " 'NoAlertPresentException',\n",
       " 'NoSuchWindowException',\n",
       " 'StaleElementReferenceException',\n",
       " 'SessionNotFoundException',\n",
       " 'TimeoutException',\n",
       " 'WebDriverException',\n",
       " 'ConnectionClosedException',\n",
       " 'ElementClickInterceptedException',\n",
       " 'ElementNotInteractableException',\n",
       " 'ErrorInResponseException',\n",
       " 'ErrorHandler.UnknownServerException',\n",
       " 'ImeActivationFailedException',\n",
       " 'ImeNotAvailableException',\n",
       " 'InsecureCertificateException',\n",
       " 'InvalidArgumentException',\n",
       " 'InvalidCookieDomainException',\n",
       " 'InvalidCoordinatesException',\n",
       " 'InvalidElementStateExceptio',\n",
       " 'InvalidSessionIdException',\n",
       " 'InvalidSwitchToTargetException',\n",
       " 'JavascriptException',\n",
       " 'JsonException',\n",
       " 'NoSuchAttributeException',\n",
       " 'MoveTargetOutOfBoundsException',\n",
       " 'NoSuchContextException',\n",
       " 'NoSuchCookieException',\n",
       " 'NotFoundException',\n",
       " 'RemoteDriverServerException',\n",
       " 'ScreenshotException',\n",
       " 'SessionNotCreatedException',\n",
       " 'UnableToSetCookieException',\n",
       " 'UnexpectedTagNameException',\n",
       " 'UnhandledAlertException',\n",
       " 'UnexpectedAlertPresentException',\n",
       " 'UnknownMethodException',\n",
       " 'UnreachableBrowserException',\n",
       " 'UnsupportedCommandException']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Name_list\n",
    "name1=driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[1]\")\n",
    "for i in name1:\n",
    "    try:\n",
    "        Name_list.append(i.text)\n",
    "    except:\n",
    "        Name_list.append(\"-\")\n",
    "Name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Description',\n",
       " 'This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.',\n",
       " 'This Selenium exception occurs when an element is presented in the DOM, but you can be able to select. Therefore, it is not possible to interact.',\n",
       " 'This Exception occurs if an element could not be found.',\n",
       " 'This Exception occurs if the frame target to be switched to does not exist.',\n",
       " 'This Exception occurs when you switch to no presented alert.',\n",
       " 'This Exception occurs if the window target to be switch does not exist.',\n",
       " 'This Selenium exception occurs happens when the web element is detached from the current DOM.',\n",
       " 'The WebDriver is acting after you quit the browser.',\n",
       " \"Thrown when there is not enough time for a command to be completed. For Example, the element searched wasn't found in the specified time.\",\n",
       " 'This Exception takes place when the WebDriver is acting right after you close the browser.',\n",
       " 'This type of Exception takes place when there is a disconnection in the driver.',\n",
       " 'The command may not be completed as the element receiving the events is concealing the element which was requested clicked.',\n",
       " 'This Selenium exception is thrown when any element is presented in the DOM. However, it is impossible to interact with such an element.',\n",
       " 'This happens while interacting with the Firefox extension or the remote driver server.',\n",
       " 'Exception is used as a placeholder in case if the server returns an error without a stack trace.',\n",
       " 'This expectation will occur when IME engine activation has failed.',\n",
       " 'It takes place when IME support is unavailable.',\n",
       " 'Navigation made the user agent to hit a certificate warning. This can cause by an invalid or expired TLS certificate.',\n",
       " 'It occurs when an argument does not belong to the expected type.',\n",
       " 'This happens when you try to add a cookie under a different domain instead of current URL.',\n",
       " 'This type of Exception matches an interacting operation that is not valid.',\n",
       " \"It occurs when command can't be finished when the element is invalid.\",\n",
       " 'This Exception took place when the given session ID is not included in the list of active sessions. It means the session does not exist or is inactive either.',\n",
       " 'This occurs when the frame or window target to be switched does not exist.',\n",
       " 'This issue occurs while executing JavaScript given by the user.',\n",
       " 'It occurs when you afford to get the session when the session is not created.',\n",
       " 'This kind of Exception occurs when the attribute of an element could not be found.',\n",
       " 'It takes place if the target provided to the ActionChains move() methodology is not valid. For Example, out of the document.',\n",
       " 'ContextAware does mobile device testing.',\n",
       " 'This Exception occurs when no cookie matching with the given pathname found for all the associated cookies of the currently browsing document.',\n",
       " 'This Exception is a subclass of WebDriverException. This will occur when an element on the DOM does not exist.',\n",
       " 'This Selenium exception is thrown when the server is not responding because of the problem that the capabilities described are not proper.',\n",
       " 'It is not possible to capture a screen.',\n",
       " 'It happens when a new session could not be successfully created.',\n",
       " 'This occurs if a driver is unable to set a cookie.',\n",
       " 'Happens if a support class did not get a web element as expected.',\n",
       " 'This expectation occurs when there is an alert, but WebDriver is not able to perform Alert operation.',\n",
       " 'It occurs when there is the appearance of an unexpected alert.',\n",
       " 'This Exception happens when the requested command matches with a known URL but and not matching with a methodology for a specific URL.',\n",
       " 'This Exception occurs only when the browser is not able to be opened or crashed because of some reason.',\n",
       " \"This occurs when remote WebDriver does n't send valid commands as expected.\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Description_list\n",
    "desc=driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[2]\")\n",
    "for i in desc:\n",
    "    try:\n",
    "        Description_list.append(i.text)\n",
    "    except:\n",
    "        Description_list.append(\"-\")\n",
    "Description_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exception name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElementNotVisibleException</td>\n",
       "      <td>This type of Selenium exception occurs when an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElementNotSelectableException</td>\n",
       "      <td>This Selenium exception occurs when an element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NoSuchElementException</td>\n",
       "      <td>This Exception occurs if an element could not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NoSuchFrameException</td>\n",
       "      <td>This Exception occurs if the frame target to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoAlertPresentException</td>\n",
       "      <td>This Exception occurs when you switch to no pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NoSuchWindowException</td>\n",
       "      <td>This Exception occurs if the window target to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>StaleElementReferenceException</td>\n",
       "      <td>This Selenium exception occurs happens when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SessionNotFoundException</td>\n",
       "      <td>The WebDriver is acting after you quit the bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TimeoutException</td>\n",
       "      <td>Thrown when there is not enough time for a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WebDriverException</td>\n",
       "      <td>This Exception takes place when the WebDriver ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ConnectionClosedException</td>\n",
       "      <td>This type of Exception takes place when there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ElementClickInterceptedException</td>\n",
       "      <td>The command may not be completed as the elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ElementNotInteractableException</td>\n",
       "      <td>This Selenium exception is thrown when any ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ErrorInResponseException</td>\n",
       "      <td>This happens while interacting with the Firefo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ErrorHandler.UnknownServerException</td>\n",
       "      <td>Exception is used as a placeholder in case if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ImeActivationFailedException</td>\n",
       "      <td>This expectation will occur when IME engine ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ImeNotAvailableException</td>\n",
       "      <td>It takes place when IME support is unavailable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>InsecureCertificateException</td>\n",
       "      <td>Navigation made the user agent to hit a certif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>InvalidArgumentException</td>\n",
       "      <td>It occurs when an argument does not belong to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>InvalidCookieDomainException</td>\n",
       "      <td>This happens when you try to add a cookie unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>InvalidCoordinatesException</td>\n",
       "      <td>This type of Exception matches an interacting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>InvalidElementStateExceptio</td>\n",
       "      <td>It occurs when command can't be finished when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>InvalidSessionIdException</td>\n",
       "      <td>This Exception took place when the given sessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>InvalidSwitchToTargetException</td>\n",
       "      <td>This occurs when the frame or window target to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JavascriptException</td>\n",
       "      <td>This issue occurs while executing JavaScript g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JsonException</td>\n",
       "      <td>It occurs when you afford to get the session w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NoSuchAttributeException</td>\n",
       "      <td>This kind of Exception occurs when the attribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MoveTargetOutOfBoundsException</td>\n",
       "      <td>It takes place if the target provided to the A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NoSuchContextException</td>\n",
       "      <td>ContextAware does mobile device testing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NoSuchCookieException</td>\n",
       "      <td>This Exception occurs when no cookie matching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NotFoundException</td>\n",
       "      <td>This Exception is a subclass of WebDriverExcep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RemoteDriverServerException</td>\n",
       "      <td>This Selenium exception is thrown when the ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ScreenshotException</td>\n",
       "      <td>It is not possible to capture a screen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SessionNotCreatedException</td>\n",
       "      <td>It happens when a new session could not be suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>UnableToSetCookieException</td>\n",
       "      <td>This occurs if a driver is unable to set a coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>UnexpectedTagNameException</td>\n",
       "      <td>Happens if a support class did not get a web e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>UnhandledAlertException</td>\n",
       "      <td>This expectation occurs when there is an alert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>UnexpectedAlertPresentException</td>\n",
       "      <td>It occurs when there is the appearance of an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>UnknownMethodException</td>\n",
       "      <td>This Exception happens when the requested comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>UnreachableBrowserException</td>\n",
       "      <td>This Exception occurs only when the browser is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>UnsupportedCommandException</td>\n",
       "      <td>This occurs when remote WebDriver does n't sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Exception name  \\\n",
       "0            ElementNotVisibleException   \n",
       "1         ElementNotSelectableException   \n",
       "2                NoSuchElementException   \n",
       "3                  NoSuchFrameException   \n",
       "4               NoAlertPresentException   \n",
       "5                 NoSuchWindowException   \n",
       "6        StaleElementReferenceException   \n",
       "7              SessionNotFoundException   \n",
       "8                      TimeoutException   \n",
       "9                    WebDriverException   \n",
       "10            ConnectionClosedException   \n",
       "11     ElementClickInterceptedException   \n",
       "12      ElementNotInteractableException   \n",
       "13             ErrorInResponseException   \n",
       "14  ErrorHandler.UnknownServerException   \n",
       "15         ImeActivationFailedException   \n",
       "16             ImeNotAvailableException   \n",
       "17         InsecureCertificateException   \n",
       "18             InvalidArgumentException   \n",
       "19         InvalidCookieDomainException   \n",
       "20          InvalidCoordinatesException   \n",
       "21          InvalidElementStateExceptio   \n",
       "22            InvalidSessionIdException   \n",
       "23       InvalidSwitchToTargetException   \n",
       "24                  JavascriptException   \n",
       "25                        JsonException   \n",
       "26             NoSuchAttributeException   \n",
       "27       MoveTargetOutOfBoundsException   \n",
       "28               NoSuchContextException   \n",
       "29                NoSuchCookieException   \n",
       "30                    NotFoundException   \n",
       "31          RemoteDriverServerException   \n",
       "32                  ScreenshotException   \n",
       "33           SessionNotCreatedException   \n",
       "34           UnableToSetCookieException   \n",
       "35           UnexpectedTagNameException   \n",
       "36              UnhandledAlertException   \n",
       "37      UnexpectedAlertPresentException   \n",
       "38               UnknownMethodException   \n",
       "39          UnreachableBrowserException   \n",
       "40          UnsupportedCommandException   \n",
       "\n",
       "                                          Description  \n",
       "0   This type of Selenium exception occurs when an...  \n",
       "1   This Selenium exception occurs when an element...  \n",
       "2   This Exception occurs if an element could not ...  \n",
       "3   This Exception occurs if the frame target to b...  \n",
       "4   This Exception occurs when you switch to no pr...  \n",
       "5   This Exception occurs if the window target to ...  \n",
       "6   This Selenium exception occurs happens when th...  \n",
       "7   The WebDriver is acting after you quit the bro...  \n",
       "8   Thrown when there is not enough time for a com...  \n",
       "9   This Exception takes place when the WebDriver ...  \n",
       "10  This type of Exception takes place when there ...  \n",
       "11  The command may not be completed as the elemen...  \n",
       "12  This Selenium exception is thrown when any ele...  \n",
       "13  This happens while interacting with the Firefo...  \n",
       "14  Exception is used as a placeholder in case if ...  \n",
       "15  This expectation will occur when IME engine ac...  \n",
       "16    It takes place when IME support is unavailable.  \n",
       "17  Navigation made the user agent to hit a certif...  \n",
       "18  It occurs when an argument does not belong to ...  \n",
       "19  This happens when you try to add a cookie unde...  \n",
       "20  This type of Exception matches an interacting ...  \n",
       "21  It occurs when command can't be finished when ...  \n",
       "22  This Exception took place when the given sessi...  \n",
       "23  This occurs when the frame or window target to...  \n",
       "24  This issue occurs while executing JavaScript g...  \n",
       "25  It occurs when you afford to get the session w...  \n",
       "26  This kind of Exception occurs when the attribu...  \n",
       "27  It takes place if the target provided to the A...  \n",
       "28           ContextAware does mobile device testing.  \n",
       "29  This Exception occurs when no cookie matching ...  \n",
       "30  This Exception is a subclass of WebDriverExcep...  \n",
       "31  This Selenium exception is thrown when the ser...  \n",
       "32            It is not possible to capture a screen.  \n",
       "33  It happens when a new session could not be suc...  \n",
       "34  This occurs if a driver is unable to set a coo...  \n",
       "35  Happens if a support class did not get a web e...  \n",
       "36  This expectation occurs when there is an alert...  \n",
       "37  It occurs when there is the appearance of an u...  \n",
       "38  This Exception happens when the requested comm...  \n",
       "39  This Exception occurs only when the browser is...  \n",
       "40  This occurs when remote WebDriver does n't sen...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "guru99=pd.DataFrame({})\n",
    "guru99['Exception name']=Name_list[1:42]\n",
    "guru99['Description']=Description_list[1:42]\n",
    "guru99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************************************************************************************************************************Completed*************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/. \n",
    "You have to find following details:\n",
    "    \n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP at current price (19-20)\n",
    "\n",
    "D) GSDP at current price (18-19)\n",
    "\n",
    "E) Share(18-19)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage http://statisticstimes.com/\n",
    "url=\"http://statisticstimes.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding & clicking element for Economy tab\n",
    "eco1=driver.find_element_by_xpath(\"//*[@id='top']/div[2]/div[2]/button\")\n",
    "eco1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding & clicking element for India from dropdown\n",
    "eco2=driver.find_element_by_xpath(\"//*[@id='top']/div[2]/div[2]/div/a[3]\")\n",
    "eco2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding & clicking element for Indian State-wise GDP\n",
    "eco3=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "eco3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Rank_list=[]\n",
    "State_list=[]\n",
    "GSDP_20=[]\n",
    "GSDP_19=[]\n",
    "Share_19=[]\n",
    "GDP_Billion=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Name_list\n",
    "r1=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[1]\")\n",
    "for i in r1:\n",
    "    try:\n",
    "        Rank_list.append(i.text)\n",
    "    except:\n",
    "        Rank_list.append(\"-\")\n",
    "Rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maharashtra',\n",
       " 'Tamil Nadu',\n",
       " 'Uttar Pradesh',\n",
       " 'Gujarat',\n",
       " 'Karnataka',\n",
       " 'West Bengal',\n",
       " 'Rajasthan',\n",
       " 'Andhra Pradesh',\n",
       " 'Telangana',\n",
       " 'Madhya Pradesh',\n",
       " 'Kerala',\n",
       " 'Delhi',\n",
       " 'Haryana',\n",
       " 'Bihar',\n",
       " 'Punjab',\n",
       " 'Odisha',\n",
       " 'Assam',\n",
       " 'Chhattisgarh',\n",
       " 'Jharkhand',\n",
       " 'Uttarakhand',\n",
       " 'Jammu & Kashmir',\n",
       " 'Himachal Pradesh',\n",
       " 'Goa',\n",
       " 'Tripura',\n",
       " 'Chandigarh',\n",
       " 'Puducherry',\n",
       " 'Meghalaya',\n",
       " 'Sikkim',\n",
       " 'Manipur',\n",
       " 'Nagaland',\n",
       " 'Arunachal Pradesh',\n",
       " 'Mizoram',\n",
       " 'Andaman & Nicobar Islands']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for State_list\n",
    "s1=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[2]\")\n",
    "for i in s1:\n",
    "    try:\n",
    "        State_list.append(i.text)\n",
    "    except:\n",
    "        State_list.append(\"-\")\n",
    "State_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '1,845,853',\n",
       " '1,687,818',\n",
       " '-',\n",
       " '1,631,977',\n",
       " '1,253,832',\n",
       " '1,020,989',\n",
       " '972,782',\n",
       " '969,604',\n",
       " '906,672',\n",
       " '-',\n",
       " '856,112',\n",
       " '831,610',\n",
       " '611,804',\n",
       " '574,760',\n",
       " '521,275',\n",
       " '-',\n",
       " '329,180',\n",
       " '328,598',\n",
       " '-',\n",
       " '-',\n",
       " '165,472',\n",
       " '80,449',\n",
       " '55,984',\n",
       " '-',\n",
       " '38,253',\n",
       " '36,572',\n",
       " '32,496',\n",
       " '31,790',\n",
       " '-',\n",
       " '-',\n",
       " '26,503',\n",
       " '-']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for GSDP(19-20)\n",
    "gdp1=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[3]\")\n",
    "for i in gdp1:\n",
    "    try:\n",
    "        GSDP_20.append(i.text)\n",
    "    except:\n",
    "        GSDP_20.append(\"-\")\n",
    "GSDP_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,632,792',\n",
       " '1,630,208',\n",
       " '1,584,764',\n",
       " '1,502,899',\n",
       " '1,493,127',\n",
       " '1,089,898',\n",
       " '942,586',\n",
       " '862,957',\n",
       " '861,031',\n",
       " '809,592',\n",
       " '781,653',\n",
       " '774,870',\n",
       " '734,163',\n",
       " '530,363',\n",
       " '526,376',\n",
       " '487,805',\n",
       " '315,881',\n",
       " '304,063',\n",
       " '297,204',\n",
       " '245,895',\n",
       " '155,956',\n",
       " '153,845',\n",
       " '73,170',\n",
       " '49,845',\n",
       " '42,114',\n",
       " '34,433',\n",
       " '33,481',\n",
       " '28,723',\n",
       " '27,870',\n",
       " '27,283',\n",
       " '24,603',\n",
       " '22,287',\n",
       " '-']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for GSDP(18-19)\n",
    "gdp2=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[4]\")\n",
    "for i in gdp2:\n",
    "    try:\n",
    "        GSDP_19.append(i.text)\n",
    "    except:\n",
    "        GSDP_19.append(\"-\")\n",
    "GSDP_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13.94%',\n",
       " '8.63%',\n",
       " '8.39%',\n",
       " '7.96%',\n",
       " '7.91%',\n",
       " '5.77%',\n",
       " '4.99%',\n",
       " '4.57%',\n",
       " '4.56%',\n",
       " '4.29%',\n",
       " '4.14%',\n",
       " '4.10%',\n",
       " '3.89%',\n",
       " '2.81%',\n",
       " '2.79%',\n",
       " '2.58%',\n",
       " '1.67%',\n",
       " '1.61%',\n",
       " '1.57%',\n",
       " '1.30%',\n",
       " '0.83%',\n",
       " '0.81%',\n",
       " '0.39%',\n",
       " '0.26%',\n",
       " '0.22%',\n",
       " '0.18%',\n",
       " '0.18%',\n",
       " '0.15%',\n",
       " '0.15%',\n",
       " '0.14%',\n",
       " '0.13%',\n",
       " '0.12%',\n",
       " '-']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Share(18-19)\n",
    "share1=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[5]\")\n",
    "for i in share1:\n",
    "    try:\n",
    "        Share_19.append(i.text)\n",
    "    except:\n",
    "        Share_19.append(\"-\")\n",
    "Share_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['399.921',\n",
       " '247.629',\n",
       " '240.726',\n",
       " '228.290',\n",
       " '226.806',\n",
       " '165.556',\n",
       " '143.179',\n",
       " '131.083',\n",
       " '130.791',\n",
       " '122.977',\n",
       " '118.733',\n",
       " '117.703',\n",
       " '111.519',\n",
       " '80.562',\n",
       " '79.957',\n",
       " '74.098',\n",
       " '47.982',\n",
       " '46.187',\n",
       " '45.145',\n",
       " '37.351',\n",
       " '23.690',\n",
       " '23.369',\n",
       " '11.115',\n",
       " '7.571',\n",
       " '6.397',\n",
       " '5.230',\n",
       " '5.086',\n",
       " '4.363',\n",
       " '4.233',\n",
       " '4.144',\n",
       " '3.737',\n",
       " '3.385',\n",
       " '-']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for GDP($ billion)\n",
    "gdp3=driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[6]\")\n",
    "for i in gdp3:\n",
    "    try:\n",
    "        GDP_Billion.append(i.text)\n",
    "    except:\n",
    "        GDP_Billion.append(\"-\")\n",
    "GDP_Billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP at current price (19-20)  \\\n",
       "0     1                Maharashtra                             -   \n",
       "1     2                 Tamil Nadu                     1,845,853   \n",
       "2     3              Uttar Pradesh                     1,687,818   \n",
       "3     4                    Gujarat                             -   \n",
       "4     5                  Karnataka                     1,631,977   \n",
       "5     6                West Bengal                     1,253,832   \n",
       "6     7                  Rajasthan                     1,020,989   \n",
       "7     8             Andhra Pradesh                       972,782   \n",
       "8     9                  Telangana                       969,604   \n",
       "9    10             Madhya Pradesh                       906,672   \n",
       "10   11                     Kerala                             -   \n",
       "11   12                      Delhi                       856,112   \n",
       "12   13                    Haryana                       831,610   \n",
       "13   14                      Bihar                       611,804   \n",
       "14   15                     Punjab                       574,760   \n",
       "15   16                     Odisha                       521,275   \n",
       "16   17                      Assam                             -   \n",
       "17   18               Chhattisgarh                       329,180   \n",
       "18   19                  Jharkhand                       328,598   \n",
       "19   20                Uttarakhand                             -   \n",
       "20   21            Jammu & Kashmir                             -   \n",
       "21   22           Himachal Pradesh                       165,472   \n",
       "22   23                        Goa                        80,449   \n",
       "23   24                    Tripura                        55,984   \n",
       "24   25                 Chandigarh                             -   \n",
       "25   26                 Puducherry                        38,253   \n",
       "26   27                  Meghalaya                        36,572   \n",
       "27   28                     Sikkim                        32,496   \n",
       "28   29                    Manipur                        31,790   \n",
       "29   30                   Nagaland                             -   \n",
       "30   31          Arunachal Pradesh                             -   \n",
       "31   32                    Mizoram                        26,503   \n",
       "32   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP at current price (18-19) Share(18-19) GDP($ billion)  \n",
       "0                      2,632,792       13.94%        399.921  \n",
       "1                      1,630,208        8.63%        247.629  \n",
       "2                      1,584,764        8.39%        240.726  \n",
       "3                      1,502,899        7.96%        228.290  \n",
       "4                      1,493,127        7.91%        226.806  \n",
       "5                      1,089,898        5.77%        165.556  \n",
       "6                        942,586        4.99%        143.179  \n",
       "7                        862,957        4.57%        131.083  \n",
       "8                        861,031        4.56%        130.791  \n",
       "9                        809,592        4.29%        122.977  \n",
       "10                       781,653        4.14%        118.733  \n",
       "11                       774,870        4.10%        117.703  \n",
       "12                       734,163        3.89%        111.519  \n",
       "13                       530,363        2.81%         80.562  \n",
       "14                       526,376        2.79%         79.957  \n",
       "15                       487,805        2.58%         74.098  \n",
       "16                       315,881        1.67%         47.982  \n",
       "17                       304,063        1.61%         46.187  \n",
       "18                       297,204        1.57%         45.145  \n",
       "19                       245,895        1.30%         37.351  \n",
       "20                       155,956        0.83%         23.690  \n",
       "21                       153,845        0.81%         23.369  \n",
       "22                        73,170        0.39%         11.115  \n",
       "23                        49,845        0.26%          7.571  \n",
       "24                        42,114        0.22%          6.397  \n",
       "25                        34,433        0.18%          5.230  \n",
       "26                        33,481        0.18%          5.086  \n",
       "27                        28,723        0.15%          4.363  \n",
       "28                        27,870        0.15%          4.233  \n",
       "29                        27,283        0.14%          4.144  \n",
       "30                        24,603        0.13%          3.737  \n",
       "31                        22,287        0.12%          3.385  \n",
       "32                             -            -              -  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "State_GDP=pd.DataFrame({})\n",
    "State_GDP['Rank']=Rank_list\n",
    "State_GDP['State']=State_list\n",
    "State_GDP['GSDP at current price (19-20)']=GSDP_20\n",
    "State_GDP['GSDP at current price (18-19)']=GSDP_19\n",
    "State_GDP['Share(18-19)']=Share_19\n",
    "State_GDP['GDP($ billion)']=GDP_Billion\n",
    "State_GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************************************************************************************************************************************************************Completed*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries for web scraping:-\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/. \n",
    "You have to find the following details:\n",
    "    \n",
    "A) Repository title\n",
    "\n",
    "B) Repository description\n",
    "\n",
    "C) Contributors count\n",
    "\n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening \"https://github.com/\n",
    "url=\"https://github.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"947ba71cf5d33594220087b1161f6c71\", element=\"247ccae5-113a-417c-9af3-38586fca1ec7\")>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for Explore Menu\n",
    "exp1=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/summary\")\n",
    "exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking Explore Menu\n",
    "exp1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"947ba71cf5d33594220087b1161f6c71\", element=\"3c1bb92a-bbe0-4056-8a83-976e3c9af0f5\")>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for Trending Option\n",
    "exp2=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking Trending Option\n",
    "exp2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Repository_titlelist=[]\n",
    "Contributors_countlist=[]\n",
    "url_all=[]\n",
    "Repository_desc=[]\n",
    "Language_usedinfo=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['programthink / zhao',\n",
       " 'AkashSingh3031 / The-Complete-FAANG-Preparation',\n",
       " 'CaffeineMC / sodium-fabric',\n",
       " 'n8n-io / n8n',\n",
       " 'facebookresearch / AugLy',\n",
       " 'vxunderground / MalwareSourceCode',\n",
       " 'abuanwar072 / Flutter-Responsive-Admin-Panel-or-Dashboard',\n",
       " 'EssayKillerBrain / EssayKiller_V2',\n",
       " 'organicmaps / organicmaps',\n",
       " 'rustdesk / rustdesk',\n",
       " 'algorithm-visualizer / algorithm-visualizer',\n",
       " 'academic / awesome-datascience',\n",
       " 'megaease / easegress',\n",
       " 'TheBobPony / getwindows11.tech',\n",
       " 'donnemartin / system-design-primer',\n",
       " 'kwai / DouZero',\n",
       " 'programthink / opensource',\n",
       " 'facebookresearch / xcit',\n",
       " 'tailwindlabs / tailwindcss',\n",
       " 'coqui-ai / TTS',\n",
       " 'v2fly / v2ray-core',\n",
       " 'nothings / stb',\n",
       " 'mltframework / shotcut',\n",
       " 'flutter / flutter']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Repository_titlelist\n",
    "rep1=driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\")\n",
    "for i in rep1:\n",
    "    try:\n",
    "        Repository_titlelist.append(i.text)\n",
    "    except:\n",
    "        Repository_titlelist.append(\"-\")\n",
    "Repository_titlelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Repository_titlelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,127',\n",
       " '355',\n",
       " '307',\n",
       " '1,340',\n",
       " '70',\n",
       " '577',\n",
       " '543',\n",
       " '509',\n",
       " '44',\n",
       " '510',\n",
       " '5,648',\n",
       " '4,539',\n",
       " '197',\n",
       " '23',\n",
       " '24,689',\n",
       " '73',\n",
       " '1,395',\n",
       " '10',\n",
       " '1,987',\n",
       " '124',\n",
       " '1,633',\n",
       " '5,550',\n",
       " '561',\n",
       " '17,600']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Contributors_countlist\n",
    "rep2=driver.find_elements_by_xpath(\"//*[@id='js-pjax-container']/div[3]/div/div[2]/article/div[2]/a[2]\")\n",
    "for i in rep2:\n",
    "    try:\n",
    "        Contributors_countlist.append(i.text)\n",
    "    except:\n",
    "        Contributors_countlist.append(\"-\")\n",
    "Contributors_countlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Contributors_countlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for Repository_desc & Language_usedinfo we need to collect urls\n",
    "link=driver.find_elements_by_xpath(\"//article[@class='Box-row']/h1/a\")\n",
    "for i in link:\n",
    "    url_all.append(i.get_attribute(\"href\"))\n",
    "url_all    \n",
    "len(url_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting information for Repository description\n",
    "for i in url_all:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        rep3=driver.find_element_by_xpath('//*[@id=\"repo-content-pjax-container\"]/div/div[2]/div[2]/div/div[1]/div/p | //*[@id=\"repo-content-pjax-container\"]/div/div[2]/div[2]/div/div[1]/div/div[1]')\n",
    "        Repository_desc.append(rep3.text)\n",
    "    except NoSuchElementException as e:\n",
    "            Repository_desc.append(\"-\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵',\n",
       " 'This repository contains all the DSA (Data-Structures, Algorithms, 450 DSA by Love Babbar Bhaiya, FAANG Questions), Technical Subjects (OS + DBMS + SQL + CN + OOPs) Theory+Questions, FAANG Interview questions, and Miscellaneous Stuff (Programming MCQs, Puzzles, Aptitude, Reasoning). The Programming languages used for demonstration are C++, Pytho…',\n",
       " 'A Fabric mod designed to improve frame rates and reduce micro-stutter',\n",
       " 'Free and open fair-code licensed node based Workflow Automation Tool. Easily automate tasks across different services.',\n",
       " 'A data augmentations library for audio, image, text, and video.',\n",
       " 'Collection of malware source code for a variety of platforms in an array of different programming languages.',\n",
       " 'Responsive Admin Panel or Dashboard using Flutter',\n",
       " '基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化',\n",
       " '🍃 Organic Maps is a better fork of MAPS.ME, an Android & iOS offline maps app for travelers, tourists, hikers, and cyclists based on top of crowd-sourced OpenStreetMap data and curated with love by MAPS.ME founders. No ads, no tracking, no data collection, no crapware.',\n",
       " 'Yet another remote desktop software',\n",
       " '🎆Interactive Online Platform that Visualizes Algorithms from Code',\n",
       " '📝 An awesome Data Science repository to learn and apply for real world problems.',\n",
       " 'A Cloud Native traffic orchestration system',\n",
       " 'Mirrors to download the leaked Windows 11 build.',\n",
       " 'Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.',\n",
       " '[ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning | 斗地主AI',\n",
       " '【编程随想】收藏的开源项目清单',\n",
       " 'Official code Cross-Covariance Image Transformer (XCiT)',\n",
       " 'A utility-first CSS framework for rapid UI development.',\n",
       " '🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production',\n",
       " 'A platform for building proxies to bypass network restrictions.',\n",
       " 'stb single-file public domain libraries for C/C++',\n",
       " 'cross-platform (Qt), open-source (GPLv3) video editor',\n",
       " 'Flutter makes it easy and fast to build beautiful apps for mobile and beyond.']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Repository_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Repository_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'Jupyter Notebook',\n",
       " 'Java',\n",
       " 'TypeScript',\n",
       " 'Python',\n",
       " 'Assembly',\n",
       " 'Dart',\n",
       " 'Python',\n",
       " 'C++',\n",
       " 'Rust',\n",
       " 'JavaScript',\n",
       " '-',\n",
       " 'Go',\n",
       " 'HTML',\n",
       " 'Python',\n",
       " 'Python',\n",
       " '-',\n",
       " 'Python',\n",
       " 'JavaScript',\n",
       " 'Jupyter Notebook',\n",
       " 'Go',\n",
       " 'C',\n",
       " 'C++',\n",
       " 'Dart']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Language_used \n",
    "for i in url_all:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        rep4=driver.find_element_by_xpath('//span[@class=\"color-text-primary text-bold mr-1\"]')\n",
    "        Language_usedinfo.append(rep4.text)\n",
    "    except NoSuchElementException as e:\n",
    "            Language_usedinfo.append(\"-\")  \n",
    "Language_usedinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Language_usedinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>programthink / zhao</td>\n",
       "      <td>【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵</td>\n",
       "      <td>2,127</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AkashSingh3031 / The-Complete-FAANG-Preparation</td>\n",
       "      <td>This repository contains all the DSA (Data-Str...</td>\n",
       "      <td>355</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CaffeineMC / sodium-fabric</td>\n",
       "      <td>A Fabric mod designed to improve frame rates a...</td>\n",
       "      <td>307</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n8n-io / n8n</td>\n",
       "      <td>Free and open fair-code licensed node based Wo...</td>\n",
       "      <td>1,340</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebookresearch / AugLy</td>\n",
       "      <td>A data augmentations library for audio, image,...</td>\n",
       "      <td>70</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vxunderground / MalwareSourceCode</td>\n",
       "      <td>Collection of malware source code for a variet...</td>\n",
       "      <td>577</td>\n",
       "      <td>Assembly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abuanwar072 / Flutter-Responsive-Admin-Panel-o...</td>\n",
       "      <td>Responsive Admin Panel or Dashboard using Flutter</td>\n",
       "      <td>543</td>\n",
       "      <td>Dart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EssayKillerBrain / EssayKiller_V2</td>\n",
       "      <td>基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化</td>\n",
       "      <td>509</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>organicmaps / organicmaps</td>\n",
       "      <td>🍃 Organic Maps is a better fork of MAPS.ME, an...</td>\n",
       "      <td>44</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rustdesk / rustdesk</td>\n",
       "      <td>Yet another remote desktop software</td>\n",
       "      <td>510</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>algorithm-visualizer / algorithm-visualizer</td>\n",
       "      <td>🎆Interactive Online Platform that Visualizes A...</td>\n",
       "      <td>5,648</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>academic / awesome-datascience</td>\n",
       "      <td>📝 An awesome Data Science repository to learn ...</td>\n",
       "      <td>4,539</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>megaease / easegress</td>\n",
       "      <td>A Cloud Native traffic orchestration system</td>\n",
       "      <td>197</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TheBobPony / getwindows11.tech</td>\n",
       "      <td>Mirrors to download the leaked Windows 11 build.</td>\n",
       "      <td>23</td>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>donnemartin / system-design-primer</td>\n",
       "      <td>Learn how to design large-scale systems. Prep ...</td>\n",
       "      <td>24,689</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kwai / DouZero</td>\n",
       "      <td>[ICML 2021] DouZero: Mastering DouDizhu with S...</td>\n",
       "      <td>73</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>programthink / opensource</td>\n",
       "      <td>【编程随想】收藏的开源项目清单</td>\n",
       "      <td>1,395</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>facebookresearch / xcit</td>\n",
       "      <td>Official code Cross-Covariance Image Transform...</td>\n",
       "      <td>10</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tailwindlabs / tailwindcss</td>\n",
       "      <td>A utility-first CSS framework for rapid UI dev...</td>\n",
       "      <td>1,987</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coqui-ai / TTS</td>\n",
       "      <td>🐸💬 - a deep learning toolkit for Text-to-Speec...</td>\n",
       "      <td>124</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>v2fly / v2ray-core</td>\n",
       "      <td>A platform for building proxies to bypass netw...</td>\n",
       "      <td>1,633</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nothings / stb</td>\n",
       "      <td>stb single-file public domain libraries for C/C++</td>\n",
       "      <td>5,550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mltframework / shotcut</td>\n",
       "      <td>cross-platform (Qt), open-source (GPLv3) video...</td>\n",
       "      <td>561</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>flutter / flutter</td>\n",
       "      <td>Flutter makes it easy and fast to build beauti...</td>\n",
       "      <td>17,600</td>\n",
       "      <td>Dart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Repository title  \\\n",
       "0                                 programthink / zhao   \n",
       "1     AkashSingh3031 / The-Complete-FAANG-Preparation   \n",
       "2                          CaffeineMC / sodium-fabric   \n",
       "3                                        n8n-io / n8n   \n",
       "4                            facebookresearch / AugLy   \n",
       "5                   vxunderground / MalwareSourceCode   \n",
       "6   abuanwar072 / Flutter-Responsive-Admin-Panel-o...   \n",
       "7                   EssayKillerBrain / EssayKiller_V2   \n",
       "8                           organicmaps / organicmaps   \n",
       "9                                 rustdesk / rustdesk   \n",
       "10        algorithm-visualizer / algorithm-visualizer   \n",
       "11                     academic / awesome-datascience   \n",
       "12                               megaease / easegress   \n",
       "13                     TheBobPony / getwindows11.tech   \n",
       "14                 donnemartin / system-design-primer   \n",
       "15                                     kwai / DouZero   \n",
       "16                          programthink / opensource   \n",
       "17                            facebookresearch / xcit   \n",
       "18                         tailwindlabs / tailwindcss   \n",
       "19                                     coqui-ai / TTS   \n",
       "20                                 v2fly / v2ray-core   \n",
       "21                                     nothings / stb   \n",
       "22                             mltframework / shotcut   \n",
       "23                                  flutter / flutter   \n",
       "\n",
       "                               Repository description Contributors count  \\\n",
       "0                        【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵              2,127   \n",
       "1   This repository contains all the DSA (Data-Str...                355   \n",
       "2   A Fabric mod designed to improve frame rates a...                307   \n",
       "3   Free and open fair-code licensed node based Wo...              1,340   \n",
       "4   A data augmentations library for audio, image,...                 70   \n",
       "5   Collection of malware source code for a variet...                577   \n",
       "6   Responsive Admin Panel or Dashboard using Flutter                543   \n",
       "7                      基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化                509   \n",
       "8   🍃 Organic Maps is a better fork of MAPS.ME, an...                 44   \n",
       "9                 Yet another remote desktop software                510   \n",
       "10  🎆Interactive Online Platform that Visualizes A...              5,648   \n",
       "11  📝 An awesome Data Science repository to learn ...              4,539   \n",
       "12        A Cloud Native traffic orchestration system                197   \n",
       "13   Mirrors to download the leaked Windows 11 build.                 23   \n",
       "14  Learn how to design large-scale systems. Prep ...             24,689   \n",
       "15  [ICML 2021] DouZero: Mastering DouDizhu with S...                 73   \n",
       "16                                    【编程随想】收藏的开源项目清单              1,395   \n",
       "17  Official code Cross-Covariance Image Transform...                 10   \n",
       "18  A utility-first CSS framework for rapid UI dev...              1,987   \n",
       "19  🐸💬 - a deep learning toolkit for Text-to-Speec...                124   \n",
       "20  A platform for building proxies to bypass netw...              1,633   \n",
       "21  stb single-file public domain libraries for C/C++              5,550   \n",
       "22  cross-platform (Qt), open-source (GPLv3) video...                561   \n",
       "23  Flutter makes it easy and fast to build beauti...             17,600   \n",
       "\n",
       "       Language used  \n",
       "0             Python  \n",
       "1   Jupyter Notebook  \n",
       "2               Java  \n",
       "3         TypeScript  \n",
       "4             Python  \n",
       "5           Assembly  \n",
       "6               Dart  \n",
       "7             Python  \n",
       "8                C++  \n",
       "9               Rust  \n",
       "10        JavaScript  \n",
       "11                 -  \n",
       "12                Go  \n",
       "13              HTML  \n",
       "14            Python  \n",
       "15            Python  \n",
       "16                 -  \n",
       "17            Python  \n",
       "18        JavaScript  \n",
       "19  Jupyter Notebook  \n",
       "20                Go  \n",
       "21                 C  \n",
       "22               C++  \n",
       "23              Dart  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "Github_rep=pd.DataFrame({})\n",
    "Github_rep['Repository title']=Repository_titlelist\n",
    "Github_rep['Repository description']=Repository_desc\n",
    "Github_rep['Contributors count']=Contributors_countlist\n",
    "Github_rep['Language used']=Language_usedinfo\n",
    "Github_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************************************************************************************Completed********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries for web scraping:-\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billboard.com.\n",
    "Url = https://www.billboard.com/.\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Song name\n",
    "\n",
    "B) Artist name\n",
    "\n",
    "C) Last week rank\n",
    "\n",
    "D) Peak rank\n",
    "\n",
    "E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.\n",
    "    \n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening \"https://www.billboard.com/\n",
    "url=\"https://www.billboard.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"4d33eb71702f6cf436ca335a2c0c570c\", element=\"3f533d9e-f452-4b30-8372-4abe9744454f\")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for charts option\n",
    "chart1=driver.find_element_by_xpath(\"//*[@id='root']/div[2]/div[2]/header/div/ul/li[1]/a\")\n",
    "chart1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking charts option\n",
    "chart1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"4d33eb71702f6cf436ca335a2c0c570c\", element=\"dbdf28dd-f5cc-4b5e-beed-e7370232bc7f\")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for hot 100-page link\n",
    "chart2=driver.find_element_by_xpath(\"//*[@id='main']/div[2]/div/div[1]/a/div[2]/div[2]/div[1]\")\n",
    "chart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking hot 100-page link\n",
    "chart2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Lastweek_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_onboard=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Butter',\n",
       " 'Good 4 U',\n",
       " 'Levitating',\n",
       " 'Peaches',\n",
       " 'Leave The Door Open',\n",
       " 'Save Your Tears',\n",
       " 'Kiss Me More',\n",
       " 'Astronaut In The Ocean',\n",
       " 'Deja Vu',\n",
       " 'Yonaguni',\n",
       " 'Montero (Call Me By Your Name)',\n",
       " 'Without You',\n",
       " 'Forever After All',\n",
       " 'Rapstar',\n",
       " 'Blinding Lights',\n",
       " 'Hats Off',\n",
       " 'Drivers License',\n",
       " 'Beautiful Mistakes',\n",
       " 'Traitor',\n",
       " 'Late At Night',\n",
       " 'Voice Of The Heroes',\n",
       " 'Best Friend',\n",
       " 'Heartbreak Anniversary',\n",
       " 'Heat Waves',\n",
       " 'Calling My Phone',\n",
       " 'Favorite Crime',\n",
       " 'Lost Cause',\n",
       " 'Happier',\n",
       " 'Up',\n",
       " 'Mood',\n",
       " '2040',\n",
       " 'Every Chance I Get',\n",
       " 'Telepatia',\n",
       " 'How It Feels',\n",
       " 'Brutal',\n",
       " 'Wockesha',\n",
       " 'Wants And Needs',\n",
       " 'Famous Friends',\n",
       " 'pov',\n",
       " 'Gone',\n",
       " 'Beat Box',\n",
       " \"My Ex's Best Friend\",\n",
       " 'Still Runnin',\n",
       " 'Track Star',\n",
       " 'What You Know Bout Love',\n",
       " 'Who I Want',\n",
       " 'my.life',\n",
       " 'Back In Blood',\n",
       " 'Time Today',\n",
       " 'Lil Bit',\n",
       " 'Blame It On You',\n",
       " 'Nobody',\n",
       " 'Todo de Ti',\n",
       " 'Enough For You',\n",
       " 'Settling Down',\n",
       " 'Still Hood',\n",
       " 'Jealousy, Jealousy',\n",
       " 'Okay',\n",
       " 'Almost Maybes',\n",
       " 'Man Of My Word',\n",
       " 'Build A Bitch',\n",
       " \"Breaking Up Was Easy In The 90's\",\n",
       " 'Glad You Exist',\n",
       " 'pride.is.the.devil',\n",
       " '1 Step Forward, 3 Steps Back',\n",
       " 'Ski',\n",
       " 'Medical',\n",
       " 'Rich Off Pain',\n",
       " 'Leave Before You Love Me',\n",
       " 'Lying',\n",
       " 'Snowflakes',\n",
       " 'Single Saturday Night',\n",
       " \"That's Facts\",\n",
       " 'Made For You',\n",
       " 'Your Power',\n",
       " 'No More Parties',\n",
       " \"We're Good\",\n",
       " 'One Too Many',\n",
       " 'Please',\n",
       " 'Up The Side',\n",
       " 'Minimum Wage',\n",
       " 'Miss The Rage',\n",
       " 'Way Less Sad',\n",
       " 'Hope Ur OK',\n",
       " 'Quicksand',\n",
       " 'Arcade',\n",
       " 'Straightenin',\n",
       " 'Gang Gang',\n",
       " \"Drinkin' Beer. Talkin' God. Amen.\",\n",
       " 'Follow You',\n",
       " 'Chasing After You',\n",
       " 'Tell Em',\n",
       " '4 Da Gang',\n",
       " 'Tombstone',\n",
       " \"What's Next\",\n",
       " 'Things A Man Oughta Know',\n",
       " 'Country Again',\n",
       " \"Drunk (And I Don't Wanna Go Home)\",\n",
       " 'If You Want To',\n",
       " 'Seeing Green']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Song_name\n",
    "song1=driver.find_elements_by_xpath(\"//*[@id='charts']/div/div[7]/div/ol/li/button/span[2]/span[1]\")\n",
    "for i in song1:\n",
    "    try:\n",
    "        Song_name.append(i.text)\n",
    "    except:\n",
    "        Song_name.append(\"-\")\n",
    "Song_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Song_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTS',\n",
       " 'Olivia Rodrigo',\n",
       " 'Dua Lipa Featuring DaBaby',\n",
       " 'Justin Bieber Featuring Daniel Caesar & Giveon',\n",
       " 'Silk Sonic (Bruno Mars & Anderson .Paak)',\n",
       " 'The Weeknd & Ariana Grande',\n",
       " 'Doja Cat Featuring SZA',\n",
       " 'Masked Wolf',\n",
       " 'Olivia Rodrigo',\n",
       " 'Bad Bunny',\n",
       " 'Lil Nas X',\n",
       " 'The Kid LAROI',\n",
       " 'Luke Combs',\n",
       " 'Polo G',\n",
       " 'The Weeknd',\n",
       " 'Lil Baby, Lil Durk & Travis Scott',\n",
       " 'Olivia Rodrigo',\n",
       " 'Maroon 5 Featuring Megan Thee Stallion',\n",
       " 'Olivia Rodrigo',\n",
       " 'Roddy Ricch',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Saweetie Featuring Doja Cat',\n",
       " 'Giveon',\n",
       " 'Glass Animals',\n",
       " 'Lil Tjay Featuring 6LACK',\n",
       " 'Olivia Rodrigo',\n",
       " 'Billie Eilish',\n",
       " 'Olivia Rodrigo',\n",
       " 'Cardi B',\n",
       " '24kGoldn Featuring iann dior',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'DJ Khaled Featuring Lil Baby & Lil Durk',\n",
       " 'Kali Uchis',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Olivia Rodrigo',\n",
       " 'Moneybagg Yo',\n",
       " 'Drake Featuring Lil Baby',\n",
       " 'Chris Young + Kane Brown',\n",
       " 'Ariana Grande',\n",
       " 'Dierks Bentley',\n",
       " 'SpotemGottem Featuring Pooh Shiesty Or DaBaby',\n",
       " 'Machine Gun Kelly X blackbear',\n",
       " 'Lil Baby, Lil Durk & Meek Mill',\n",
       " 'Mooski',\n",
       " 'Pop Smoke',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'J. Cole, 21 Savage & Morray',\n",
       " 'Pooh Shiesty Featuring Lil Durk',\n",
       " 'Moneybagg Yo',\n",
       " 'Nelly & Florida Georgia Line',\n",
       " 'Jason Aldean',\n",
       " 'Dylan Scott',\n",
       " 'Rauw Alejandro',\n",
       " 'Olivia Rodrigo',\n",
       " 'Miranda Lambert',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Olivia Rodrigo',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Jordan Davis',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Bella Poarch',\n",
       " 'Sam Hunt',\n",
       " 'Dan + Shay',\n",
       " 'J. Cole & Lil Baby',\n",
       " 'Olivia Rodrigo',\n",
       " 'Young Thug & Gunna',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Lil Baby, Lil Durk & Rod Wave',\n",
       " 'Marshmello X Jonas Brothers',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Tom MacDonald',\n",
       " 'Cole Swindell',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Jake Owen',\n",
       " 'Billie Eilish',\n",
       " 'Coi Leray Featuring Lil Durk',\n",
       " 'Dua Lipa',\n",
       " 'Keith Urban Duet With P!nk',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Lil Baby, Lil Durk & Young Thug',\n",
       " 'Blake Shelton',\n",
       " 'Trippie Redd & Playboi Carti',\n",
       " 'AJR',\n",
       " 'Olivia Rodrigo',\n",
       " 'Morray',\n",
       " 'Duncan Laurence',\n",
       " 'Migos',\n",
       " 'Polo G & Lil Wayne',\n",
       " 'Chase Rice Featuring Florida Georgia Line',\n",
       " 'Imagine Dragons',\n",
       " 'Ryan Hurd With Maren Morris',\n",
       " 'Cochise & $NOT',\n",
       " '42 Dugg & Roddy Ricch',\n",
       " 'Rod Wave',\n",
       " 'Drake',\n",
       " 'Lainey Wilson',\n",
       " 'Thomas Rhett',\n",
       " 'Elle King & Miranda Lambert',\n",
       " 'Lil Baby & Lil Durk',\n",
       " 'Nicki Minaj, Drake & Lil Wayne']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Artist_name\n",
    "song2=driver.find_elements_by_xpath(\"//*[@id='charts']/div/div[7]/div/ol/li/button/span[2]/span[2]\")\n",
    "for i in song2:\n",
    "    try:\n",
    "        Artist_name.append(i.text)\n",
    "    except:\n",
    "        Artist_name.append(\"-\")\n",
    "Artist_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '6',\n",
       " '4',\n",
       " '5',\n",
       " '7',\n",
       " '9',\n",
       " '8',\n",
       " '-',\n",
       " '10',\n",
       " '11',\n",
       " '15',\n",
       " '14',\n",
       " '17',\n",
       " '-',\n",
       " '13',\n",
       " '18',\n",
       " '12',\n",
       " '-',\n",
       " '81',\n",
       " '22',\n",
       " '23',\n",
       " '25',\n",
       " '21',\n",
       " '16',\n",
       " '-',\n",
       " '20',\n",
       " '24',\n",
       " '30',\n",
       " '-',\n",
       " '26',\n",
       " '34',\n",
       " '-',\n",
       " '19',\n",
       " '33',\n",
       " '39',\n",
       " '43',\n",
       " '37',\n",
       " '36',\n",
       " '29',\n",
       " '31',\n",
       " '-',\n",
       " '32',\n",
       " '41',\n",
       " '-',\n",
       " '28',\n",
       " '35',\n",
       " '48',\n",
       " '53',\n",
       " '63',\n",
       " '50',\n",
       " '66',\n",
       " '27',\n",
       " '47',\n",
       " '-',\n",
       " '42',\n",
       " '-',\n",
       " '51',\n",
       " '-',\n",
       " '56',\n",
       " '49',\n",
       " '68',\n",
       " '45',\n",
       " '38',\n",
       " '55',\n",
       " '-',\n",
       " '-',\n",
       " '72',\n",
       " '-',\n",
       " '-',\n",
       " '70',\n",
       " '-',\n",
       " '57',\n",
       " '60',\n",
       " '59',\n",
       " '54',\n",
       " '71',\n",
       " '-',\n",
       " '-',\n",
       " '69',\n",
       " '58',\n",
       " '77',\n",
       " '52',\n",
       " '73',\n",
       " '85',\n",
       " '78',\n",
       " '61',\n",
       " '75',\n",
       " '86',\n",
       " '87',\n",
       " '64',\n",
       " '74',\n",
       " '79',\n",
       " '76',\n",
       " '93',\n",
       " '89',\n",
       " '92',\n",
       " '-',\n",
       " '67']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Lastweek_rank\n",
    "song3=driver.find_elements_by_xpath(\"//div[@class='chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "for i in song3:\n",
    "    try:\n",
    "        Lastweek_rank.append(i.text)\n",
    "    except:\n",
    "        Lastweek_rank.append(\"-\")\n",
    "Lastweek_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Lastweek_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '5',\n",
       " '6',\n",
       " '3',\n",
       " '10',\n",
       " '1',\n",
       " '8',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '16',\n",
       " '1',\n",
       " '18',\n",
       " '9',\n",
       " '20',\n",
       " '21',\n",
       " '14',\n",
       " '17',\n",
       " '24',\n",
       " '3',\n",
       " '16',\n",
       " '27',\n",
       " '15',\n",
       " '1',\n",
       " '1',\n",
       " '31',\n",
       " '20',\n",
       " '33',\n",
       " '34',\n",
       " '12',\n",
       " '33',\n",
       " '2',\n",
       " '38',\n",
       " '37',\n",
       " '36',\n",
       " '12',\n",
       " '20',\n",
       " '43',\n",
       " '31',\n",
       " '9',\n",
       " '46',\n",
       " '2',\n",
       " '13',\n",
       " '31',\n",
       " '50',\n",
       " '51',\n",
       " '50',\n",
       " '53',\n",
       " '14',\n",
       " '47',\n",
       " '56',\n",
       " '24',\n",
       " '58',\n",
       " '51',\n",
       " '60',\n",
       " '56',\n",
       " '32',\n",
       " '63',\n",
       " '7',\n",
       " '19',\n",
       " '18',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '70',\n",
       " '73',\n",
       " '32',\n",
       " '10',\n",
       " '26',\n",
       " '31',\n",
       " '62',\n",
       " '79',\n",
       " '80',\n",
       " '69',\n",
       " '11',\n",
       " '77',\n",
       " '29',\n",
       " '65',\n",
       " '78',\n",
       " '38',\n",
       " '33',\n",
       " '75',\n",
       " '68',\n",
       " '87',\n",
       " '64',\n",
       " '67',\n",
       " '11',\n",
       " '1',\n",
       " '93',\n",
       " '73',\n",
       " '79',\n",
       " '99',\n",
       " '12']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Peak_rank\n",
    "song4=driver.find_elements_by_xpath(\"//div[@class='chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "for i in song4:\n",
    "    try:\n",
    "        Peak_rank.append(i.text)\n",
    "    except:\n",
    "        Peak_rank.append(\"-\")\n",
    "Peak_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Peak_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '4',\n",
       " '36',\n",
       " '12',\n",
       " '14',\n",
       " '26',\n",
       " '9',\n",
       " '17',\n",
       " '10',\n",
       " '1',\n",
       " '11',\n",
       " '27',\n",
       " '33',\n",
       " '9',\n",
       " '79',\n",
       " '1',\n",
       " '22',\n",
       " '14',\n",
       " '3',\n",
       " '1',\n",
       " '2',\n",
       " '22',\n",
       " '17',\n",
       " '21',\n",
       " '17',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '18',\n",
       " '44',\n",
       " '1',\n",
       " '6',\n",
       " '16',\n",
       " '1',\n",
       " '3',\n",
       " '7',\n",
       " '14',\n",
       " '11',\n",
       " '14',\n",
       " '13',\n",
       " '21',\n",
       " '43',\n",
       " '1',\n",
       " '16',\n",
       " '40',\n",
       " '1',\n",
       " '4',\n",
       " '23',\n",
       " '18',\n",
       " '12',\n",
       " '7',\n",
       " '15',\n",
       " '2',\n",
       " '3',\n",
       " '13',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '20',\n",
       " '1',\n",
       " '4',\n",
       " '15',\n",
       " '18',\n",
       " '4',\n",
       " '3',\n",
       " '8',\n",
       " '1',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '1',\n",
       " '6',\n",
       " '1',\n",
       " '16',\n",
       " '6',\n",
       " '18',\n",
       " '17',\n",
       " '26',\n",
       " '1',\n",
       " '1',\n",
       " '5',\n",
       " '5',\n",
       " '7',\n",
       " '3',\n",
       " '18',\n",
       " '9',\n",
       " '4',\n",
       " '3',\n",
       " '2',\n",
       " '10',\n",
       " '7',\n",
       " '2',\n",
       " '10',\n",
       " '12',\n",
       " '14',\n",
       " '4',\n",
       " '6',\n",
       " '7',\n",
       " '1',\n",
       " '4']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Weeks_onboard\n",
    "song5=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "for i in song5:\n",
    "    try:\n",
    "        Weeks_onboard.append(i.text)\n",
    "    except:\n",
    "        Weeks_onboard.append(\"-\")\n",
    "Weeks_onboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Weeks_onboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks Onboard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Butter</td>\n",
       "      <td>BTS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good 4 U</td>\n",
       "      <td>Olivia Rodrigo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Levitating</td>\n",
       "      <td>Dua Lipa Featuring DaBaby</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peaches</td>\n",
       "      <td>Justin Bieber Featuring Daniel Caesar &amp; Giveon</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leave The Door Open</td>\n",
       "      <td>Silk Sonic (Bruno Mars &amp; Anderson .Paak)</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Things A Man Oughta Know</td>\n",
       "      <td>Lainey Wilson</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Country Again</td>\n",
       "      <td>Thomas Rhett</td>\n",
       "      <td>89</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Drunk (And I Don't Wanna Go Home)</td>\n",
       "      <td>Elle King &amp; Miranda Lambert</td>\n",
       "      <td>92</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>If You Want To</td>\n",
       "      <td>Lil Baby &amp; Lil Durk</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Seeing Green</td>\n",
       "      <td>Nicki Minaj, Drake &amp; Lil Wayne</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Song Name  \\\n",
       "0                              Butter   \n",
       "1                            Good 4 U   \n",
       "2                          Levitating   \n",
       "3                             Peaches   \n",
       "4                 Leave The Door Open   \n",
       "..                                ...   \n",
       "95           Things A Man Oughta Know   \n",
       "96                      Country Again   \n",
       "97  Drunk (And I Don't Wanna Go Home)   \n",
       "98                     If You Want To   \n",
       "99                       Seeing Green   \n",
       "\n",
       "                                       Artist Name Last Week Rank Peak Rank  \\\n",
       "0                                              BTS              1         1   \n",
       "1                                   Olivia Rodrigo              2         1   \n",
       "2                        Dua Lipa Featuring DaBaby              3         2   \n",
       "3   Justin Bieber Featuring Daniel Caesar & Giveon              6         1   \n",
       "4         Silk Sonic (Bruno Mars & Anderson .Paak)              4         1   \n",
       "..                                             ...            ...       ...   \n",
       "95                                   Lainey Wilson             93        93   \n",
       "96                                    Thomas Rhett             89        73   \n",
       "97                     Elle King & Miranda Lambert             92        79   \n",
       "98                             Lil Baby & Lil Durk              -        99   \n",
       "99                  Nicki Minaj, Drake & Lil Wayne             67        12   \n",
       "\n",
       "   Weeks Onboard  \n",
       "0              3  \n",
       "1              4  \n",
       "2             36  \n",
       "3             12  \n",
       "4             14  \n",
       "..           ...  \n",
       "95             4  \n",
       "96             6  \n",
       "97             7  \n",
       "98             1  \n",
       "99             4  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "billboard_hot100=pd.DataFrame({})\n",
    "billboard_hot100['Song Name']=Song_name\n",
    "billboard_hot100['Artist Name']=Artist_name\n",
    "billboard_hot100['Last Week Rank']=Lastweek_rank\n",
    "billboard_hot100['Peak Rank']=Peak_rank\n",
    "billboard_hot100['Weeks Onboard']=Weeks_onboard\n",
    "billboard_hot100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************************************************Completed****************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/.\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Name\n",
    "\n",
    "B) Designation\n",
    "\n",
    "C) Company\n",
    "\n",
    "D) Skills they hire for\n",
    "\n",
    "E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage https://www.naukri.com/\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"02d11618d41f3b6154175db1bf54d664\", element=\"583c2b4a-dea8-4ce9-a356-e39a2e5e9b6c\")>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for recruiters option\n",
    "ro1=driver.find_element_by_xpath(\"//*[@id='root']/div[1]/div/ul[1]/li[2]/a\")\n",
    "ro1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking recruiters option link\n",
    "driver.get(ro1.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"02d11618d41f3b6154175db1bf54d664\", element=\"5785028a-74cc-44ad-b541-97e31e521d8f\")>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for search bar\n",
    "ro2=driver.find_element_by_xpath(\"//*[@id='skill']/div[1]/div[2]/input\")\n",
    "ro2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching for “Data science” in Search bar\n",
    "ro2.send_keys(\"Data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the Search Button using Xpath function\n",
    "ro3=driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\")\n",
    "ro3.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Recruiter_name=[]\n",
    "Recruiter_Designation=[]\n",
    "Recruiter_Company=[]\n",
    "Recruiter_Skills=[]\n",
    "Recruiter_Location=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aakash Harit',\n",
       " 'shravan Kumar Gaddam',\n",
       " 'MARSIAN Technologies LLP',\n",
       " 'Anik Agrawal',\n",
       " 'subhas patel',\n",
       " 'Abhishek - Only Analytics Hiring - India and',\n",
       " 'Institute for Financial Management and Resear',\n",
       " 'Balu Ramesh',\n",
       " 'Asif Lucknowi',\n",
       " 'InstaFinancials',\n",
       " 'Kalpana Dumpala',\n",
       " 'Mubarak',\n",
       " 'Kushal Rastogi',\n",
       " 'Ruchi Dhote',\n",
       " 'Mahesh Babu Channa',\n",
       " 'Kapil Devang',\n",
       " 'Manisha Yadav',\n",
       " 'Riya Rajesh',\n",
       " 'Rashmi Bhattacharjee',\n",
       " 'Faizan Kareem',\n",
       " 'Rithika dadwal',\n",
       " 'Azahar Shaikh',\n",
       " 'Sandhya Khandagale',\n",
       " 'Shaun Rao',\n",
       " 'Manas',\n",
       " 'kumar',\n",
       " 'Sunil Vedula',\n",
       " 'Rajat Kumar',\n",
       " 'Priya Khare',\n",
       " 'Dhruv Dev Dubey',\n",
       " 'Jayanth N',\n",
       " 'SREEDHAR',\n",
       " 'Radha Manivasagam',\n",
       " 'Prateek Kumar',\n",
       " 'Amit Sharma',\n",
       " 'Kanan',\n",
       " 'Shashikant Chaudhary',\n",
       " 'Brad',\n",
       " 'Rutuja Pawar',\n",
       " 'Madhusudhan Sridhar',\n",
       " 'Ankit Sinha',\n",
       " 'Gaurav Chouhan',\n",
       " 'Rashi Kacker',\n",
       " 'Ashwini',\n",
       " 'Balaji Kolli',\n",
       " 'Rajani Nagaraj',\n",
       " 'ROHIT Kumar',\n",
       " 'Amir Chowdhury',\n",
       " 'Shailja Mishra',\n",
       " 'Sunny Sharma']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Recruiter_name\n",
    "r1=driver.find_elements_by_xpath(\"//div[@class='recInfo']/div/p/a[1]\")\n",
    "for i in r1:\n",
    "    try:\n",
    "        Recruiter_name.append(i.text)\n",
    "    except:\n",
    "        Recruiter_name.append(\"-\")\n",
    "Recruiter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Recruiter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HR Manager',\n",
       " 'Company Recruiter',\n",
       " 'Company HR',\n",
       " 'Company Recruiter',\n",
       " 'Founder CEO',\n",
       " 'Recruitment Lead Consultant',\n",
       " 'Programme Manager',\n",
       " 'HR Administrator',\n",
       " 'Director',\n",
       " 'Human Resource',\n",
       " 'Executive Hiring',\n",
       " 'Company HR',\n",
       " 'Company HR',\n",
       " 'Senior Executive Talent Acquisition',\n",
       " 'HR Team Lead',\n",
       " 'HR Manager',\n",
       " 'HR Executive',\n",
       " 'Manager Talent Acquisition',\n",
       " 'HR Head',\n",
       " 'HR MANAGER',\n",
       " 'HR Recruiter',\n",
       " 'Company Recruiter',\n",
       " 'HR Recruiter',\n",
       " 'Manager Human Resources',\n",
       " 'Lead Talent acquisition',\n",
       " 'Proprietor',\n",
       " 'CEO',\n",
       " 'Founder CEO',\n",
       " 'Senior Manager',\n",
       " 'Company Recruitment Head',\n",
       " 'Project Manager',\n",
       " 'Recruitment Consultant',\n",
       " 'HR Executive',\n",
       " 'Head',\n",
       " 'Consultant',\n",
       " 'senior technology instructor',\n",
       " 'HR Recruiter/HR Excutive',\n",
       " 'Manager, Technical Recruiting',\n",
       " 'Technical Recruiter',\n",
       " 'Erp Implementer',\n",
       " 'Head Analytics',\n",
       " 'Chief Technical Officer',\n",
       " 'Sr Product Manager',\n",
       " 'Director Global Delivery',\n",
       " 'Co Founder',\n",
       " 'HR Manager',\n",
       " 'Architect',\n",
       " 'Managing Partner',\n",
       " 'HR Manager',\n",
       " 'Managing Director - HR']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Recruiter_Designation\n",
    "r2=driver.find_elements_by_xpath(\"//div[@class='recInfo']/div/p/span[1]\")\n",
    "for i in r2:\n",
    "    try:\n",
    "        Recruiter_Designation.append(i.text)\n",
    "    except:\n",
    "        Recruiter_Designation.append(\"-\")\n",
    "Recruiter_Designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Recruiter_Designation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Science Network',\n",
       " 'Shore Infotech India Pvt. Ltd',\n",
       " 'MARSIAN Technologies LLP',\n",
       " 'Enerlytics Software Solutions Pvt Ltd',\n",
       " 'LibraryXProject',\n",
       " 'Apidel Technologies Division of Transpower',\n",
       " 'IFMR',\n",
       " 'Techvantage Systems Pvt Ltd',\n",
       " 'Weupskill- Live Wire India',\n",
       " 'CBL Data Science Private Limited',\n",
       " 'Innominds Software',\n",
       " 'MoneyTap',\n",
       " 'QuantMagnum Technologies Pvt. Ltd.',\n",
       " 'Bristlecone India Ltd',\n",
       " 'SocialPrachar.com',\n",
       " 'BISP Solutions',\n",
       " 'Easi Tax',\n",
       " 'Novelworx Digital Solutions',\n",
       " 'AXESTRACK SOFTWARE SOLUTIONS PRIVATE...',\n",
       " 'FirstTech Consaltants Pvt.Ltd',\n",
       " 'Affine Analytics',\n",
       " 'NEAL ANALYTICS SERVICES PVT LTD',\n",
       " 'Compumatrice Multimedia Pvt Ltd',\n",
       " 'Exela Technologies',\n",
       " 'Autumn Leaf Consulting Services Private...',\n",
       " 'trainin',\n",
       " 'Nanoprecise Sci Corp',\n",
       " 'R.S Consultancy &amp; Services',\n",
       " 'Independent Consultant',\n",
       " 'Confidential',\n",
       " 'Dollarbird Information Services Pvt, Ltd',\n",
       " 'JOBSMILL BUSINESS SOLUTIONS PRIVATE LIMITED',\n",
       " 'Techcovery',\n",
       " 'Trisect',\n",
       " 'ASCO consulting',\n",
       " 'NY INST',\n",
       " '3D India Staffing Research &amp; Consulting...',\n",
       " 'O.C. Tanner',\n",
       " 'Demand Matrix',\n",
       " 'MADHUSUDHAN SRIDHAR',\n",
       " 'Suntech Global',\n",
       " 'Strategic Consulting Lab',\n",
       " 'Impel Labs Pvt. Ltd.',\n",
       " 'MRP Advisers',\n",
       " 'Saras Solutions India Pvt Ltd',\n",
       " 'WildJasmine',\n",
       " 'LNT Private Limited',\n",
       " 'Granular.ai',\n",
       " 'Certybox Pvt.Ltd.',\n",
       " 'Western Service Providers']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Recruiter_Company\n",
    "r3=driver.find_elements_by_xpath(\"//div[@class='recInfo']/div/p/a[2]\")\n",
    "for i in r3:\n",
    "    try:\n",
    "        Recruiter_Company.append(i.text)\n",
    "    except:\n",
    "        Recruiter_Company.append(\"-\")\n",
    "Recruiter_Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Recruiter_Company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Classic ASP Developer, Internet Marketing Professional, Data Science SME, Content Writers, SEO Professional, Revenue Professional',\n",
       " '.Net, Java, Data Science, Linux Administration, Sql Server Development, Winforms, Wcf Services, Wpf, Telecom Engineering, Technical Management, Software',\n",
       " 'Data Science, Artificial Intelligence, Machine Learning, Business Analytics',\n",
       " 'Mean Stack, javascript, angularjs, mongodb, Web Services, rest, express, Node.js, Big Data, iot, Data Science, Cloud Computing, saas, Aws',\n",
       " 'Hadoop, Spark, Digital Strategy, Data Architecture, Command Center, Cdp, Dmp, Kafka, Data Science, Data Analysis, Big Data Analytics, Real Time Analysis, SQL',\n",
       " 'Analytics, Business Intelligence, Business Analytics, Predictive Modeling, Predictive Analytics, Data Science, Data Analysis, Data Analytics, Big Data, Big',\n",
       " 'Data Science',\n",
       " 'Machine Learning, algorithms, Go Getter, Computer Science, spark, Big Data, hdfs, sql, cassandra, hadoop, python, scala, java, Data Science, Front End',\n",
       " 'Technical Training, Software Development, Presentation Skills, B.tech, M.tech, B.e., mca, msc, Computer Science, freshers, jobs in indore, Data Science, itil',\n",
       " 'Software Development, It Sales, Account Management, Data Analysis, Customer Service, Sr, Software Engineering, Mvc, Ajax, Asp.net, Html, C#, Javascript',\n",
       " 'Qa, Ui/ux, Java Developer, Java Architect, C++/qt, Php, Lamp, Api, J2ee, Java, Soa, Esb, Middleware, Bigdata Achitect, Hadoop Architect, Deep',\n",
       " 'Business Intelligence, Data Warehousing, Data Science, Business Analytics, Customer Support, Business Reporting, Bi',\n",
       " 'Office Administration, Hr Administration, telecalling, client relationship management, Client Acquisition, Sales, Reception, HR, Recruitment, Onboarding, Human',\n",
       " 'Qlikview, Qlik Sense, Microsoft Azure, Power Bi, Data Science, Machine Learning',\n",
       " 'Social Media, digital media maketing, seo, smm, smo, sem, Content Wirting, social media marketing, social media manager, digital media marketing manager',\n",
       " 'Big Data, Hadoop, Data Analytics, Data Science',\n",
       " 'Telecalling, Client Interaction, Marketing, Research, Web Development, Social Media Marketing, Data Entry Operation, Excel, Ms Office, Invoicing',\n",
       " 'Data Science',\n",
       " 'Corporate Sales, Software Development, Software Sales, Marketing, Creative Designing, Corporate Planning, Senior Management, Crm, Client Relationship',\n",
       " 'Data Analytics, Data Science, Machine Learning, Deep Learning, Nlp, Data Mining, Python, R, Database Administration, Text Mining',\n",
       " 'Data Science, Machine Learning, Python, R, Deep Learning, Big Data, Hadoop',\n",
       " 'Data Science, Artificial Intelligence, Machine Learning, Data Analytics',\n",
       " 'Big Data, Data Science, Artificial Intelligence, Hadoop, Ui Development, Php, Freelancing, .Net, Software Testing, Sap, Leadership Hiring',\n",
       " 'Java, Net, Angularjs, Hr, Infrastructure, Management, Project Management, Business Analysis, Data Science, Information Technology, Technology',\n",
       " 'Software Architecture, Vp Engineering, Product Management, analytics, Data Science, Node.js, Principal Engineer, Big Data, python, angularjs, React.js',\n",
       " 'Data Science, Hadoop, Rpas, Devops, Python, Aws, Teaching, Big Data',\n",
       " 'Signal Processing, Machine Learning, Neural Networks, Data Science, Predictive Analytics, Time Series Analysis, Data Visualization, Technical Leadership, Data',\n",
       " 'Web Technologies, Project Management, Software Architecture, Data Science, Object Oriented Programming, Computer Science, Electrical Engineering, Architecture',\n",
       " 'Data Science, Artificial Intelligence, analytics, Business Intelligence, python, tableau, Power Bi, qlikview, sql, Data Warehousing, Data Visualization',\n",
       " 'Server Administartion, Verilog, Vhdl, Digital Marketing, Market Research, Property Research, Legal, It And Non It Recruitment, Logistics, Supply Chain, Bfsi',\n",
       " 'Data Analytics, Managed Services, Team Leading, python, Machine Learning, Google Analytics, Dmp, Aws, Campaign Analytics, Digital Campaigns, Audience',\n",
       " 'Data Science, Machine Learning, Big Data Analytics, Spark, Python, R, Networking, Network Engineering, Placement, Training, Sql, Marketing, Mainframes, All',\n",
       " 'Python, Artificial Intelligence, Machine Learning, Data Science',\n",
       " 'Java, Python, Angularjs, Software Testing, Machine Learning, Data Science, Javascript, Django, React.js, Node.js, Augmented Reality, Virtual Reality, Advanced',\n",
       " 'Machine Learning, Artificial Intelligence, Data Science, Software Engineering, Software Development, Graduate Engineer Trainee, Fresher, Data Analytics, Java',\n",
       " 'C, C++, Artificial Intelligence, Python, Php, Web Development, Matlab, Data Science, Augmented Reality, C C++',\n",
       " 'Relationship Management, Retail Sales, Private Banking, Mutual Funds, NISM, Equity, Finance, Financial Products, Financial Services, Verbal, Written',\n",
       " 'Data Science, Software Engineering',\n",
       " 'Data Science, Big Data Analytics, Digital Marketing, Content Writing, Ui Development, Database Development, Qa Automation, Python, Project Management',\n",
       " 'Data Science, Recruitment, Salary',\n",
       " 'B.Tech, Tableau, Statistics, R, Analytics, Time Series, Data Science, Business Solutions, SQL, Technical Skills, SSAS, SQL Server, Analysis Services, Qlikview',\n",
       " 'Software Development, Business Intelligence, Big Data Analytics, Database Administration, Data Science, Microsoft Azure, Spark, Cassandra, Object Oriented',\n",
       " 'Data Science, Node.js, Angularjs',\n",
       " 'Data Science, Media Marketing, Resource Planning, Managed Services, Display Advertising, Machine Learning, Python, Etl, Sql',\n",
       " 'Data Analysis, Learning, Data Science, Computer Science, Communication Skills',\n",
       " 'Java, Hadoop, R, Machine Learning, Spark, Flume, Hdfs, Data Mining, Sas, Big, Data Science, Cloudera, Impala, Bigdata',\n",
       " 'Software Development, Core Java, Unit Testing, Customer Experience, Problem Solving, Communication Skills, Mysql, Data Science, Sales Management, Analytics',\n",
       " 'Machine Learning, Data Science, Product Management, New Product, Data Analysis, Computer Vision, Deep Learning, Python, Remote Sensing',\n",
       " 'consulting, Education Counseling, Educational Sales, Institutional Sales, pmp, Data Science, Business Development, Revenue Generation, Sales Achievement, new',\n",
       " 'Software Professionals, Engineering, Technical Management, Financial Management, Human Resource Management, Banking, Google Adwords, Business Analysis, It']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Recruiter_Skills\n",
    "r4=driver.find_elements_by_xpath(\"//div[@class='recInfo']/div[2]\")\n",
    "for i in r4:\n",
    "    try:\n",
    "        Recruiter_Skills.append(i.text)\n",
    "    except:\n",
    "        Recruiter_Skills.append(\"-\")\n",
    "Recruiter_Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Recruiter_Skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Delhi',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Pune',\n",
       " 'Ahmedabad',\n",
       " 'UK - (london)',\n",
       " 'Vadodara / Baroda',\n",
       " 'Chennai',\n",
       " 'Trivandrum',\n",
       " 'Indore',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Mumbai',\n",
       " 'Pune',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Bhopal',\n",
       " 'Navi Mumbai',\n",
       " 'Cochin',\n",
       " 'Delhi',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Pune',\n",
       " 'Pune',\n",
       " 'Pune',\n",
       " 'Pune',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'No Location available',\n",
       " 'Delhi',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Mysoru / Mysore',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Noida',\n",
       " 'New Delhi',\n",
       " 'Chennai',\n",
       " 'Aligarh',\n",
       " 'Salt Lake City',\n",
       " 'Pune',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Mumbai',\n",
       " 'Indore',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'MYSORE',\n",
       " 'Hyderabad / Secunderabad',\n",
       " 'Bengaluru / Bangalore',\n",
       " 'Mumbai',\n",
       " 'No Location available',\n",
       " 'Noida',\n",
       " 'Mumbai']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Recruiter_Location\n",
    "soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "locate=soup.find_all('div',attrs={'vcard'})\n",
    "for i in locate:\n",
    "    try:\n",
    "        Recruiter_Location.append(i.find('small').text)\n",
    "    except AttributeError :\n",
    "        Recruiter_Location.append(\"No Location available\")\n",
    "Recruiter_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Recruiter_Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recruiter Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills they hire for</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>Classic ASP Developer, Internet Marketing Prof...</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Shore Infotech India Pvt. Ltd</td>\n",
       "      <td>.Net, Java, Data Science, Linux Administration...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Enerlytics Software Solutions Pvt Ltd</td>\n",
       "      <td>Mean Stack, javascript, angularjs, mongodb, We...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subhas patel</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>LibraryXProject</td>\n",
       "      <td>Hadoop, Spark, Digital Strategy, Data Architec...</td>\n",
       "      <td>UK - (london)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abhishek - Only Analytics Hiring - India and</td>\n",
       "      <td>Recruitment Lead Consultant</td>\n",
       "      <td>Apidel Technologies Division of Transpower</td>\n",
       "      <td>Analytics, Business Intelligence, Business Ana...</td>\n",
       "      <td>Vadodara / Baroda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Institute for Financial Management and Resear</td>\n",
       "      <td>Programme Manager</td>\n",
       "      <td>IFMR</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balu Ramesh</td>\n",
       "      <td>HR Administrator</td>\n",
       "      <td>Techvantage Systems Pvt Ltd</td>\n",
       "      <td>Machine Learning, algorithms, Go Getter, Compu...</td>\n",
       "      <td>Trivandrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Asif Lucknowi</td>\n",
       "      <td>Director</td>\n",
       "      <td>Weupskill- Live Wire India</td>\n",
       "      <td>Technical Training, Software Development, Pres...</td>\n",
       "      <td>Indore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InstaFinancials</td>\n",
       "      <td>Human Resource</td>\n",
       "      <td>CBL Data Science Private Limited</td>\n",
       "      <td>Software Development, It Sales, Account Manage...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kalpana Dumpala</td>\n",
       "      <td>Executive Hiring</td>\n",
       "      <td>Innominds Software</td>\n",
       "      <td>Qa, Ui/ux, Java Developer, Java Architect, C++...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mubarak</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MoneyTap</td>\n",
       "      <td>Business Intelligence, Data Warehousing, Data ...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kushal Rastogi</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>QuantMagnum Technologies Pvt. Ltd.</td>\n",
       "      <td>Office Administration, Hr Administration, tele...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ruchi Dhote</td>\n",
       "      <td>Senior Executive Talent Acquisition</td>\n",
       "      <td>Bristlecone India Ltd</td>\n",
       "      <td>Qlikview, Qlik Sense, Microsoft Azure, Power B...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mahesh Babu Channa</td>\n",
       "      <td>HR Team Lead</td>\n",
       "      <td>SocialPrachar.com</td>\n",
       "      <td>Social Media, digital media maketing, seo, smm...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kapil Devang</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>BISP Solutions</td>\n",
       "      <td>Big Data, Hadoop, Data Analytics, Data Science</td>\n",
       "      <td>Bhopal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Manisha Yadav</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Easi Tax</td>\n",
       "      <td>Telecalling, Client Interaction, Marketing, Re...</td>\n",
       "      <td>Navi Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Riya Rajesh</td>\n",
       "      <td>Manager Talent Acquisition</td>\n",
       "      <td>Novelworx Digital Solutions</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Cochin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rashmi Bhattacharjee</td>\n",
       "      <td>HR Head</td>\n",
       "      <td>AXESTRACK SOFTWARE SOLUTIONS PRIVATE...</td>\n",
       "      <td>Corporate Sales, Software Development, Softwar...</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Faizan Kareem</td>\n",
       "      <td>HR MANAGER</td>\n",
       "      <td>FirstTech Consaltants Pvt.Ltd</td>\n",
       "      <td>Data Analytics, Data Science, Machine Learning...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rithika dadwal</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Affine Analytics</td>\n",
       "      <td>Data Science, Machine Learning, Python, R, Dee...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Azahar Shaikh</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>NEAL ANALYTICS SERVICES PVT LTD</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sandhya Khandagale</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Compumatrice Multimedia Pvt Ltd</td>\n",
       "      <td>Big Data, Data Science, Artificial Intelligenc...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Shaun Rao</td>\n",
       "      <td>Manager Human Resources</td>\n",
       "      <td>Exela Technologies</td>\n",
       "      <td>Java, Net, Angularjs, Hr, Infrastructure, Mana...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Manas</td>\n",
       "      <td>Lead Talent acquisition</td>\n",
       "      <td>Autumn Leaf Consulting Services Private...</td>\n",
       "      <td>Software Architecture, Vp Engineering, Product...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kumar</td>\n",
       "      <td>Proprietor</td>\n",
       "      <td>trainin</td>\n",
       "      <td>Data Science, Hadoop, Rpas, Devops, Python, Aw...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sunil Vedula</td>\n",
       "      <td>CEO</td>\n",
       "      <td>Nanoprecise Sci Corp</td>\n",
       "      <td>Signal Processing, Machine Learning, Neural Ne...</td>\n",
       "      <td>No Location available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Rajat Kumar</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>R.S Consultancy &amp;amp; Services</td>\n",
       "      <td>Web Technologies, Project Management, Software...</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Priya Khare</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>Independent Consultant</td>\n",
       "      <td>Data Science, Artificial Intelligence, analyti...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Dhruv Dev Dubey</td>\n",
       "      <td>Company Recruitment Head</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>Server Administartion, Verilog, Vhdl, Digital ...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Jayanth N</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>Dollarbird Information Services Pvt, Ltd</td>\n",
       "      <td>Data Analytics, Managed Services, Team Leading...</td>\n",
       "      <td>Mysoru / Mysore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SREEDHAR</td>\n",
       "      <td>Recruitment Consultant</td>\n",
       "      <td>JOBSMILL BUSINESS SOLUTIONS PRIVATE LIMITED</td>\n",
       "      <td>Data Science, Machine Learning, Big Data Analy...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Radha Manivasagam</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Techcovery</td>\n",
       "      <td>Python, Artificial Intelligence, Machine Learn...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Prateek Kumar</td>\n",
       "      <td>Head</td>\n",
       "      <td>Trisect</td>\n",
       "      <td>Java, Python, Angularjs, Software Testing, Mac...</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Amit Sharma</td>\n",
       "      <td>Consultant</td>\n",
       "      <td>ASCO consulting</td>\n",
       "      <td>Machine Learning, Artificial Intelligence, Dat...</td>\n",
       "      <td>New Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Kanan</td>\n",
       "      <td>senior technology instructor</td>\n",
       "      <td>NY INST</td>\n",
       "      <td>C, C++, Artificial Intelligence, Python, Php, ...</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Shashikant Chaudhary</td>\n",
       "      <td>HR Recruiter/HR Excutive</td>\n",
       "      <td>3D India Staffing Research &amp;amp; Consulting...</td>\n",
       "      <td>Relationship Management, Retail Sales, Private...</td>\n",
       "      <td>Aligarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Brad</td>\n",
       "      <td>Manager, Technical Recruiting</td>\n",
       "      <td>O.C. Tanner</td>\n",
       "      <td>Data Science, Software Engineering</td>\n",
       "      <td>Salt Lake City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rutuja Pawar</td>\n",
       "      <td>Technical Recruiter</td>\n",
       "      <td>Demand Matrix</td>\n",
       "      <td>Data Science, Big Data Analytics, Digital Mark...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Madhusudhan Sridhar</td>\n",
       "      <td>Erp Implementer</td>\n",
       "      <td>MADHUSUDHAN SRIDHAR</td>\n",
       "      <td>Data Science, Recruitment, Salary</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Ankit Sinha</td>\n",
       "      <td>Head Analytics</td>\n",
       "      <td>Suntech Global</td>\n",
       "      <td>B.Tech, Tableau, Statistics, R, Analytics, Tim...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Gaurav Chouhan</td>\n",
       "      <td>Chief Technical Officer</td>\n",
       "      <td>Strategic Consulting Lab</td>\n",
       "      <td>Software Development, Business Intelligence, B...</td>\n",
       "      <td>Indore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Rashi Kacker</td>\n",
       "      <td>Sr Product Manager</td>\n",
       "      <td>Impel Labs Pvt. Ltd.</td>\n",
       "      <td>Data Science, Node.js, Angularjs</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ashwini</td>\n",
       "      <td>Director Global Delivery</td>\n",
       "      <td>MRP Advisers</td>\n",
       "      <td>Data Science, Media Marketing, Resource Planni...</td>\n",
       "      <td>MYSORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Balaji Kolli</td>\n",
       "      <td>Co Founder</td>\n",
       "      <td>Saras Solutions India Pvt Ltd</td>\n",
       "      <td>Data Analysis, Learning, Data Science, Compute...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Rajani Nagaraj</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>WildJasmine</td>\n",
       "      <td>Java, Hadoop, R, Machine Learning, Spark, Flum...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ROHIT Kumar</td>\n",
       "      <td>Architect</td>\n",
       "      <td>LNT Private Limited</td>\n",
       "      <td>Software Development, Core Java, Unit Testing,...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Amir Chowdhury</td>\n",
       "      <td>Managing Partner</td>\n",
       "      <td>Granular.ai</td>\n",
       "      <td>Machine Learning, Data Science, Product Manage...</td>\n",
       "      <td>No Location available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Shailja Mishra</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Certybox Pvt.Ltd.</td>\n",
       "      <td>consulting, Education Counseling, Educational ...</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sunny Sharma</td>\n",
       "      <td>Managing Director - HR</td>\n",
       "      <td>Western Service Providers</td>\n",
       "      <td>Software Professionals, Engineering, Technical...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Recruiter Name  \\\n",
       "0                                    Aakash Harit   \n",
       "1                            shravan Kumar Gaddam   \n",
       "2                        MARSIAN Technologies LLP   \n",
       "3                                    Anik Agrawal   \n",
       "4                                    subhas patel   \n",
       "5    Abhishek - Only Analytics Hiring - India and   \n",
       "6   Institute for Financial Management and Resear   \n",
       "7                                     Balu Ramesh   \n",
       "8                                   Asif Lucknowi   \n",
       "9                                 InstaFinancials   \n",
       "10                                Kalpana Dumpala   \n",
       "11                                        Mubarak   \n",
       "12                                 Kushal Rastogi   \n",
       "13                                    Ruchi Dhote   \n",
       "14                             Mahesh Babu Channa   \n",
       "15                                   Kapil Devang   \n",
       "16                                  Manisha Yadav   \n",
       "17                                    Riya Rajesh   \n",
       "18                           Rashmi Bhattacharjee   \n",
       "19                                  Faizan Kareem   \n",
       "20                                 Rithika dadwal   \n",
       "21                                  Azahar Shaikh   \n",
       "22                             Sandhya Khandagale   \n",
       "23                                      Shaun Rao   \n",
       "24                                          Manas   \n",
       "25                                          kumar   \n",
       "26                                   Sunil Vedula   \n",
       "27                                    Rajat Kumar   \n",
       "28                                    Priya Khare   \n",
       "29                                Dhruv Dev Dubey   \n",
       "30                                      Jayanth N   \n",
       "31                                       SREEDHAR   \n",
       "32                              Radha Manivasagam   \n",
       "33                                  Prateek Kumar   \n",
       "34                                    Amit Sharma   \n",
       "35                                          Kanan   \n",
       "36                           Shashikant Chaudhary   \n",
       "37                                           Brad   \n",
       "38                                   Rutuja Pawar   \n",
       "39                            Madhusudhan Sridhar   \n",
       "40                                    Ankit Sinha   \n",
       "41                                 Gaurav Chouhan   \n",
       "42                                   Rashi Kacker   \n",
       "43                                        Ashwini   \n",
       "44                                   Balaji Kolli   \n",
       "45                                 Rajani Nagaraj   \n",
       "46                                    ROHIT Kumar   \n",
       "47                                 Amir Chowdhury   \n",
       "48                                 Shailja Mishra   \n",
       "49                                   Sunny Sharma   \n",
       "\n",
       "                            Designation  \\\n",
       "0                            HR Manager   \n",
       "1                     Company Recruiter   \n",
       "2                            Company HR   \n",
       "3                     Company Recruiter   \n",
       "4                           Founder CEO   \n",
       "5           Recruitment Lead Consultant   \n",
       "6                     Programme Manager   \n",
       "7                      HR Administrator   \n",
       "8                              Director   \n",
       "9                        Human Resource   \n",
       "10                     Executive Hiring   \n",
       "11                           Company HR   \n",
       "12                           Company HR   \n",
       "13  Senior Executive Talent Acquisition   \n",
       "14                         HR Team Lead   \n",
       "15                           HR Manager   \n",
       "16                         HR Executive   \n",
       "17           Manager Talent Acquisition   \n",
       "18                              HR Head   \n",
       "19                           HR MANAGER   \n",
       "20                         HR Recruiter   \n",
       "21                    Company Recruiter   \n",
       "22                         HR Recruiter   \n",
       "23              Manager Human Resources   \n",
       "24              Lead Talent acquisition   \n",
       "25                           Proprietor   \n",
       "26                                  CEO   \n",
       "27                          Founder CEO   \n",
       "28                       Senior Manager   \n",
       "29             Company Recruitment Head   \n",
       "30                      Project Manager   \n",
       "31               Recruitment Consultant   \n",
       "32                         HR Executive   \n",
       "33                                 Head   \n",
       "34                           Consultant   \n",
       "35         senior technology instructor   \n",
       "36             HR Recruiter/HR Excutive   \n",
       "37        Manager, Technical Recruiting   \n",
       "38                  Technical Recruiter   \n",
       "39                      Erp Implementer   \n",
       "40                       Head Analytics   \n",
       "41              Chief Technical Officer   \n",
       "42                   Sr Product Manager   \n",
       "43             Director Global Delivery   \n",
       "44                           Co Founder   \n",
       "45                           HR Manager   \n",
       "46                            Architect   \n",
       "47                     Managing Partner   \n",
       "48                           HR Manager   \n",
       "49               Managing Director - HR   \n",
       "\n",
       "                                           Company  \\\n",
       "0                             Data Science Network   \n",
       "1                    Shore Infotech India Pvt. Ltd   \n",
       "2                         MARSIAN Technologies LLP   \n",
       "3            Enerlytics Software Solutions Pvt Ltd   \n",
       "4                                  LibraryXProject   \n",
       "5       Apidel Technologies Division of Transpower   \n",
       "6                                             IFMR   \n",
       "7                      Techvantage Systems Pvt Ltd   \n",
       "8                       Weupskill- Live Wire India   \n",
       "9                 CBL Data Science Private Limited   \n",
       "10                              Innominds Software   \n",
       "11                                        MoneyTap   \n",
       "12              QuantMagnum Technologies Pvt. Ltd.   \n",
       "13                           Bristlecone India Ltd   \n",
       "14                               SocialPrachar.com   \n",
       "15                                  BISP Solutions   \n",
       "16                                        Easi Tax   \n",
       "17                     Novelworx Digital Solutions   \n",
       "18         AXESTRACK SOFTWARE SOLUTIONS PRIVATE...   \n",
       "19                   FirstTech Consaltants Pvt.Ltd   \n",
       "20                                Affine Analytics   \n",
       "21                 NEAL ANALYTICS SERVICES PVT LTD   \n",
       "22                 Compumatrice Multimedia Pvt Ltd   \n",
       "23                              Exela Technologies   \n",
       "24      Autumn Leaf Consulting Services Private...   \n",
       "25                                         trainin   \n",
       "26                            Nanoprecise Sci Corp   \n",
       "27                  R.S Consultancy &amp; Services   \n",
       "28                          Independent Consultant   \n",
       "29                                    Confidential   \n",
       "30        Dollarbird Information Services Pvt, Ltd   \n",
       "31     JOBSMILL BUSINESS SOLUTIONS PRIVATE LIMITED   \n",
       "32                                      Techcovery   \n",
       "33                                         Trisect   \n",
       "34                                 ASCO consulting   \n",
       "35                                         NY INST   \n",
       "36  3D India Staffing Research &amp; Consulting...   \n",
       "37                                     O.C. Tanner   \n",
       "38                                   Demand Matrix   \n",
       "39                             MADHUSUDHAN SRIDHAR   \n",
       "40                                  Suntech Global   \n",
       "41                        Strategic Consulting Lab   \n",
       "42                            Impel Labs Pvt. Ltd.   \n",
       "43                                    MRP Advisers   \n",
       "44                   Saras Solutions India Pvt Ltd   \n",
       "45                                     WildJasmine   \n",
       "46                             LNT Private Limited   \n",
       "47                                     Granular.ai   \n",
       "48                               Certybox Pvt.Ltd.   \n",
       "49                       Western Service Providers   \n",
       "\n",
       "                                 Skills they hire for  \\\n",
       "0   Classic ASP Developer, Internet Marketing Prof...   \n",
       "1   .Net, Java, Data Science, Linux Administration...   \n",
       "2   Data Science, Artificial Intelligence, Machine...   \n",
       "3   Mean Stack, javascript, angularjs, mongodb, We...   \n",
       "4   Hadoop, Spark, Digital Strategy, Data Architec...   \n",
       "5   Analytics, Business Intelligence, Business Ana...   \n",
       "6                                        Data Science   \n",
       "7   Machine Learning, algorithms, Go Getter, Compu...   \n",
       "8   Technical Training, Software Development, Pres...   \n",
       "9   Software Development, It Sales, Account Manage...   \n",
       "10  Qa, Ui/ux, Java Developer, Java Architect, C++...   \n",
       "11  Business Intelligence, Data Warehousing, Data ...   \n",
       "12  Office Administration, Hr Administration, tele...   \n",
       "13  Qlikview, Qlik Sense, Microsoft Azure, Power B...   \n",
       "14  Social Media, digital media maketing, seo, smm...   \n",
       "15     Big Data, Hadoop, Data Analytics, Data Science   \n",
       "16  Telecalling, Client Interaction, Marketing, Re...   \n",
       "17                                       Data Science   \n",
       "18  Corporate Sales, Software Development, Softwar...   \n",
       "19  Data Analytics, Data Science, Machine Learning...   \n",
       "20  Data Science, Machine Learning, Python, R, Dee...   \n",
       "21  Data Science, Artificial Intelligence, Machine...   \n",
       "22  Big Data, Data Science, Artificial Intelligenc...   \n",
       "23  Java, Net, Angularjs, Hr, Infrastructure, Mana...   \n",
       "24  Software Architecture, Vp Engineering, Product...   \n",
       "25  Data Science, Hadoop, Rpas, Devops, Python, Aw...   \n",
       "26  Signal Processing, Machine Learning, Neural Ne...   \n",
       "27  Web Technologies, Project Management, Software...   \n",
       "28  Data Science, Artificial Intelligence, analyti...   \n",
       "29  Server Administartion, Verilog, Vhdl, Digital ...   \n",
       "30  Data Analytics, Managed Services, Team Leading...   \n",
       "31  Data Science, Machine Learning, Big Data Analy...   \n",
       "32  Python, Artificial Intelligence, Machine Learn...   \n",
       "33  Java, Python, Angularjs, Software Testing, Mac...   \n",
       "34  Machine Learning, Artificial Intelligence, Dat...   \n",
       "35  C, C++, Artificial Intelligence, Python, Php, ...   \n",
       "36  Relationship Management, Retail Sales, Private...   \n",
       "37                 Data Science, Software Engineering   \n",
       "38  Data Science, Big Data Analytics, Digital Mark...   \n",
       "39                  Data Science, Recruitment, Salary   \n",
       "40  B.Tech, Tableau, Statistics, R, Analytics, Tim...   \n",
       "41  Software Development, Business Intelligence, B...   \n",
       "42                   Data Science, Node.js, Angularjs   \n",
       "43  Data Science, Media Marketing, Resource Planni...   \n",
       "44  Data Analysis, Learning, Data Science, Compute...   \n",
       "45  Java, Hadoop, R, Machine Learning, Spark, Flum...   \n",
       "46  Software Development, Core Java, Unit Testing,...   \n",
       "47  Machine Learning, Data Science, Product Manage...   \n",
       "48  consulting, Education Counseling, Educational ...   \n",
       "49  Software Professionals, Engineering, Technical...   \n",
       "\n",
       "                    Location  \n",
       "0                      Delhi  \n",
       "1   Hyderabad / Secunderabad  \n",
       "2                       Pune  \n",
       "3                  Ahmedabad  \n",
       "4              UK - (london)  \n",
       "5          Vadodara / Baroda  \n",
       "6                    Chennai  \n",
       "7                 Trivandrum  \n",
       "8                     Indore  \n",
       "9      Bengaluru / Bangalore  \n",
       "10  Hyderabad / Secunderabad  \n",
       "11     Bengaluru / Bangalore  \n",
       "12                    Mumbai  \n",
       "13                      Pune  \n",
       "14  Hyderabad / Secunderabad  \n",
       "15                    Bhopal  \n",
       "16               Navi Mumbai  \n",
       "17                    Cochin  \n",
       "18                     Delhi  \n",
       "19  Hyderabad / Secunderabad  \n",
       "20                      Pune  \n",
       "21                      Pune  \n",
       "22                      Pune  \n",
       "23                      Pune  \n",
       "24     Bengaluru / Bangalore  \n",
       "25     Bengaluru / Bangalore  \n",
       "26     No Location available  \n",
       "27                     Delhi  \n",
       "28     Bengaluru / Bangalore  \n",
       "29     Bengaluru / Bangalore  \n",
       "30           Mysoru / Mysore  \n",
       "31  Hyderabad / Secunderabad  \n",
       "32     Bengaluru / Bangalore  \n",
       "33                     Noida  \n",
       "34                 New Delhi  \n",
       "35                   Chennai  \n",
       "36                   Aligarh  \n",
       "37            Salt Lake City  \n",
       "38                      Pune  \n",
       "39     Bengaluru / Bangalore  \n",
       "40                    Mumbai  \n",
       "41                    Indore  \n",
       "42     Bengaluru / Bangalore  \n",
       "43                    MYSORE  \n",
       "44  Hyderabad / Secunderabad  \n",
       "45     Bengaluru / Bangalore  \n",
       "46                    Mumbai  \n",
       "47     No Location available  \n",
       "48                     Noida  \n",
       "49                    Mumbai  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "recruiters_ds=pd.DataFrame({})\n",
    "recruiters_ds['Recruiter Name']=Recruiter_name\n",
    "recruiters_ds['Designation']=Recruiter_Designation\n",
    "recruiters_ds['Company']=Recruiter_Company\n",
    "recruiters_ds['Skills they hire for']=Recruiter_Skills\n",
    "recruiters_ds['Location']=Recruiter_Location\n",
    "recruiters_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************************************************************************************************************************************************************************Completed******************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/.\n",
    "    \n",
    "You have to find the following details:\n",
    "    \n",
    "A) Book name\n",
    "\n",
    "B) Author name\n",
    "\n",
    "C) Volumes sold\n",
    "\n",
    "D) Publisher\n",
    "\n",
    "E) Genre\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "url=\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher_info=[]\n",
    "Genre_info=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Da Vinci Code,The',\n",
       " 'Harry Potter and the Deathly Hallows',\n",
       " \"Harry Potter and the Philosopher's Stone\",\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " 'Fifty Shades of Grey',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban',\n",
       " 'Angels and Demons',\n",
       " \"Harry Potter and the Half-blood Prince:Children's Edition\",\n",
       " 'Fifty Shades Darker',\n",
       " 'Twilight',\n",
       " 'Girl with the Dragon Tattoo,The:Millennium Trilogy',\n",
       " 'Fifty Shades Freed',\n",
       " 'Lost Symbol,The',\n",
       " 'New Moon',\n",
       " 'Deception Point',\n",
       " 'Eclipse',\n",
       " 'Lovely Bones,The',\n",
       " 'Curious Incident of the Dog in the Night-time,The',\n",
       " 'Digital Fortress',\n",
       " 'Short History of Nearly Everything,A',\n",
       " 'Girl Who Played with Fire,The:Millennium Trilogy',\n",
       " 'Breaking Dawn',\n",
       " 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar',\n",
       " 'Gruffalo,The',\n",
       " \"Jamie's 30-Minute Meals\",\n",
       " 'Kite Runner,The',\n",
       " 'One Day',\n",
       " 'Thousand Splendid Suns,A',\n",
       " \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\",\n",
       " \"Time Traveler's Wife,The\",\n",
       " 'Atonement',\n",
       " \"Bridget Jones's Diary:A Novel\",\n",
       " 'World According to Clarkson,The',\n",
       " \"Captain Corelli's Mandolin\",\n",
       " 'Sound of Laughter,The',\n",
       " 'Life of Pi',\n",
       " 'Billy Connolly',\n",
       " 'Child Called It,A',\n",
       " \"Gruffalo's Child,The\",\n",
       " \"Angela's Ashes:A Memoir of a Childhood\",\n",
       " 'Birdsong',\n",
       " 'Northern Lights:His Dark Materials S.',\n",
       " 'Labyrinth',\n",
       " 'Harry Potter and the Half-blood Prince',\n",
       " 'Help,The',\n",
       " 'Man and Boy',\n",
       " 'Memoirs of a Geisha',\n",
       " \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\",\n",
       " 'Island,The',\n",
       " 'PS, I Love You',\n",
       " 'You are What You Eat:The Plan That Will Change Your Life',\n",
       " 'Shadow of the Wind,The',\n",
       " 'Tales of Beedle the Bard,The',\n",
       " 'Broker,The',\n",
       " \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\",\n",
       " 'Subtle Knife,The:His Dark Materials S.',\n",
       " 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation',\n",
       " \"Delia's How to Cook:(Bk.1)\",\n",
       " 'Chocolat',\n",
       " 'Boy in the Striped Pyjamas,The',\n",
       " \"My Sister's Keeper\",\n",
       " 'Amber Spyglass,The:His Dark Materials S.',\n",
       " 'To Kill a Mockingbird',\n",
       " 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin',\n",
       " 'Dear Fatty',\n",
       " 'Short History of Tractors in Ukrainian,A',\n",
       " 'Hannibal',\n",
       " 'Lord of the Rings,The',\n",
       " 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio',\n",
       " 'Interpretation of Murder,The',\n",
       " 'Sharon Osbourne Extreme:My Autobiography',\n",
       " 'Alchemist,The:A Fable About Following Your Dream',\n",
       " \"At My Mother's Knee ...:and Other Low Joints\",\n",
       " 'Notes from a Small Island',\n",
       " 'Return of the Naked Chef,The',\n",
       " 'Bridget Jones: The Edge of Reason',\n",
       " \"Jamie's Italy\",\n",
       " 'I Can Make You Thin',\n",
       " 'Down Under',\n",
       " 'Summons,The',\n",
       " 'Small Island',\n",
       " 'Nigella Express',\n",
       " 'Brick Lane',\n",
       " \"Memory Keeper's Daughter,The\",\n",
       " 'Room on the Broom',\n",
       " 'About a Boy',\n",
       " 'My Booky Wook',\n",
       " 'God Delusion,The',\n",
       " '\"Beano\" Annual,The',\n",
       " 'White Teeth',\n",
       " 'House at Riverton,The',\n",
       " 'Book Thief,The',\n",
       " 'Nights of Rain and Stars',\n",
       " 'Ghost,The',\n",
       " 'Happy Days with the Naked Chef',\n",
       " 'Hunger Games,The:Hunger Games Trilogy',\n",
       " \"Lost Boy,The:A Foster Child's Search for the Love of a Family\",\n",
       " \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Book_name\n",
    "b1=driver.find_elements_by_xpath(\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[2]\")\n",
    "for i in b1:\n",
    "    try:\n",
    "        Book_name.append(i.text)\n",
    "    except:\n",
    "        Book_name.append(\"-\")\n",
    "Book_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brown, Dan',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'James, E. L.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Brown, Dan',\n",
       " 'Rowling, J.K.',\n",
       " 'James, E. L.',\n",
       " 'Meyer, Stephenie',\n",
       " 'Larsson, Stieg',\n",
       " 'James, E. L.',\n",
       " 'Brown, Dan',\n",
       " 'Meyer, Stephenie',\n",
       " 'Brown, Dan',\n",
       " 'Meyer, Stephenie',\n",
       " 'Sebold, Alice',\n",
       " 'Haddon, Mark',\n",
       " 'Brown, Dan',\n",
       " 'Bryson, Bill',\n",
       " 'Larsson, Stieg',\n",
       " 'Meyer, Stephenie',\n",
       " 'Carle, Eric',\n",
       " 'Donaldson, Julia',\n",
       " 'Oliver, Jamie',\n",
       " 'Hosseini, Khaled',\n",
       " 'Nicholls, David',\n",
       " 'Hosseini, Khaled',\n",
       " 'Larsson, Stieg',\n",
       " 'Niffenegger, Audrey',\n",
       " 'McEwan, Ian',\n",
       " 'Fielding, Helen',\n",
       " 'Clarkson, Jeremy',\n",
       " 'Bernieres, Louis de',\n",
       " 'Kay, Peter',\n",
       " 'Martel, Yann',\n",
       " 'Stephenson, Pamela',\n",
       " 'Pelzer, Dave',\n",
       " 'Donaldson, Julia',\n",
       " 'McCourt, Frank',\n",
       " 'Faulks, Sebastian',\n",
       " 'Pullman, Philip',\n",
       " 'Mosse, Kate',\n",
       " 'Rowling, J.K.',\n",
       " 'Stockett, Kathryn',\n",
       " 'Parsons, Tony',\n",
       " 'Golden, Arthur',\n",
       " 'McCall Smith, Alexander',\n",
       " 'Hislop, Victoria',\n",
       " 'Ahern, Cecelia',\n",
       " 'McKeith, Gillian',\n",
       " 'Zafon, Carlos Ruiz',\n",
       " 'Rowling, J.K.',\n",
       " 'Grisham, John',\n",
       " 'Atkins, Robert C.',\n",
       " 'Pullman, Philip',\n",
       " 'Truss, Lynne',\n",
       " 'Smith, Delia',\n",
       " 'Harris, Joanne',\n",
       " 'Boyne, John',\n",
       " 'Picoult, Jodi',\n",
       " 'Pullman, Philip',\n",
       " 'Lee, Harper',\n",
       " 'Gray, John',\n",
       " 'French, Dawn',\n",
       " 'Lewycka, Marina',\n",
       " 'Harris, Thomas',\n",
       " 'Tolkien, J. R. R.',\n",
       " 'Moore, Michael',\n",
       " 'Rubenfeld, Jed',\n",
       " 'Osbourne, Sharon',\n",
       " 'Coelho, Paulo',\n",
       " \"O'Grady, Paul\",\n",
       " 'Bryson, Bill',\n",
       " 'Oliver, Jamie',\n",
       " 'Fielding, Helen',\n",
       " 'Oliver, Jamie',\n",
       " 'McKenna, Paul',\n",
       " 'Bryson, Bill',\n",
       " 'Grisham, John',\n",
       " 'Levy, Andrea',\n",
       " 'Lawson, Nigella',\n",
       " 'Ali, Monica',\n",
       " 'Edwards, Kim',\n",
       " 'Donaldson, Julia',\n",
       " 'Hornby, Nick',\n",
       " 'Brand, Russell',\n",
       " 'Dawkins, Richard',\n",
       " '0',\n",
       " 'Smith, Zadie',\n",
       " 'Morton, Kate',\n",
       " 'Zusak, Markus',\n",
       " 'Binchy, Maeve',\n",
       " 'Harris, Robert',\n",
       " 'Oliver, Jamie',\n",
       " 'Collins, Suzanne',\n",
       " 'Pelzer, Dave',\n",
       " 'Oliver, Jamie']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Author_name\n",
    "b2=driver.find_elements_by_xpath(\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[3]\")\n",
    "for i in b2:\n",
    "    try:\n",
    "        Author_name.append(i.text)\n",
    "    except:\n",
    "        Author_name.append(\"-\")\n",
    "Author_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5,094,805',\n",
       " '4,475,152',\n",
       " '4,200,654',\n",
       " '4,179,479',\n",
       " '3,758,936',\n",
       " '3,583,215',\n",
       " '3,484,047',\n",
       " '3,377,906',\n",
       " '3,193,946',\n",
       " '2,950,264',\n",
       " '2,479,784',\n",
       " '2,315,405',\n",
       " '2,233,570',\n",
       " '2,193,928',\n",
       " '2,183,031',\n",
       " '2,152,737',\n",
       " '2,062,145',\n",
       " '2,052,876',\n",
       " '2,005,598',\n",
       " '1,979,552',\n",
       " '1,928,900',\n",
       " '1,852,919',\n",
       " '1,814,784',\n",
       " '1,787,118',\n",
       " '1,783,535',\n",
       " '1,781,269',\n",
       " '1,743,266',\n",
       " '1,629,119',\n",
       " '1,616,068',\n",
       " '1,583,992',\n",
       " '1,555,135',\n",
       " '1,546,886',\n",
       " '1,539,428',\n",
       " '1,508,205',\n",
       " '1,489,403',\n",
       " '1,352,318',\n",
       " '1,310,207',\n",
       " '1,310,176',\n",
       " '1,231,957',\n",
       " '1,217,712',\n",
       " '1,208,711',\n",
       " '1,204,058',\n",
       " '1,184,967',\n",
       " '1,181,503',\n",
       " '1,181,093',\n",
       " '1,153,181',\n",
       " '1,132,336',\n",
       " '1,130,802',\n",
       " '1,126,337',\n",
       " '1,115,549',\n",
       " '1,108,328',\n",
       " '1,107,379',\n",
       " '1,104,403',\n",
       " '1,092,349',\n",
       " '1,090,847',\n",
       " '1,087,262',\n",
       " '1,054,196',\n",
       " '1,037,160',\n",
       " '1,023,688',\n",
       " '1,015,956',\n",
       " '1,009,873',\n",
       " '1,004,414',\n",
       " '1,003,780',\n",
       " '1,002,314',\n",
       " '998,213',\n",
       " '992,846',\n",
       " '986,753',\n",
       " '986,115',\n",
       " '970,509',\n",
       " '967,466',\n",
       " '963,353',\n",
       " '962,515',\n",
       " '959,496',\n",
       " '956,114',\n",
       " '945,640',\n",
       " '931,312',\n",
       " '925,425',\n",
       " '924,695',\n",
       " '906,968',\n",
       " '905,086',\n",
       " '890,847',\n",
       " '869,671',\n",
       " '869,659',\n",
       " '862,602',\n",
       " '856,540',\n",
       " '845,858',\n",
       " '842,535',\n",
       " '828,215',\n",
       " '820,563',\n",
       " '816,907',\n",
       " '816,585',\n",
       " '815,586',\n",
       " '814,370',\n",
       " '809,641',\n",
       " '808,900',\n",
       " '807,311',\n",
       " '794,201',\n",
       " '792,187',\n",
       " '791,507',\n",
       " '791,095']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Volumes_sold\n",
    "b3=driver.find_elements_by_xpath(\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[4]\")\n",
    "for i in b3:\n",
    "    try:\n",
    "        Volumes_sold.append(i.text)\n",
    "    except:\n",
    "        Volumes_sold.append(\"-\")\n",
    "Volumes_sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Volumes_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transworld',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Transworld',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Little, Brown Book',\n",
       " 'Quercus',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Little, Brown Book',\n",
       " 'Transworld',\n",
       " 'Little, Brown Book',\n",
       " 'Pan Macmillan',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Quercus',\n",
       " 'Little, Brown Book',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Bloomsbury',\n",
       " 'Hodder & Stoughton',\n",
       " 'Bloomsbury',\n",
       " 'Quercus',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Canongate',\n",
       " 'HarperCollins',\n",
       " 'Orion',\n",
       " 'Pan Macmillan',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Scholastic Ltd.',\n",
       " 'Orion',\n",
       " 'Bloomsbury',\n",
       " 'Penguin',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Little, Brown Book',\n",
       " 'Headline',\n",
       " 'HarperCollins',\n",
       " 'Penguin',\n",
       " 'Orion',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Scholastic Ltd.',\n",
       " 'Profile Books Group',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Random House Childrens Books G',\n",
       " 'Hodder & Stoughton',\n",
       " 'Scholastic Ltd.',\n",
       " 'Random House',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Penguin',\n",
       " 'Random House',\n",
       " 'HarperCollins',\n",
       " 'Penguin',\n",
       " 'Headline',\n",
       " 'Little, Brown Book',\n",
       " 'HarperCollins',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Random House',\n",
       " 'Headline',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Hodder & Stoughton',\n",
       " 'Transworld',\n",
       " 'D.C. Thomson',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Transworld',\n",
       " 'Orion',\n",
       " 'Random House',\n",
       " 'Penguin',\n",
       " 'Scholastic Ltd.',\n",
       " 'Orion',\n",
       " 'Penguin']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Publisher_info\n",
    "b4=driver.find_elements_by_xpath(\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[5]\")\n",
    "for i in b4:\n",
    "    try:\n",
    "        Publisher_info.append(i.text)\n",
    "    except:\n",
    "        Publisher_info.append(\"-\")\n",
    "Publisher_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Publisher_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crime, Thriller & Adventure',\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " 'Romance & Sagas',\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " 'Crime, Thriller & Adventure',\n",
       " \"Children's Fiction\",\n",
       " 'Romance & Sagas',\n",
       " 'Young Adult Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Romance & Sagas',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Popular Science',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'Picture Books',\n",
       " 'Picture Books',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Humour: Collections & General',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Biography: The Arts',\n",
       " 'Autobiography: General',\n",
       " 'Picture Books',\n",
       " 'Autobiography: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Science Fiction & Fantasy',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Fitness & Diet',\n",
       " 'General & Literary Fiction',\n",
       " \"Children's Fiction\",\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Fitness & Diet',\n",
       " 'Young Adult Fiction',\n",
       " 'Usage & Writing Guides',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Popular Culture & Media: General Interest',\n",
       " 'Autobiography: The Arts',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Science Fiction & Fantasy',\n",
       " 'Current Affairs & Issues',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Autobiography: The Arts',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: The Arts',\n",
       " 'Travel Writing',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'National & Regional Cuisine',\n",
       " 'Fitness & Diet',\n",
       " 'Travel Writing',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Picture Books',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: The Arts',\n",
       " 'Popular Science',\n",
       " \"Children's Annuals\",\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Food & Drink: General',\n",
       " 'Young Adult Fiction',\n",
       " 'Biography: General',\n",
       " 'Food & Drink: General']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Genre_info\n",
    "b5=driver.find_elements_by_xpath(\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[6]\")\n",
    "for i in b5:\n",
    "    try:\n",
    "        Genre_info.append(i.text)\n",
    "    except:\n",
    "        Genre_info.append(\"-\")\n",
    "Genre_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Genre_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher Info</th>\n",
       "      <th>Genre Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes Sold   Publisher Info                   Genre Info  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "top_novels=pd.DataFrame({})\n",
    "top_novels['Book Name']=Book_name\n",
    "top_novels['Author Name']=Author_name\n",
    "top_novels['Volumes Sold']=Volumes_sold\n",
    "top_novels['Publisher Info']=Publisher_info\n",
    "top_novels['Genre Info']=Genre_info\n",
    "top_novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************************************************Completed*********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/.\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Name\n",
    "\n",
    "B) Year span\n",
    "\n",
    "C) Genre\n",
    "\n",
    "D) Run time\n",
    "\n",
    "E) Ratings\n",
    "\n",
    "F) Votes\n",
    "\n",
    "Ans: Solution is as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage https://www.imdb.com/list/ls095964455/\n",
    "url=\"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "TvSeries_Name=[]\n",
    "TvSeries_YearSpan=[]\n",
    "TvSeries_Genre=[]\n",
    "TvSeries_RunTime=[]\n",
    "TvSeries_Ratings=[]\n",
    "TvSeries_Votes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Game of Thrones',\n",
       " 'Stranger Things',\n",
       " 'The Walking Dead',\n",
       " '13 Reasons Why',\n",
       " 'The 100',\n",
       " 'Orange Is the New Black',\n",
       " 'Riverdale',\n",
       " \"Grey's Anatomy\",\n",
       " 'The Flash',\n",
       " 'Arrow',\n",
       " 'Money Heist',\n",
       " 'The Big Bang Theory',\n",
       " 'Black Mirror',\n",
       " 'Sherlock',\n",
       " 'Vikings',\n",
       " 'Pretty Little Liars',\n",
       " 'The Vampire Diaries',\n",
       " 'American Horror Story',\n",
       " 'Breaking Bad',\n",
       " 'Lucifer',\n",
       " 'Supernatural',\n",
       " 'Prison Break',\n",
       " 'How to Get Away with Murder',\n",
       " 'Teen Wolf',\n",
       " 'The Simpsons',\n",
       " 'Once Upon a Time',\n",
       " 'Narcos',\n",
       " 'Daredevil',\n",
       " 'Friends',\n",
       " 'How I Met Your Mother',\n",
       " 'Suits',\n",
       " 'Mr. Robot',\n",
       " 'The Originals',\n",
       " 'Supergirl',\n",
       " 'Gossip Girl',\n",
       " 'Sense8',\n",
       " 'Gotham',\n",
       " 'Westworld',\n",
       " 'Jessica Jones',\n",
       " 'Modern Family',\n",
       " 'Rick and Morty',\n",
       " 'Shadowhunters',\n",
       " 'The End of the F***ing World',\n",
       " 'House of Cards',\n",
       " 'Dark',\n",
       " 'Elite',\n",
       " 'Sex Education',\n",
       " 'Shameless',\n",
       " 'New Girl',\n",
       " 'Agents of S.H.I.E.L.D.',\n",
       " 'You',\n",
       " 'Dexter',\n",
       " 'Fear the Walking Dead',\n",
       " 'Family Guy',\n",
       " 'The Blacklist',\n",
       " 'Lost',\n",
       " 'Peaky Blinders',\n",
       " 'House',\n",
       " 'Quantico',\n",
       " 'Orphan Black',\n",
       " 'Homeland',\n",
       " 'Blindspot',\n",
       " \"DC's Legends of Tomorrow\",\n",
       " \"The Handmaid's Tale\",\n",
       " 'Chilling Adventures of Sabrina',\n",
       " 'The Good Doctor',\n",
       " 'Jane the Virgin',\n",
       " 'Glee',\n",
       " 'South Park',\n",
       " 'Brooklyn Nine-Nine',\n",
       " 'Under the Dome',\n",
       " 'The Umbrella Academy',\n",
       " 'True Detective',\n",
       " 'The OA',\n",
       " 'Desperate Housewives',\n",
       " 'Better Call Saul',\n",
       " 'Bates Motel',\n",
       " 'The Punisher',\n",
       " 'Atypical',\n",
       " 'Dynasty',\n",
       " 'This Is Us',\n",
       " 'The Good Place',\n",
       " 'Iron Fist',\n",
       " 'The Rain',\n",
       " 'Mindhunter',\n",
       " 'Revenge',\n",
       " 'Luke Cage',\n",
       " 'Scandal',\n",
       " 'The Defenders',\n",
       " 'Big Little Lies',\n",
       " 'Insatiable',\n",
       " 'The Mentalist',\n",
       " 'The Crown',\n",
       " 'Chernobyl',\n",
       " 'iZombie',\n",
       " 'Reign',\n",
       " 'A Series of Unfortunate Events',\n",
       " 'Criminal Minds',\n",
       " 'Scream: The TV Series',\n",
       " 'The Haunting of Hill House']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_Name\n",
    "t1=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/h3/a\")\n",
    "for i in t1:\n",
    "    try:\n",
    "        TvSeries_Name.append(i.text)\n",
    "    except:\n",
    "        TvSeries_Name.append(\"-\")\n",
    "TvSeries_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2011–2019)',\n",
       " '(2016– )',\n",
       " '(2010–2022)',\n",
       " '(2017–2020)',\n",
       " '(2014–2020)',\n",
       " '(2013–2019)',\n",
       " '(2017– )',\n",
       " '(2005– )',\n",
       " '(2014– )',\n",
       " '(2012–2020)',\n",
       " '(2017–2021)',\n",
       " '(2007–2019)',\n",
       " '(2011– )',\n",
       " '(2010–2017)',\n",
       " '(2013–2020)',\n",
       " '(2010–2017)',\n",
       " '(2009–2017)',\n",
       " '(2011– )',\n",
       " '(2008–2013)',\n",
       " '(2016– )',\n",
       " '(2005–2020)',\n",
       " '(2005–2017)',\n",
       " '(2014–2020)',\n",
       " '(2011–2017)',\n",
       " '(1989– )',\n",
       " '(2011–2018)',\n",
       " '(2015–2017)',\n",
       " '(2015–2018)',\n",
       " '(1994–2004)',\n",
       " '(2005–2014)',\n",
       " '(2011–2019)',\n",
       " '(2015–2019)',\n",
       " '(2013–2018)',\n",
       " '(2015–2021)',\n",
       " '(2007–2012)',\n",
       " '(2015–2018)',\n",
       " '(2014–2019)',\n",
       " '(2016– )',\n",
       " '(2015–2019)',\n",
       " '(2009–2020)',\n",
       " '(2013– )',\n",
       " '(2016–2019)',\n",
       " '(2017–2019)',\n",
       " '(2013–2018)',\n",
       " '(2017–2020)',\n",
       " '(2018– )',\n",
       " '(2019– )',\n",
       " '(2011–2021)',\n",
       " '(2011–2018)',\n",
       " '(2013–2020)',\n",
       " '(2018– )',\n",
       " '(2006–2021)',\n",
       " '(2015– )',\n",
       " '(1999– )',\n",
       " '(2013– )',\n",
       " '(2004–2010)',\n",
       " '(2013– )',\n",
       " '(2004–2012)',\n",
       " '(2015–2018)',\n",
       " '(2013–2017)',\n",
       " '(2011–2020)',\n",
       " '(2015–2020)',\n",
       " '(2016– )',\n",
       " '(2017– )',\n",
       " '(2018–2020)',\n",
       " '(2017– )',\n",
       " '(2014–2019)',\n",
       " '(2009–2015)',\n",
       " '(1997– )',\n",
       " '(2013–2022)',\n",
       " '(2013–2015)',\n",
       " '(2019– )',\n",
       " '(2014–2019)',\n",
       " '(2016–2019)',\n",
       " '(2004–2012)',\n",
       " '(2015– )',\n",
       " '(2013–2017)',\n",
       " '(2017–2019)',\n",
       " '(2017–2021)',\n",
       " '(2017– )',\n",
       " '(2016–2022)',\n",
       " '(2016–2020)',\n",
       " '(2017–2018)',\n",
       " '(2018–2020)',\n",
       " '(2017–2019)',\n",
       " '(2011–2015)',\n",
       " '(2016–2018)',\n",
       " '(2012–2018)',\n",
       " '(2017)',\n",
       " '(2017–2019)',\n",
       " '(2018–2019)',\n",
       " '(2008–2015)',\n",
       " '(2016– )',\n",
       " '(2019)',\n",
       " '(2015–2019)',\n",
       " '(2013–2017)',\n",
       " '(2017–2019)',\n",
       " '(2005–2020)',\n",
       " '(2015–2019)',\n",
       " '(2018)']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_YearSpan\n",
    "t2=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/h3/span[2]\")\n",
    "for i in t2:\n",
    "    try:\n",
    "        TvSeries_YearSpan.append(i.text)\n",
    "    except:\n",
    "        TvSeries_YearSpan.append(\"-\")\n",
    "TvSeries_YearSpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_YearSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action, Adventure, Drama',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama, Horror, Thriller',\n",
       " 'Drama, Mystery, Thriller',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Romance',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Action, Crime, Mystery',\n",
       " 'Comedy, Romance',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Mystery, Romance',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama, Horror, Thriller',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Crime, Drama, Fantasy',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Action, Crime, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Drama, Fantasy',\n",
       " 'Animation, Comedy',\n",
       " 'Adventure, Fantasy, Romance',\n",
       " 'Biography, Crime, Drama',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Romance',\n",
       " 'Comedy, Romance',\n",
       " 'Comedy, Drama',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Romance',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Crime, Drama',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Drama, Romance',\n",
       " 'Animation, Adventure, Comedy',\n",
       " 'Action, Drama, Fantasy',\n",
       " 'Adventure, Comedy, Crime',\n",
       " 'Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Comedy, Drama',\n",
       " 'Comedy, Drama',\n",
       " 'Comedy',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Crime, Drama, Romance',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Horror, Sci-Fi',\n",
       " 'Animation, Comedy',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Adventure, Drama, Fantasy',\n",
       " 'Crime, Drama',\n",
       " 'Drama, Mystery',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Drama, Sci-Fi',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Crime, Drama',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama',\n",
       " 'Comedy',\n",
       " 'Comedy, Drama, Music',\n",
       " 'Animation, Comedy',\n",
       " 'Comedy, Crime',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Adventure, Comedy',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Fantasy, Mystery',\n",
       " 'Comedy, Drama, Mystery',\n",
       " 'Crime, Drama',\n",
       " 'Drama, Horror, Mystery',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Drama',\n",
       " 'Drama',\n",
       " 'Comedy, Drama, Romance',\n",
       " 'Comedy, Drama, Fantasy',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Drama, Mystery, Thriller',\n",
       " 'Action, Crime, Drama',\n",
       " 'Drama, Thriller',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Comedy, Drama, Thriller',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Biography, Drama, History',\n",
       " 'Drama, History, Thriller',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Drama, Fantasy',\n",
       " 'Adventure, Comedy, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Drama, Horror, Mystery']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_Genre\n",
    "t3=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p[1]/span[5]\")\n",
    "for i in t3:\n",
    "    try:\n",
    "        TvSeries_Genre.append(i.text)\n",
    "    except:\n",
    "        TvSeries_Genre.append(\"-\")\n",
    "TvSeries_Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_Genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57 min',\n",
       " '51 min',\n",
       " '44 min',\n",
       " '60 min',\n",
       " '43 min',\n",
       " '59 min',\n",
       " '45 min',\n",
       " '41 min',\n",
       " '43 min',\n",
       " '42 min',\n",
       " '70 min',\n",
       " '22 min',\n",
       " '60 min',\n",
       " '88 min',\n",
       " '44 min',\n",
       " '44 min',\n",
       " '43 min',\n",
       " '60 min',\n",
       " '49 min',\n",
       " '42 min',\n",
       " '44 min',\n",
       " '44 min',\n",
       " '43 min',\n",
       " '41 min',\n",
       " '22 min',\n",
       " '60 min',\n",
       " '49 min',\n",
       " '54 min',\n",
       " '22 min',\n",
       " '22 min',\n",
       " '44 min',\n",
       " '49 min',\n",
       " '45 min',\n",
       " '43 min',\n",
       " '42 min',\n",
       " '60 min',\n",
       " '42 min',\n",
       " '62 min',\n",
       " '56 min',\n",
       " '22 min',\n",
       " '23 min',\n",
       " '42 min',\n",
       " '25 min',\n",
       " '51 min',\n",
       " '60 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '46 min',\n",
       " '22 min',\n",
       " '45 min',\n",
       " '45 min',\n",
       " '53 min',\n",
       " '44 min',\n",
       " '22 min',\n",
       " '43 min',\n",
       " '44 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '42 min',\n",
       " '44 min',\n",
       " '55 min',\n",
       " '42 min',\n",
       " '42 min',\n",
       " '60 min',\n",
       " '60 min',\n",
       " '41 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '22 min',\n",
       " '22 min',\n",
       " '43 min',\n",
       " '60 min',\n",
       " '55 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '46 min',\n",
       " '45 min',\n",
       " '53 min',\n",
       " '30 min',\n",
       " '42 min',\n",
       " '45 min',\n",
       " '22 min',\n",
       " '55 min',\n",
       " '45 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '55 min',\n",
       " '43 min',\n",
       " '50 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '43 min',\n",
       " '58 min',\n",
       " '330 min',\n",
       " '42 min',\n",
       " '42 min',\n",
       " '50 min',\n",
       " '42 min',\n",
       " '45 min',\n",
       " '572 min']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_RunTime\n",
    "t4=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p[1]/span[3]\")\n",
    "for i in t4:\n",
    "    try:\n",
    "        TvSeries_RunTime.append(i.text)\n",
    "    except:\n",
    "        TvSeries_RunTime.append(\"-\")\n",
    "TvSeries_RunTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_RunTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.3',\n",
       " '8.7',\n",
       " '8.2',\n",
       " '7.6',\n",
       " '7.6',\n",
       " '8',\n",
       " '6.8',\n",
       " '7.5',\n",
       " '7.6',\n",
       " '7.5',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '8.8',\n",
       " '9.1',\n",
       " '8.5',\n",
       " '7.4',\n",
       " '7.7',\n",
       " '8',\n",
       " '9.4',\n",
       " '8.1',\n",
       " '8.4',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '7.6',\n",
       " '8.6',\n",
       " '7.7',\n",
       " '8.8',\n",
       " '8.6',\n",
       " '8.9',\n",
       " '8.3',\n",
       " '8.4',\n",
       " '8.5',\n",
       " '8.2',\n",
       " '6.2',\n",
       " '7.4',\n",
       " '8.3',\n",
       " '7.8',\n",
       " '8.6',\n",
       " '7.9',\n",
       " '8.4',\n",
       " '9.2',\n",
       " '6.6',\n",
       " '8.1',\n",
       " '8.7',\n",
       " '8.8',\n",
       " '7.5',\n",
       " '8.3',\n",
       " '8.5',\n",
       " '7.7',\n",
       " '7.5',\n",
       " '7.7',\n",
       " '8.6',\n",
       " '6.9',\n",
       " '8.1',\n",
       " '8',\n",
       " '8.3',\n",
       " '8.8',\n",
       " '8.7',\n",
       " '6.7',\n",
       " '8.3',\n",
       " '8.3',\n",
       " '7.4',\n",
       " '6.8',\n",
       " '8.4',\n",
       " '7.5',\n",
       " '8.1',\n",
       " '7.8',\n",
       " '6.7',\n",
       " '8.7',\n",
       " '8.4',\n",
       " '6.6',\n",
       " '8',\n",
       " '8.9',\n",
       " '7.8',\n",
       " '7.5',\n",
       " '8.8',\n",
       " '8.1',\n",
       " '8.5',\n",
       " '8.3',\n",
       " '7.3',\n",
       " '8.7',\n",
       " '8.2',\n",
       " '6.5',\n",
       " '6.3',\n",
       " '8.6',\n",
       " '7.8',\n",
       " '7.3',\n",
       " '7.7',\n",
       " '7.3',\n",
       " '8.5',\n",
       " '6.5',\n",
       " '8.1',\n",
       " '8.6',\n",
       " '9.4',\n",
       " '7.8',\n",
       " '7.5',\n",
       " '7.8',\n",
       " '8',\n",
       " '7.1',\n",
       " '8.6']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_Ratings\n",
    "t5=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/div[1]/div[1]/span[2]\")\n",
    "for i in t5:\n",
    "    try:\n",
    "        TvSeries_Ratings.append(i.text)\n",
    "    except:\n",
    "        TvSeries_Ratings.append(\"-\")\n",
    "TvSeries_Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_Ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,826,041',\n",
       " '865,949',\n",
       " '875,512',\n",
       " '263,023',\n",
       " '224,450',\n",
       " '282,881',\n",
       " '125,250',\n",
       " '261,810',\n",
       " '314,924',\n",
       " '412,563',\n",
       " '332,628',\n",
       " '737,963',\n",
       " '463,214',\n",
       " '831,123',\n",
       " '452,370',\n",
       " '154,815',\n",
       " '290,778',\n",
       " '282,773',\n",
       " '1,529,194',\n",
       " '254,163',\n",
       " '400,290',\n",
       " '488,124',\n",
       " '134,445',\n",
       " '131,300',\n",
       " '372,722',\n",
       " '211,391',\n",
       " '371,206',\n",
       " '371,415',\n",
       " '865,867',\n",
       " '618,706',\n",
       " '371,574',\n",
       " '342,530',\n",
       " '121,267',\n",
       " '114,382',\n",
       " '157,851',\n",
       " '142,919',\n",
       " '215,023',\n",
       " '440,241',\n",
       " '196,508',\n",
       " '371,693',\n",
       " '396,462',\n",
       " '55,476',\n",
       " '154,713',\n",
       " '473,963',\n",
       " '306,082',\n",
       " '54,777',\n",
       " '171,869',\n",
       " '211,284',\n",
       " '198,338',\n",
       " '203,706',\n",
       " '154,090',\n",
       " '658,982',\n",
       " '116,701',\n",
       " '310,319',\n",
       " '208,373',\n",
       " '507,606',\n",
       " '381,602',\n",
       " '422,920',\n",
       " '57,889',\n",
       " '103,531',\n",
       " '320,410',\n",
       " '67,701',\n",
       " '94,580',\n",
       " '188,340',\n",
       " '81,384',\n",
       " '67,933',\n",
       " '40,761',\n",
       " '138,945',\n",
       " '337,616',\n",
       " '243,040',\n",
       " '102,193',\n",
       " '171,422',\n",
       " '511,183',\n",
       " '93,680',\n",
       " '118,705',\n",
       " '341,785',\n",
       " '99,312',\n",
       " '191,820',\n",
       " '64,508',\n",
       " '16,188',\n",
       " '114,735',\n",
       " '124,271',\n",
       " '117,593',\n",
       " '32,384',\n",
       " '230,891',\n",
       " '114,207',\n",
       " '119,183',\n",
       " '68,270',\n",
       " '93,517',\n",
       " '165,131',\n",
       " '23,679',\n",
       " '168,915',\n",
       " '166,546',\n",
       " '583,445',\n",
       " '61,677',\n",
       " '44,617',\n",
       " '55,137',\n",
       " '168,178',\n",
       " '34,933',\n",
       " '191,895']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for TvSeries_Votes\n",
    "t6=driver.find_elements_by_xpath(\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/p[4]/span[2]\")\n",
    "for i in t6:\n",
    "    try:\n",
    "        TvSeries_Votes.append(i.text)\n",
    "    except:\n",
    "        TvSeries_Votes.append(\"-\")\n",
    "TvSeries_Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TvSeries_Votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TvSeries Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>RunTime</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1,826,041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>865,949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>875,512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>263,023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>224,450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama, Fantasy</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>44,617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>55,137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005–2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8</td>\n",
       "      <td>168,178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>34,933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>191,895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TvSeries Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things     (2016– )    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)            Drama, Fantasy   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds  (2005–2020)     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "    RunTime Ratings      Votes  \n",
       "0    57 min     9.3  1,826,041  \n",
       "1    51 min     8.7    865,949  \n",
       "2    44 min     8.2    875,512  \n",
       "3    60 min     7.6    263,023  \n",
       "4    43 min     7.6    224,450  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.5     44,617  \n",
       "96   50 min     7.8     55,137  \n",
       "97   42 min       8    168,178  \n",
       "98   45 min     7.1     34,933  \n",
       "99  572 min     8.6    191,895  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "TvSeries_IMDB=pd.DataFrame({})\n",
    "TvSeries_IMDB['TvSeries Name']=TvSeries_Name\n",
    "TvSeries_IMDB['Year Span']=TvSeries_YearSpan\n",
    "TvSeries_IMDB['Genre']=TvSeries_Genre\n",
    "TvSeries_IMDB['RunTime']=TvSeries_RunTime\n",
    "TvSeries_IMDB['Ratings']=TvSeries_Ratings\n",
    "TvSeries_IMDB['Votes']=TvSeries_Votes\n",
    "TvSeries_IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************************************************************Completed*****************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/.\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Dataset name\n",
    "\n",
    "B) Data type\n",
    "\n",
    "C) Task\n",
    "\n",
    "D) Attribute type\n",
    "\n",
    "E) No of instances\n",
    "\n",
    "F) No of attribute\n",
    "\n",
    "G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    "    \n",
    "Ans: Solution is as:-   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening webpage https://archive.ics.uci.edu/\n",
    "url=\"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"bb14e8290c7a6a488eb811f0ed5535dc\", element=\"d6538c9d-7e8c-4ab2-88ba-8f7a3ccc0382\")>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for View All Data sets\n",
    "view_ds=driver.find_element_by_xpath(\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a/font/b\")\n",
    "view_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking View All Data sets Link\n",
    "view_ds.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty Lists so that we can store data in these lists while scraping\n",
    "Dataset_Name=[]\n",
    "Data_type=[]\n",
    "Task_info=[]\n",
    "Attribute_type=[]\n",
    "Num_instances=[]\n",
    "Num_attribute=[]\n",
    "Year_info=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abalone',\n",
       " 'Adult',\n",
       " 'Annealing',\n",
       " 'Anonymous Microsoft Web Data',\n",
       " 'Arrhythmia',\n",
       " 'Artificial Characters',\n",
       " 'Audiology (Original)',\n",
       " 'Audiology (Standardized)',\n",
       " 'Auto MPG',\n",
       " 'Automobile',\n",
       " 'Badges',\n",
       " 'Balance Scale',\n",
       " 'Balloons',\n",
       " 'Breast Cancer',\n",
       " 'Breast Cancer Wisconsin (Original)',\n",
       " 'Breast Cancer Wisconsin (Prognostic)',\n",
       " 'Breast Cancer Wisconsin (Diagnostic)',\n",
       " 'Pittsburgh Bridges',\n",
       " 'Car Evaluation',\n",
       " 'Census Income',\n",
       " 'Chess (King-Rook vs. King-Knight)',\n",
       " 'Chess (King-Rook vs. King-Pawn)',\n",
       " 'Chess (King-Rook vs. King)',\n",
       " 'Chess (Domain Theories)',\n",
       " 'Bach Chorales',\n",
       " 'Connect-4',\n",
       " 'Credit Approval',\n",
       " 'Japanese Credit Screening',\n",
       " 'Computer Hardware',\n",
       " 'Contraceptive Method Choice',\n",
       " 'Covertype',\n",
       " 'Cylinder Bands',\n",
       " 'Dermatology',\n",
       " 'Diabetes',\n",
       " 'DGP2 - The Second Data Generation Program',\n",
       " 'Document Understanding',\n",
       " 'EBL Domain Theories',\n",
       " 'Echocardiogram',\n",
       " 'Ecoli',\n",
       " 'Flags',\n",
       " 'Function Finding',\n",
       " 'Glass Identification',\n",
       " \"Haberman's Survival\",\n",
       " 'Hayes-Roth',\n",
       " 'Heart Disease',\n",
       " 'Hepatitis',\n",
       " 'Horse Colic',\n",
       " 'ICU',\n",
       " 'Image Segmentation',\n",
       " 'Internet Advertisements',\n",
       " 'Ionosphere',\n",
       " 'Iris',\n",
       " 'ISOLET',\n",
       " 'Kinship',\n",
       " 'Labor Relations',\n",
       " 'LED Display Domain',\n",
       " 'Lenses',\n",
       " 'Letter Recognition',\n",
       " 'Liver Disorders',\n",
       " 'Logic Theorist',\n",
       " 'Lung Cancer',\n",
       " 'Lymphography',\n",
       " 'Mechanical Analysis',\n",
       " 'Meta-data',\n",
       " 'Mobile Robots',\n",
       " 'Molecular Biology (Promoter Gene Sequences)',\n",
       " 'Molecular Biology (Protein Secondary Structure)',\n",
       " 'Molecular Biology (Splice-junction Gene Sequences)',\n",
       " \"MONK's Problems\",\n",
       " 'Moral Reasoner',\n",
       " 'Multiple Features',\n",
       " 'Mushroom',\n",
       " 'Musk (Version 1)',\n",
       " 'Musk (Version 2)',\n",
       " 'Nursery',\n",
       " 'Othello Domain Theory',\n",
       " 'Page Blocks Classification',\n",
       " 'Optical Recognition of Handwritten Digits',\n",
       " 'Pen-Based Recognition of Handwritten Digits',\n",
       " 'Post-Operative Patient',\n",
       " 'Primary Tumor',\n",
       " 'Prodigy',\n",
       " 'Qualitative Structure Activity Relationships',\n",
       " 'Quadruped Mammals',\n",
       " 'Servo',\n",
       " 'Shuttle Landing Control',\n",
       " 'Solar Flare',\n",
       " 'Soybean (Large)',\n",
       " 'Soybean (Small)',\n",
       " 'Challenger USA Space Shuttle O-Ring',\n",
       " 'Low Resolution Spectrometer',\n",
       " 'Spambase',\n",
       " 'SPECT Heart',\n",
       " 'SPECTF Heart',\n",
       " 'Sponge',\n",
       " 'Statlog Project',\n",
       " 'Student Loan Relational',\n",
       " 'Teaching Assistant Evaluation',\n",
       " 'Tic-Tac-Toe Endgame',\n",
       " 'Thyroid Disease',\n",
       " 'Trains',\n",
       " 'University',\n",
       " 'Congressional Voting Records',\n",
       " 'Water Treatment Plant',\n",
       " 'Waveform Database Generator (Version 1)',\n",
       " 'Waveform Database Generator (Version 2)',\n",
       " 'Wine',\n",
       " 'Yeast',\n",
       " 'Zoo',\n",
       " 'Undocumented',\n",
       " 'Twenty Newsgroups',\n",
       " 'Australian Sign Language signs',\n",
       " 'Australian Sign Language signs (High Quality)',\n",
       " 'US Census Data (1990)',\n",
       " 'Census-Income (KDD)',\n",
       " 'Coil 1999 Competition Data',\n",
       " 'Corel Image Features',\n",
       " 'E. Coli Genes',\n",
       " 'EEG Database',\n",
       " 'El Nino',\n",
       " 'Entree Chicago Recommendation Data',\n",
       " 'CMU Face Images',\n",
       " 'Insurance Company Benchmark (COIL 2000)',\n",
       " 'Internet Usage Data',\n",
       " 'IPUMS Census Database',\n",
       " 'Japanese Vowels',\n",
       " 'KDD Cup 1998 Data',\n",
       " 'KDD Cup 1999 Data',\n",
       " 'M. Tuberculosis Genes',\n",
       " 'Movie',\n",
       " 'MSNBC.com Anonymous Web Data',\n",
       " 'NSF Research Award Abstracts 1990-2003',\n",
       " 'Pioneer-1 Mobile Robot Data',\n",
       " 'Pseudo Periodic Synthetic Time Series',\n",
       " 'Reuters-21578 Text Categorization Collection',\n",
       " 'Robot Execution Failures',\n",
       " 'Synthetic Control Chart Time Series',\n",
       " 'Syskill and Webert Web Page Ratings',\n",
       " 'UNIX User Data',\n",
       " 'Volcanoes on Venus - JARtool experiment',\n",
       " 'Statlog (Australian Credit Approval)',\n",
       " 'Statlog (German Credit Data)',\n",
       " 'Statlog (Heart)',\n",
       " 'Statlog (Landsat Satellite)',\n",
       " 'Statlog (Image Segmentation)',\n",
       " 'Statlog (Shuttle)',\n",
       " 'Statlog (Vehicle Silhouettes)',\n",
       " 'Connectionist Bench (Nettalk Corpus)',\n",
       " 'Connectionist Bench (Sonar, Mines vs. Rocks)',\n",
       " 'Connectionist Bench (Vowel Recognition - Deterding Data)',\n",
       " 'Economic Sanctions',\n",
       " 'Protein Data',\n",
       " 'Cloud',\n",
       " 'CalIt2 Building People Counts',\n",
       " 'Dodgers Loop Sensor',\n",
       " 'Poker Hand',\n",
       " 'MAGIC Gamma Telescope',\n",
       " 'UJI Pen Characters',\n",
       " 'Mammographic Mass',\n",
       " 'Forest Fires',\n",
       " 'Reuters Transcribed Subset',\n",
       " 'Bag of Words',\n",
       " 'Concrete Compressive Strength',\n",
       " 'Hill-Valley',\n",
       " 'Arcene',\n",
       " 'Dexter',\n",
       " 'Dorothea',\n",
       " 'Gisette',\n",
       " 'Madelon',\n",
       " 'Ozone Level Detection',\n",
       " 'Abscisic Acid Signaling Network',\n",
       " 'Parkinsons',\n",
       " 'Character Trajectories',\n",
       " 'Blood Transfusion Service Center',\n",
       " 'UJI Pen Characters (Version 2)',\n",
       " 'Semeion Handwritten Digit',\n",
       " 'SECOM',\n",
       " 'Plants',\n",
       " 'Libras Movement',\n",
       " 'Concrete Slump Test',\n",
       " 'Communities and Crime',\n",
       " 'Acute Inflammations',\n",
       " 'Wine Quality',\n",
       " 'URL Reputation',\n",
       " 'p53 Mutants',\n",
       " 'Parkinsons Telemonitoring',\n",
       " 'Demospongiae',\n",
       " 'Opinosis Opinion ⁄ Review',\n",
       " 'Breast Tissue',\n",
       " 'Cardiotocography',\n",
       " 'Wall-Following Robot Navigation Data',\n",
       " 'Spoken Arabic Digit',\n",
       " 'Localization Data for Person Activity',\n",
       " 'AutoUniv',\n",
       " 'Steel Plates Faults',\n",
       " 'MiniBooNE particle identification',\n",
       " 'YearPredictionMSD',\n",
       " 'PEMS-SF',\n",
       " 'OpinRank Review Dataset',\n",
       " 'Relative location of CT slices on axial axis',\n",
       " 'Online Handwritten Assamese Characters Dataset',\n",
       " 'PubChem Bioassay Data',\n",
       " 'Record Linkage Comparison Patterns',\n",
       " 'Communities and Crime Unnormalized',\n",
       " 'Vertebral Column',\n",
       " 'EMG Physical Action Data Set',\n",
       " 'Vicon Physical Action Data Set',\n",
       " 'Amazon Commerce reviews set',\n",
       " 'Amazon Access Samples',\n",
       " 'Reuter_50_50',\n",
       " 'Farm Ads',\n",
       " 'DBWorld e-mails',\n",
       " 'KEGG Metabolic Relation Network (Directed)',\n",
       " 'KEGG Metabolic Reaction Network (Undirected)',\n",
       " 'Bank Marketing',\n",
       " 'YouTube Comedy Slam Preference Data',\n",
       " 'Gas Sensor Array Drift Dataset',\n",
       " 'ILPD (Indian Liver Patient Dataset)',\n",
       " 'OPPORTUNITY Activity Recognition',\n",
       " 'Nomao',\n",
       " 'SMS Spam Collection',\n",
       " 'Skin Segmentation',\n",
       " 'Planning Relax',\n",
       " 'PAMAP2 Physical Activity Monitoring',\n",
       " 'Restaurant & consumer data',\n",
       " 'CNAE-9',\n",
       " 'Individual household electric power consumption',\n",
       " 'seeds',\n",
       " 'Northix',\n",
       " 'QtyT40I10D100K',\n",
       " 'Legal Case Reports',\n",
       " 'Human Activity Recognition Using Smartphones',\n",
       " 'One-hundred plant species leaves data set',\n",
       " 'Energy efficiency',\n",
       " 'Yacht Hydrodynamics',\n",
       " 'Fertility',\n",
       " 'Daphnet Freezing of Gait',\n",
       " '3D Road Network (North Jutland, Denmark)',\n",
       " 'ISTANBUL STOCK EXCHANGE',\n",
       " 'Buzz in social media',\n",
       " 'First-order theorem proving',\n",
       " 'Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)',\n",
       " 'Gas sensor arrays in open sampling settings',\n",
       " 'Climate Model Simulation Crashes',\n",
       " 'MicroMass',\n",
       " 'QSAR biodegradation',\n",
       " 'BLOGGER',\n",
       " 'Daily and Sports Activities',\n",
       " 'User Knowledge Modeling',\n",
       " 'Reuters RCV1 RCV2 Multilingual, Multiview Text Categorization Test collection',\n",
       " 'NYSK',\n",
       " 'Turkiye Student Evaluation',\n",
       " \"ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\",\n",
       " 'EEG Eye State',\n",
       " 'Physicochemical Properties of Protein Tertiary Structure',\n",
       " 'seismic-bumps',\n",
       " 'banknote authentication',\n",
       " 'USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat',\n",
       " 'YouTube Multiview Video Games Dataset',\n",
       " 'Gas Sensor Array Drift Dataset at Different Concentrations',\n",
       " 'Activities of Daily Living (ADLs) Recognition Using Binary Sensors',\n",
       " 'SkillCraft1 Master Table Dataset',\n",
       " 'Weight Lifting Exercises monitored with Inertial Measurement Units',\n",
       " 'SML2010',\n",
       " 'Bike Sharing Dataset',\n",
       " 'Predict keywords activities in a online social media',\n",
       " 'Thoracic Surgery Data',\n",
       " 'EMG dataset in Lower Limb',\n",
       " 'SUSY',\n",
       " 'HIGGS',\n",
       " 'Qualitative_Bankruptcy',\n",
       " 'LSVT Voice Rehabilitation',\n",
       " 'Dataset for ADL Recognition with Wrist-worn Accelerometer',\n",
       " 'Wilt',\n",
       " 'User Identification From Walking Activity',\n",
       " 'Activity Recognition from Single Chest-Mounted Accelerometer',\n",
       " 'Leaf',\n",
       " 'Dresses_Attribute_Sales',\n",
       " 'Tamilnadu Electricity Board Hourly Readings',\n",
       " 'Airfoil Self-Noise',\n",
       " 'Wholesale customers',\n",
       " 'Twitter Data set for Arabic Sentiment Analysis',\n",
       " 'Combined Cycle Power Plant',\n",
       " 'Urban Land Cover',\n",
       " 'Diabetes 130-US hospitals for years 1999-2008',\n",
       " 'Bach Choral Harmony',\n",
       " 'StoneFlakes',\n",
       " 'Tennis Major Tournament Match Statistics',\n",
       " 'Parkinson Speech Dataset with Multiple Types of Sound Recordings',\n",
       " 'Gesture Phase Segmentation',\n",
       " 'Perfume Data',\n",
       " 'BlogFeedback',\n",
       " 'REALDISP Activity Recognition Dataset',\n",
       " 'Newspaper and magazine images segmentation dataset',\n",
       " 'AAAI 2014 Accepted Papers',\n",
       " 'Gas sensor array under flow modulation',\n",
       " 'Gas sensor array exposed to turbulent gas mixtures',\n",
       " 'UJIIndoorLoc',\n",
       " 'Sentence Classification',\n",
       " 'Dow Jones Index',\n",
       " 'sEMG for Basic Hand movements',\n",
       " 'AAAI 2013 Accepted Papers',\n",
       " 'Geographical Original of Music',\n",
       " 'Condition Based Maintenance of Naval Propulsion Plants',\n",
       " 'Grammatical Facial Expressions',\n",
       " 'NoisyOffice',\n",
       " 'MHEALTH Dataset',\n",
       " 'Student Performance',\n",
       " 'ElectricityLoadDiagrams20112014',\n",
       " 'Gas sensor array under dynamic gas mixtures',\n",
       " 'microblogPCU',\n",
       " 'Firm-Teacher_Clave-Direction_Classification',\n",
       " 'Dataset for Sensorless Drive Diagnosis',\n",
       " 'TV News Channel Commercial Detection Dataset',\n",
       " 'Phishing Websites',\n",
       " 'Greenhouse Gas Observing Network',\n",
       " 'Diabetic Retinopathy Debrecen Data Set',\n",
       " 'HIV-1 protease cleavage',\n",
       " 'Sentiment Labelled Sentences',\n",
       " 'Online News Popularity',\n",
       " 'Forest type mapping',\n",
       " 'wiki4HE',\n",
       " 'Online Video Characteristics and Transcoding Time Dataset',\n",
       " 'Chronic_Kidney_Disease',\n",
       " 'Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014',\n",
       " 'Folio',\n",
       " 'Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015',\n",
       " 'Cuff-Less Blood Pressure Estimation',\n",
       " 'Smartphone-Based Recognition of Human Activities and Postural Transitions',\n",
       " 'Mice Protein Expression',\n",
       " 'UJIIndoorLoc-Mag',\n",
       " 'Heterogeneity Activity Recognition',\n",
       " 'Educational Process Mining (EPM): A Learning Analytics Data Set',\n",
       " 'HEPMASS',\n",
       " 'Indoor User Movement Prediction from RSS data',\n",
       " 'Open University Learning Analytics dataset',\n",
       " 'default of credit card clients',\n",
       " 'Mesothelioma’s disease data set',\n",
       " 'Online Retail',\n",
       " 'SIFT10M',\n",
       " 'GPS Trajectories',\n",
       " 'Detect Malacious Executable(AntiVirus)',\n",
       " 'Occupancy Detection',\n",
       " 'Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson’s Disease',\n",
       " 'News Aggregator',\n",
       " 'Air Quality',\n",
       " 'Twin gas sensor arrays',\n",
       " 'Gas sensors for home activity monitoring',\n",
       " 'Facebook Comment Volume Dataset',\n",
       " 'Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL)',\n",
       " 'Polish companies bankruptcy data',\n",
       " 'Activity Recognition system based on Multisensor data fusion (AReM)',\n",
       " 'Dota2 Games Results',\n",
       " 'Facebook metrics',\n",
       " 'UbiqLog (smartphone lifelogging)',\n",
       " 'NIPS Conference Papers 1987-2015',\n",
       " 'HTRU2',\n",
       " 'Drug consumption (quantified)',\n",
       " 'Appliances energy prediction',\n",
       " 'Miskolc IIS Hybrid IPS',\n",
       " 'KDC-4007 dataset Collection',\n",
       " 'Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone',\n",
       " 'DrivFace',\n",
       " 'Website Phishing',\n",
       " 'YouTube Spam Collection',\n",
       " 'Beijing PM2.5 Data',\n",
       " 'Cargo 2000 Freight Tracking and Tracing',\n",
       " 'Cervical cancer (Risk Factors)',\n",
       " 'Quality Assessment of Digital Colposcopies',\n",
       " 'KASANDR',\n",
       " 'FMA: A Dataset For Music Analysis',\n",
       " 'Air quality',\n",
       " 'Epileptic Seizure Recognition',\n",
       " 'Devanagari Handwritten Character Dataset',\n",
       " 'Stock portfolio performance',\n",
       " 'MoCap Hand Postures',\n",
       " 'Early biomarkers of Parkinson�s disease based on natural connected speech',\n",
       " 'Data for Software Engineering Teamwork Assessment in Education Setting',\n",
       " 'PM2.5 Data of Five Chinese Cities',\n",
       " 'Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet',\n",
       " 'Sales_Transactions_Dataset_Weekly',\n",
       " 'Las Vegas Strip',\n",
       " 'Eco-hotel',\n",
       " 'MEU-Mobile KSD',\n",
       " 'Crowdsourced Mapping',\n",
       " 'gene expression cancer RNA-Seq',\n",
       " 'Hybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer',\n",
       " 'chestnut – LARVIC',\n",
       " 'Burst Header Packet (BHP) flooding attack on Optical Burst Switching (OBS) Network',\n",
       " 'Motion Capture Hand Postures',\n",
       " 'Anuran Calls (MFCCs)',\n",
       " 'TTC-3600: Benchmark dataset for Turkish text categorization',\n",
       " 'Gastrointestinal Lesions in Regular Colonoscopy',\n",
       " 'Daily Demand Forecasting Orders',\n",
       " 'Paper Reviews',\n",
       " 'extention of Z-Alizadeh sani dataset',\n",
       " 'Z-Alizadeh Sani',\n",
       " 'Dynamic Features of VirusShare Executables',\n",
       " 'IDA2016Challenge',\n",
       " 'DSRC Vehicle Communications',\n",
       " 'Mturk User-Perceived Clusters over Images',\n",
       " 'Character Font Images',\n",
       " 'DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels',\n",
       " 'Autistic Spectrum Disorder Screening Data for Children',\n",
       " 'Autistic Spectrum Disorder Screening Data for Adolescent',\n",
       " 'APS Failure at Scania Trucks',\n",
       " 'Wireless Indoor Localization',\n",
       " 'HCC Survival',\n",
       " 'CSM (Conventional and Social Media Movies) Dataset 2014 and 2015',\n",
       " 'University of Tehran Question Dataset 2016 (UTQD.2016)',\n",
       " 'Autism Screening Adult',\n",
       " 'Activity recognition with healthy older people using a batteryless wearable sensor',\n",
       " 'Immunotherapy Dataset',\n",
       " 'Cryotherapy Dataset',\n",
       " 'OCT data & Color Fundus Images of Left & Right Eyes',\n",
       " 'Discrete Tone Image Dataset',\n",
       " 'News Popularity in Multiple Social Media Platforms',\n",
       " 'Ultrasonic flowmeter diagnostics',\n",
       " 'ICMLA 2014 Accepted Papers Data Set',\n",
       " 'BLE RSSI Dataset for Indoor localization and Navigation',\n",
       " 'Container Crane Controller Data Set',\n",
       " 'Residential Building Data Set',\n",
       " 'Health News in Twitter',\n",
       " 'chipseq',\n",
       " 'SGEMM GPU kernel performance',\n",
       " 'Repeat Consumption Matrices',\n",
       " 'detection_of_IoT_botnet_attacks_N_BaIoT',\n",
       " 'Absenteeism at work',\n",
       " 'SCADI',\n",
       " 'Condition monitoring of hydraulic systems',\n",
       " 'Carbon Nanotubes',\n",
       " 'Optical Interconnection Network',\n",
       " 'Sports articles for objectivity analysis',\n",
       " 'Breast Cancer Coimbra',\n",
       " 'GNFUV Unmanned Surface Vehicles Sensor Data',\n",
       " 'Dishonest Internet users Dataset',\n",
       " 'Victorian Era Authorship Attribution',\n",
       " 'Simulated Falls and Daily Living Activities Data Set',\n",
       " 'Multimodal Damage Identification for Humanitarian Computing',\n",
       " 'EEG Steady-State Visual Evoked Potential Signals',\n",
       " 'Roman Urdu Data Set',\n",
       " 'Avila',\n",
       " 'PANDOR',\n",
       " 'Drug Review Dataset (Druglib.com)',\n",
       " 'Drug Review Dataset (Drugs.com)',\n",
       " 'Physical Unclonable Functions',\n",
       " 'Superconductivty Data',\n",
       " 'WESAD (Wearable Stress and Affect Detection)',\n",
       " 'GNFUV Unmanned Surface Vehicles Sensor Data Set 2',\n",
       " 'Student Academics Performance',\n",
       " 'Online Shoppers Purchasing Intention Dataset',\n",
       " 'PMU-UD',\n",
       " \"Parkinson's Disease Classification\",\n",
       " 'Electrical Grid Stability Simulated Data',\n",
       " 'Caesarian Section Classification Dataset',\n",
       " 'BAUM-1',\n",
       " 'BAUM-2',\n",
       " 'Audit Data',\n",
       " 'BuddyMove Data Set',\n",
       " 'Real estate valuation data set',\n",
       " 'Early biomarkers of Parkinson’s disease based on natural connected speech Data Set',\n",
       " 'Somerville Happiness Survey',\n",
       " '2.4 GHZ Indoor Channel Measurements',\n",
       " 'EMG data for gestures',\n",
       " 'Parking Birmingham',\n",
       " 'Behavior of the urban traffic of the city of Sao Paulo in Brazil',\n",
       " 'Travel Reviews',\n",
       " 'Tarvel Review Ratings',\n",
       " 'Rice Leaf Diseases',\n",
       " 'Gas sensor array temperature modulation',\n",
       " 'Facebook Live Sellers in Thailand',\n",
       " 'Parkinson Dataset with replicated acoustic features',\n",
       " 'Metro Interstate Traffic Volume',\n",
       " 'Query Analytics Workloads Dataset',\n",
       " 'Wave Energy Converters',\n",
       " 'PPG-DaLiA',\n",
       " 'Alcohol QCM Sensor Dataset',\n",
       " 'Divorce Predictors data set',\n",
       " 'Incident management process enriched event log',\n",
       " 'Opinion Corpus for Lebanese Arabic Reviews (OCLAR)',\n",
       " 'MEx',\n",
       " 'Beijing Multi-Site Air-Quality Data',\n",
       " 'Online Retail II',\n",
       " 'Hepatitis C Virus (HCV) for Egyptian patients',\n",
       " 'QSAR fish toxicity',\n",
       " 'QSAR aquatic toxicity',\n",
       " 'Human Activity Recognition from Continuous Ambient Sensor Data',\n",
       " 'WISDM Smartphone and Smartwatch Activity and Biometrics Dataset',\n",
       " 'QSAR oral toxicity',\n",
       " 'QSAR androgen receptor',\n",
       " 'QSAR Bioconcentration classes dataset',\n",
       " 'QSAR fish bioconcentration factor (BCF)',\n",
       " 'A study of Asian Religious and Biblical Texts',\n",
       " 'Real-time Election Results: Portugal 2019',\n",
       " 'Bias correction of numerical prediction model temperature forecast',\n",
       " 'Bar Crawl: Detecting Heavy Drinking',\n",
       " 'Kitsune Network Attack Dataset',\n",
       " 'Shoulder Implant X-Ray Manufacturer Classification',\n",
       " 'Speaker Accent Recognition',\n",
       " 'Heart failure clinical records',\n",
       " 'Deepfakes: Medical Image Tamper Detection',\n",
       " 'selfBACK',\n",
       " 'South German Credit',\n",
       " 'Exasens',\n",
       " 'Swarm Behaviour',\n",
       " 'Crop mapping using fused optical-radar data set',\n",
       " 'BitcoinHeistRansomwareAddressDataset',\n",
       " 'Facebook Large Page-Page Network',\n",
       " 'Amphibians',\n",
       " 'Early stage diabetes risk prediction dataset.',\n",
       " 'Turkish Spam V01',\n",
       " 'Stock keeping units',\n",
       " 'Demand Forecasting for a store',\n",
       " 'Detect Malware Types',\n",
       " 'Wave Energy Converters',\n",
       " 'Youtube cookery channels viewers comments in Hinglish',\n",
       " 'Pedestrian in Traffic Dataset',\n",
       " 'Cervical Cancer Behavior Risk',\n",
       " 'Sattriya_Dance_Single_Hand_Gestures Dataset',\n",
       " 'Divorce Predictors data set',\n",
       " '3W dataset',\n",
       " 'Malware static and dynamic features VxHeaven and Virus Total',\n",
       " 'Internet Firewall Data',\n",
       " 'User Profiling and Abusive Language Detection Dataset',\n",
       " 'Estimation of obesity levels based on eating habits and physical condition',\n",
       " 'Rice (Cammeo and Osmancik)',\n",
       " 'Vehicle routing and scheduling problems',\n",
       " 'Algerian Forest Fires Dataset',\n",
       " 'Breath Metabolomics',\n",
       " 'Horton General Hospital',\n",
       " 'UrbanGB, urban road accidents coordinates labelled by the urban center',\n",
       " 'Gas Turbine CO and NOx Emission Data Set',\n",
       " 'Activity recognition using wearable physiological measurements',\n",
       " 'clickstream data for online shopping',\n",
       " 'CNNpred: CNN-based stock market prediction using a diverse set of variables',\n",
       " 'Apartment for rent classified',\n",
       " ': Simulated Data set of Iraqi tourism places',\n",
       " 'Nasarian CAD Dataset',\n",
       " 'Monolithic Columns in Troad and Mysia Region',\n",
       " 'Bar Crawl: Detecting Heavy Drinking',\n",
       " 'Seoul Bike Sharing Demand',\n",
       " 'Person Classification Gait Data',\n",
       " 'Shill Bidding Dataset',\n",
       " 'Iranian Churn Dataset',\n",
       " 'Unmanned Aerial Vehicle (UAV) Intrusion Detection',\n",
       " 'Bone marrow transplant: children',\n",
       " 'Exasens',\n",
       " 'COVID-19 Surveillance',\n",
       " 'Refractive errors',\n",
       " 'Shoulder Implant X-Ray Manufacturer Classification',\n",
       " 'CLINC150',\n",
       " 'HCV data',\n",
       " 'Taiwanese Bankruptcy Prediction',\n",
       " 'South German Credit (UPDATE)',\n",
       " 'IIWA14-R820-Gazebo-Dataset-10Trajectories',\n",
       " 'Guitar Chords finger positions',\n",
       " 'Russian Corpus of Biographical Texts',\n",
       " 'Codon usage',\n",
       " 'Intelligent Media Accelerometer and Gyroscope (IM-AccGyro) Dataset',\n",
       " 'Myocardial infarction complications',\n",
       " 'Hungarian Chickenpox Cases',\n",
       " 'Simulated data for survival modelling',\n",
       " 'Student Performance on an entrance examination',\n",
       " 'Chemical Composition of Ceramic Samples',\n",
       " 'Labeled Text Forum Threads Dataset',\n",
       " 'Stock keeping units',\n",
       " 'BLE RSSI dataset for Indoor localization',\n",
       " 'Basketball dataset',\n",
       " 'GitHub MUSAE',\n",
       " 'Anticancer peptides',\n",
       " 'Monolithic Columns in Troad and Mysia Region',\n",
       " 'Gender by Name',\n",
       " 'Iranian Churn Dataset',\n",
       " 'Unmanned Aerial Vehicle (UAV) Intrusion Detection',\n",
       " 'Shoulder Implant Manufacture Classification',\n",
       " 'LastFM Asia Social Network',\n",
       " 'Wheat kernels',\n",
       " 'Productivity Prediction of Garment Employees',\n",
       " 'Multi-view Brain Networks',\n",
       " 'LastFM Asia Social Network',\n",
       " 'Wisesight Sentiment Corpus',\n",
       " 'AI4I 2020 Predictive Maintenance Dataset',\n",
       " 'Dry Bean Dataset',\n",
       " 'in-vehicle coupon recommendation',\n",
       " 'Gait Classification',\n",
       " 'Wikipedia Math Essentials',\n",
       " 'Wikipedia Math Essentials',\n",
       " 'Synchronous Machine Data Set']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Dataset_Name\n",
    "d1=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]/table/tbody/tr/td[2]/p/b/a\")\n",
    "for i in d1:\n",
    "    try:\n",
    "        Dataset_Name.append(i.text)\n",
    "    except:\n",
    "        Dataset_Name.append(\"-\")\n",
    "Dataset_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Univariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Data-Generator ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Univariate, Time-Series ',\n",
       " 'Multivariate, Spatial ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Data-Generator ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Relational ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Data-Generator ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Sequential, Domain-Theory ',\n",
       " 'Sequential ',\n",
       " 'Sequential, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate, Data-Generator ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Data-Generator ',\n",
       " 'Multivariate, Data-Generator ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Relational ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Spatio-temporal ',\n",
       " 'Transactional, Sequential ',\n",
       " 'Image ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Relational ',\n",
       " 'Multivariate, Relational ',\n",
       " 'Sequential ',\n",
       " 'Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Univariate, Time-Series ',\n",
       " 'Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Text ',\n",
       " 'Text, Sequential ',\n",
       " 'Image ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Domain-Theory ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Univariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Text ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Text, Domain-Theory ',\n",
       " 'Time-Series, Domain-Theory ',\n",
       " 'Multivariate, Text, Domain-Theory ',\n",
       " 'Text ',\n",
       " 'Text ',\n",
       " 'Multivariate, Univariate, Text ',\n",
       " 'Multivariate, Univariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Univariate ',\n",
       " 'Multivariate, Text, Domain-Theory ',\n",
       " 'Univariate ',\n",
       " 'Univariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Univariate, Text ',\n",
       " 'Sequential ',\n",
       " 'Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Sequential, Text ',\n",
       " 'Multivariate, Univariate, Time-Series ',\n",
       " 'Time-Series, Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Sequential ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Domain-Theory ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series, Text ',\n",
       " 'Univariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Univariate, Sequential, Time-Series ',\n",
       " 'Univariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Univariate, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Univariate, Sequential, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series, Domain-Theory ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Text ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Sequential, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " ' ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " ' ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Text ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Sequential, Text ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " ' ',\n",
       " 'Sequential ',\n",
       " 'Univariate ',\n",
       " 'Univariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Univariate, Domain-Theory ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Univariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Univariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Univariate ',\n",
       " 'Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Univariate, Sequential, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Univariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Text ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Time-Series, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series, Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Sequential, Time-Series ',\n",
       " 'Multivariate, Univariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Univariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Univariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Sequential ',\n",
       " 'Sequential, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " ' ',\n",
       " 'Text ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Sequential, Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Sequential ',\n",
       " 'Multivariate ',\n",
       " 'Text ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate, Text ',\n",
       " 'Multivariate, Time-Series ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Multivariate ',\n",
       " 'Time-Series ',\n",
       " 'Time-Series ',\n",
       " 'Multivariate ']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Data_type\n",
    "d2=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]\")\n",
    "for i in d2[1:]:\n",
    "    try:\n",
    "        Data_type.append(i.text)\n",
    "    except:\n",
    "        Data_type.append(\"-\")\n",
    "Data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Recommender-Systems ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Function-Learning ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Relational-Learning ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Recommender-Systems ',\n",
       " 'Classification ',\n",
       " 'Regression, Description ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Causal-Discovery ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Causal-Discovery ',\n",
       " 'Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression, Clustering, Causal-Discovery ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Regression, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Regression, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression, Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression, Clustering, Causa ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering, Causal-Discovery ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Causal-Discovery ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Regression, Clustering, Causal-Discovery ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Clustering, Causal-Discovery ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Causal-Discovery ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Causal-Discovery ',\n",
       " 'Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering, Causal-Discovery ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Causal-Discovery ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Clustering ',\n",
       " 'Classification, Regression ',\n",
       " ' ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Recommendation ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression, Clustering ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering, Causal-Discovery ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Causal-Discovery ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Causal-Discovery ',\n",
       " 'Clustering ',\n",
       " 'Regression, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Clustering ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression ',\n",
       " 'Classification, Clustering ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification, Regression, Causal-Discovery ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Classification ',\n",
       " 'Regression ',\n",
       " 'Regression ',\n",
       " 'Regression ']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Task_info\n",
    "d3=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]/p\")\n",
    "for i in d3[1:]:\n",
    "    try:\n",
    "        Task_info.append(i.text)\n",
    "    except:\n",
    "        Task_info.append(\"-\")\n",
    "Task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Task_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Categorical, Integer, Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " ' ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer ',\n",
       " ' ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Real, Integer ',\n",
       " 'Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Categorical, Integer, Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Categorical, Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Categorical, Real ',\n",
       " 'Real ',\n",
       " 'Categorical ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " 'Categorical, Real ',\n",
       " ' ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Categorical ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Categorical, Integer ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Categorical ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Categorical, Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Categorical ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Categorical ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " 'Integer ',\n",
       " 'Real ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " 'Integer ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Integer, Real ',\n",
       " ' ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ',\n",
       " 'Real ']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Attribute_type\n",
    "d4=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p\")\n",
    "for i in d4[1:]:\n",
    "    try:\n",
    "        Attribute_type.append(i.text)\n",
    "    except:\n",
    "        Attribute_type.append(\"-\")\n",
    "Attribute_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Attribute_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4177 ',\n",
       " '48842 ',\n",
       " '798 ',\n",
       " '37711 ',\n",
       " '452 ',\n",
       " '6000 ',\n",
       " '226 ',\n",
       " '226 ',\n",
       " '398 ',\n",
       " '205 ',\n",
       " '294 ',\n",
       " '625 ',\n",
       " '16 ',\n",
       " '286 ',\n",
       " '699 ',\n",
       " '198 ',\n",
       " '569 ',\n",
       " '108 ',\n",
       " '1728 ',\n",
       " '48842 ',\n",
       " ' ',\n",
       " '3196 ',\n",
       " '28056 ',\n",
       " ' ',\n",
       " '100 ',\n",
       " '67557 ',\n",
       " '690 ',\n",
       " '125 ',\n",
       " '209 ',\n",
       " '1473 ',\n",
       " '581012 ',\n",
       " '512 ',\n",
       " '366 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '132 ',\n",
       " '336 ',\n",
       " '194 ',\n",
       " '352 ',\n",
       " '214 ',\n",
       " '306 ',\n",
       " '160 ',\n",
       " '303 ',\n",
       " '155 ',\n",
       " '368 ',\n",
       " ' ',\n",
       " '2310 ',\n",
       " '3279 ',\n",
       " '351 ',\n",
       " '150 ',\n",
       " '7797 ',\n",
       " '104 ',\n",
       " '57 ',\n",
       " ' ',\n",
       " '24 ',\n",
       " '20000 ',\n",
       " '345 ',\n",
       " ' ',\n",
       " '32 ',\n",
       " '148 ',\n",
       " '209 ',\n",
       " '528 ',\n",
       " ' ',\n",
       " '106 ',\n",
       " '128 ',\n",
       " '3190 ',\n",
       " '432 ',\n",
       " '202 ',\n",
       " '2000 ',\n",
       " '8124 ',\n",
       " '476 ',\n",
       " '6598 ',\n",
       " '12960 ',\n",
       " ' ',\n",
       " '5473 ',\n",
       " '5620 ',\n",
       " '10992 ',\n",
       " '90 ',\n",
       " '339 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '167 ',\n",
       " '15 ',\n",
       " '1389 ',\n",
       " '307 ',\n",
       " '47 ',\n",
       " '23 ',\n",
       " '531 ',\n",
       " '4601 ',\n",
       " '267 ',\n",
       " '267 ',\n",
       " '76 ',\n",
       " ' ',\n",
       " '1000 ',\n",
       " '151 ',\n",
       " '958 ',\n",
       " '7200 ',\n",
       " '10 ',\n",
       " '285 ',\n",
       " '435 ',\n",
       " '527 ',\n",
       " '5000 ',\n",
       " '5000 ',\n",
       " '178 ',\n",
       " '1484 ',\n",
       " '101 ',\n",
       " ' ',\n",
       " '20000 ',\n",
       " '6650 ',\n",
       " '2565 ',\n",
       " '2458285 ',\n",
       " '299285 ',\n",
       " '340 ',\n",
       " '68040 ',\n",
       " ' ',\n",
       " '122 ',\n",
       " '178080 ',\n",
       " '50672 ',\n",
       " '640 ',\n",
       " '9000 ',\n",
       " '10104 ',\n",
       " '256932 ',\n",
       " '640 ',\n",
       " '191779 ',\n",
       " '4000000 ',\n",
       " ' ',\n",
       " '10000 ',\n",
       " '989818 ',\n",
       " '129000 ',\n",
       " ' ',\n",
       " '100000 ',\n",
       " '21578 ',\n",
       " '463 ',\n",
       " '600 ',\n",
       " '332 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '690 ',\n",
       " '1000 ',\n",
       " '270 ',\n",
       " '6435 ',\n",
       " '2310 ',\n",
       " '58000 ',\n",
       " '946 ',\n",
       " '20008 ',\n",
       " '208 ',\n",
       " '528 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1024 ',\n",
       " '10080 ',\n",
       " '50400 ',\n",
       " '1025010 ',\n",
       " '19020 ',\n",
       " '1364 ',\n",
       " '961 ',\n",
       " '517 ',\n",
       " '200 ',\n",
       " '8000000 ',\n",
       " '1030 ',\n",
       " '606 ',\n",
       " '900 ',\n",
       " '2600 ',\n",
       " '1950 ',\n",
       " '13500 ',\n",
       " '4400 ',\n",
       " '2536 ',\n",
       " '300 ',\n",
       " '197 ',\n",
       " '2858 ',\n",
       " '748 ',\n",
       " '11640 ',\n",
       " '1593 ',\n",
       " '1567 ',\n",
       " '22632 ',\n",
       " '360 ',\n",
       " '103 ',\n",
       " '1994 ',\n",
       " '120 ',\n",
       " '4898 ',\n",
       " '2396130 ',\n",
       " '16772 ',\n",
       " '5875 ',\n",
       " '503 ',\n",
       " '51 ',\n",
       " '106 ',\n",
       " '2126 ',\n",
       " '5456 ',\n",
       " '8800 ',\n",
       " '164860 ',\n",
       " ' ',\n",
       " '1941 ',\n",
       " '130065 ',\n",
       " '515345 ',\n",
       " '440 ',\n",
       " ' ',\n",
       " '53500 ',\n",
       " '8235 ',\n",
       " ' ',\n",
       " '5749132 ',\n",
       " '2215 ',\n",
       " '310 ',\n",
       " '10000 ',\n",
       " '3000 ',\n",
       " '1500 ',\n",
       " '30000 ',\n",
       " '2500 ',\n",
       " '4143 ',\n",
       " '64 ',\n",
       " '53414 ',\n",
       " '65554 ',\n",
       " '45211 ',\n",
       " '1138562 ',\n",
       " '13910 ',\n",
       " '583 ',\n",
       " '2551 ',\n",
       " '34465 ',\n",
       " '5574 ',\n",
       " '245057 ',\n",
       " '182 ',\n",
       " '3850505 ',\n",
       " '138 ',\n",
       " '1080 ',\n",
       " '2075259 ',\n",
       " '210 ',\n",
       " '115 ',\n",
       " '3960456 ',\n",
       " ' ',\n",
       " '10299 ',\n",
       " '1600 ',\n",
       " '768 ',\n",
       " '308 ',\n",
       " '100 ',\n",
       " '237 ',\n",
       " '434874 ',\n",
       " '536 ',\n",
       " '140000 ',\n",
       " '6118 ',\n",
       " '165632 ',\n",
       " '18000 ',\n",
       " '540 ',\n",
       " '931 ',\n",
       " '1055 ',\n",
       " '100 ',\n",
       " '9120 ',\n",
       " '403 ',\n",
       " '111740 ',\n",
       " '10421 ',\n",
       " '5820 ',\n",
       " '403 ',\n",
       " '14980 ',\n",
       " '45730 ',\n",
       " '2584 ',\n",
       " '1372 ',\n",
       " '306 ',\n",
       " '120000 ',\n",
       " '13910 ',\n",
       " '2747 ',\n",
       " '3395 ',\n",
       " '39242 ',\n",
       " '4137 ',\n",
       " '17389 ',\n",
       " '51 ',\n",
       " '470 ',\n",
       " '132 ',\n",
       " '5000000 ',\n",
       " '11000000 ',\n",
       " '250 ',\n",
       " '126 ',\n",
       " ' ',\n",
       " '4889 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '340 ',\n",
       " '501 ',\n",
       " '45781 ',\n",
       " '1503 ',\n",
       " '440 ',\n",
       " '2000 ',\n",
       " '9568 ',\n",
       " '168 ',\n",
       " '100000 ',\n",
       " '5665 ',\n",
       " '79 ',\n",
       " '127 ',\n",
       " '1040 ',\n",
       " '9900 ',\n",
       " '560 ',\n",
       " '60021 ',\n",
       " '1419 ',\n",
       " '101 ',\n",
       " '399 ',\n",
       " '58 ',\n",
       " '180 ',\n",
       " '21048 ',\n",
       " ' ',\n",
       " '750 ',\n",
       " '3000 ',\n",
       " '150 ',\n",
       " '1059 ',\n",
       " '11934 ',\n",
       " '27965 ',\n",
       " '216 ',\n",
       " '120 ',\n",
       " '649 ',\n",
       " '370 ',\n",
       " '4178504 ',\n",
       " '221579 ',\n",
       " '10800 ',\n",
       " '58509 ',\n",
       " '129685 ',\n",
       " '2456 ',\n",
       " '2921 ',\n",
       " '1151 ',\n",
       " '6590 ',\n",
       " '3000 ',\n",
       " '39797 ',\n",
       " '326 ',\n",
       " '913 ',\n",
       " '168286 ',\n",
       " '400 ',\n",
       " '314080 ',\n",
       " '637 ',\n",
       " '1710671 ',\n",
       " '12000 ',\n",
       " '10929 ',\n",
       " '1080 ',\n",
       " '40000 ',\n",
       " '43930257 ',\n",
       " '230318 ',\n",
       " '10500000 ',\n",
       " '13197 ',\n",
       " ' ',\n",
       " '30000 ',\n",
       " '324 ',\n",
       " '541909 ',\n",
       " '11164866 ',\n",
       " '163 ',\n",
       " '373 ',\n",
       " '20560 ',\n",
       " '40 ',\n",
       " '422937 ',\n",
       " '9358 ',\n",
       " '640 ',\n",
       " '919438 ',\n",
       " '40949 ',\n",
       " '5744 ',\n",
       " '10503 ',\n",
       " '42240 ',\n",
       " '102944 ',\n",
       " '500 ',\n",
       " '9782222 ',\n",
       " '11463 ',\n",
       " '17898 ',\n",
       " '1885 ',\n",
       " '19735 ',\n",
       " '1540 ',\n",
       " '4007 ',\n",
       " '153540 ',\n",
       " '606 ',\n",
       " '1353 ',\n",
       " '1956 ',\n",
       " '43824 ',\n",
       " '3942 ',\n",
       " '858 ',\n",
       " '287 ',\n",
       " '17764280 ',\n",
       " '106574 ',\n",
       " '9358 ',\n",
       " '11500 ',\n",
       " '92000 ',\n",
       " '315 ',\n",
       " '78095 ',\n",
       " '130 ',\n",
       " '74 ',\n",
       " '52854 ',\n",
       " '77 ',\n",
       " '811 ',\n",
       " '504 ',\n",
       " '401 ',\n",
       " '2856 ',\n",
       " '10546 ',\n",
       " '801 ',\n",
       " '1540 ',\n",
       " '1451 ',\n",
       " '1075 ',\n",
       " '78095 ',\n",
       " '7195 ',\n",
       " '3600 ',\n",
       " '76 ',\n",
       " '60 ',\n",
       " '405 ',\n",
       " '303 ',\n",
       " '303 ',\n",
       " '107888 ',\n",
       " '76000 ',\n",
       " '10000 ',\n",
       " '180 ',\n",
       " '745000 ',\n",
       " '12234 ',\n",
       " '292 ',\n",
       " '104 ',\n",
       " '60000 ',\n",
       " '2000 ',\n",
       " '165 ',\n",
       " '217 ',\n",
       " '1175 ',\n",
       " '704 ',\n",
       " '75128 ',\n",
       " '90 ',\n",
       " '90 ',\n",
       " '50 ',\n",
       " '71 ',\n",
       " '93239 ',\n",
       " '540 ',\n",
       " '105 ',\n",
       " '6611 ',\n",
       " '15 ',\n",
       " '372 ',\n",
       " '58000 ',\n",
       " '4960 ',\n",
       " '241600 ',\n",
       " '130000 ',\n",
       " '7062606 ',\n",
       " '740 ',\n",
       " '70 ',\n",
       " '2205 ',\n",
       " '10721 ',\n",
       " '640 ',\n",
       " '1000 ',\n",
       " '116 ',\n",
       " '1672 ',\n",
       " '322 ',\n",
       " '93600 ',\n",
       " '3060 ',\n",
       " '5879 ',\n",
       " '9200 ',\n",
       " '20000 ',\n",
       " '20867 ',\n",
       " ' ',\n",
       " '4143 ',\n",
       " '215063 ',\n",
       " '6000000 ',\n",
       " '21263 ',\n",
       " '63000000 ',\n",
       " '10190 ',\n",
       " '300 ',\n",
       " '12330 ',\n",
       " '5180 ',\n",
       " '756 ',\n",
       " '10000 ',\n",
       " '80 ',\n",
       " '1184 ',\n",
       " '1047 ',\n",
       " '777 ',\n",
       " '249 ',\n",
       " '414 ',\n",
       " ' ',\n",
       " '143 ',\n",
       " '7840 ',\n",
       " '30000 ',\n",
       " '35717 ',\n",
       " '135 ',\n",
       " '980 ',\n",
       " '5456 ',\n",
       " '120 ',\n",
       " '4095000 ',\n",
       " '7051 ',\n",
       " '240 ',\n",
       " '48204 ',\n",
       " '260000 ',\n",
       " '288000 ',\n",
       " '8300000 ',\n",
       " '125 ',\n",
       " '170 ',\n",
       " '141712 ',\n",
       " '3916 ',\n",
       " '6262 ',\n",
       " '420768 ',\n",
       " '1067371 ',\n",
       " '1385 ',\n",
       " '908 ',\n",
       " '546 ',\n",
       " '13956534 ',\n",
       " '15630426 ',\n",
       " '8992 ',\n",
       " '1687 ',\n",
       " '779 ',\n",
       " '1056 ',\n",
       " '590 ',\n",
       " '21643 ',\n",
       " '7750 ',\n",
       " '14057567 ',\n",
       " '27170754 ',\n",
       " '597 ',\n",
       " '329 ',\n",
       " '299 ',\n",
       " '20000 ',\n",
       " '26136 ',\n",
       " '1000 ',\n",
       " '399 ',\n",
       " '24017 ',\n",
       " '325834 ',\n",
       " '2916697 ',\n",
       " '22470 ',\n",
       " '189 ',\n",
       " '520 ',\n",
       " '826 ',\n",
       " '2279 ',\n",
       " '28764 ',\n",
       " '7107 ',\n",
       " '288000 ',\n",
       " '9800 ',\n",
       " '4760 ',\n",
       " '72 ',\n",
       " '1450 ',\n",
       " '170 ',\n",
       " '1984 ',\n",
       " '2955 ',\n",
       " '65532 ',\n",
       " '65919 ',\n",
       " '2111 ',\n",
       " '3810 ',\n",
       " '18 ',\n",
       " '244 ',\n",
       " '104 ',\n",
       " '139 ',\n",
       " '360177 ',\n",
       " '36733 ',\n",
       " '4480 ',\n",
       " '165474 ',\n",
       " '1985 ',\n",
       " '10000 ',\n",
       " '232 ',\n",
       " '150 ',\n",
       " '11 ',\n",
       " '14057567 ',\n",
       " '8760 ',\n",
       " '48 ',\n",
       " '6321 ',\n",
       " '3150 ',\n",
       " '17256 ',\n",
       " '187 ',\n",
       " '399 ',\n",
       " '14 ',\n",
       " '467 ',\n",
       " '597 ',\n",
       " '23700 ',\n",
       " '615 ',\n",
       " '6819 ',\n",
       " '1000 ',\n",
       " ' ',\n",
       " '2633 ',\n",
       " '200 ',\n",
       " '13028 ',\n",
       " '800 ',\n",
       " '1700 ',\n",
       " '521 ',\n",
       " '120000 ',\n",
       " '666 ',\n",
       " '88 ',\n",
       " '200 ',\n",
       " '2279 ',\n",
       " '23570 ',\n",
       " '10000 ',\n",
       " '37700 ',\n",
       " '1850 ',\n",
       " '11 ',\n",
       " '147270 ',\n",
       " '3150 ',\n",
       " '17256 ',\n",
       " '597 ',\n",
       " '7624 ',\n",
       " '314 ',\n",
       " '1197 ',\n",
       " '70 ',\n",
       " '7624 ',\n",
       " '26737 ',\n",
       " '10000 ',\n",
       " '13611 ',\n",
       " '12684 ',\n",
       " '48 ',\n",
       " '731 ',\n",
       " '731 ',\n",
       " '557 ']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Num_instances\n",
    "d5=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]/p\")\n",
    "for i in d5[1:]:\n",
    "    try:\n",
    "        Num_instances.append(i.text)\n",
    "    except:\n",
    "        Num_instances.append(\"-\")\n",
    "Num_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Num_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8 ',\n",
       " '14 ',\n",
       " '38 ',\n",
       " '294 ',\n",
       " '279 ',\n",
       " '7 ',\n",
       " ' ',\n",
       " '69 ',\n",
       " '8 ',\n",
       " '26 ',\n",
       " '1 ',\n",
       " '4 ',\n",
       " '4 ',\n",
       " '9 ',\n",
       " '10 ',\n",
       " '34 ',\n",
       " '32 ',\n",
       " '13 ',\n",
       " '6 ',\n",
       " '14 ',\n",
       " '22 ',\n",
       " '36 ',\n",
       " '6 ',\n",
       " ' ',\n",
       " '6 ',\n",
       " '42 ',\n",
       " '15 ',\n",
       " ' ',\n",
       " '9 ',\n",
       " '9 ',\n",
       " '54 ',\n",
       " '39 ',\n",
       " '33 ',\n",
       " '20 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '12 ',\n",
       " '8 ',\n",
       " '30 ',\n",
       " ' ',\n",
       " '10 ',\n",
       " '3 ',\n",
       " '5 ',\n",
       " '75 ',\n",
       " '19 ',\n",
       " '27 ',\n",
       " ' ',\n",
       " '19 ',\n",
       " '1558 ',\n",
       " '34 ',\n",
       " '4 ',\n",
       " '617 ',\n",
       " '12 ',\n",
       " '16 ',\n",
       " '7 ',\n",
       " '4 ',\n",
       " '16 ',\n",
       " '7 ',\n",
       " ' ',\n",
       " '56 ',\n",
       " '18 ',\n",
       " '8 ',\n",
       " '22 ',\n",
       " ' ',\n",
       " '58 ',\n",
       " ' ',\n",
       " '61 ',\n",
       " '7 ',\n",
       " ' ',\n",
       " '649 ',\n",
       " '22 ',\n",
       " '168 ',\n",
       " '168 ',\n",
       " '8 ',\n",
       " ' ',\n",
       " '10 ',\n",
       " '64 ',\n",
       " '16 ',\n",
       " '8 ',\n",
       " '17 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '72 ',\n",
       " '4 ',\n",
       " '6 ',\n",
       " '10 ',\n",
       " '35 ',\n",
       " '35 ',\n",
       " '4 ',\n",
       " '102 ',\n",
       " '57 ',\n",
       " '22 ',\n",
       " '44 ',\n",
       " '45 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '5 ',\n",
       " '9 ',\n",
       " '21 ',\n",
       " '32 ',\n",
       " '17 ',\n",
       " '16 ',\n",
       " '38 ',\n",
       " '21 ',\n",
       " '40 ',\n",
       " '13 ',\n",
       " '8 ',\n",
       " '17 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '15 ',\n",
       " '22 ',\n",
       " '68 ',\n",
       " '40 ',\n",
       " '17 ',\n",
       " '89 ',\n",
       " ' ',\n",
       " '4 ',\n",
       " '12 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '86 ',\n",
       " '72 ',\n",
       " '61 ',\n",
       " '12 ',\n",
       " '481 ',\n",
       " '42 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '5 ',\n",
       " '90 ',\n",
       " ' ',\n",
       " '5 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '14 ',\n",
       " '20 ',\n",
       " '13 ',\n",
       " '36 ',\n",
       " '19 ',\n",
       " '9 ',\n",
       " '18 ',\n",
       " '4 ',\n",
       " '60 ',\n",
       " '10 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '10 ',\n",
       " '4 ',\n",
       " '3 ',\n",
       " '11 ',\n",
       " '11 ',\n",
       " ' ',\n",
       " '6 ',\n",
       " '13 ',\n",
       " ' ',\n",
       " '100000 ',\n",
       " '9 ',\n",
       " '101 ',\n",
       " '10000 ',\n",
       " '20000 ',\n",
       " '100000 ',\n",
       " '5000 ',\n",
       " '500 ',\n",
       " '73 ',\n",
       " '43 ',\n",
       " '23 ',\n",
       " '3 ',\n",
       " '5 ',\n",
       " ' ',\n",
       " '256 ',\n",
       " '591 ',\n",
       " '70 ',\n",
       " '91 ',\n",
       " '10 ',\n",
       " '128 ',\n",
       " '6 ',\n",
       " '12 ',\n",
       " '3231961 ',\n",
       " '5409 ',\n",
       " '26 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '10 ',\n",
       " '23 ',\n",
       " '24 ',\n",
       " '13 ',\n",
       " '8 ',\n",
       " ' ',\n",
       " '27 ',\n",
       " '50 ',\n",
       " '90 ',\n",
       " '138672 ',\n",
       " ' ',\n",
       " '386 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '12 ',\n",
       " '147 ',\n",
       " '6 ',\n",
       " '8 ',\n",
       " '27 ',\n",
       " '10000 ',\n",
       " '20000 ',\n",
       " '10000 ',\n",
       " '54877 ',\n",
       " '4702 ',\n",
       " '24 ',\n",
       " '29 ',\n",
       " '17 ',\n",
       " '3 ',\n",
       " '128 ',\n",
       " '10 ',\n",
       " '242 ',\n",
       " '120 ',\n",
       " ' ',\n",
       " '4 ',\n",
       " '13 ',\n",
       " '52 ',\n",
       " '47 ',\n",
       " '857 ',\n",
       " '9 ',\n",
       " '7 ',\n",
       " '200 ',\n",
       " '4 ',\n",
       " ' ',\n",
       " '561 ',\n",
       " '64 ',\n",
       " '8 ',\n",
       " '7 ',\n",
       " '10 ',\n",
       " '9 ',\n",
       " '4 ',\n",
       " '8 ',\n",
       " '77 ',\n",
       " '51 ',\n",
       " '18 ',\n",
       " '1950000 ',\n",
       " '18 ',\n",
       " '1300 ',\n",
       " '41 ',\n",
       " '6 ',\n",
       " '5625 ',\n",
       " '5 ',\n",
       " ' ',\n",
       " '7 ',\n",
       " '33 ',\n",
       " '5 ',\n",
       " '15 ',\n",
       " '9 ',\n",
       " '19 ',\n",
       " '5 ',\n",
       " '5 ',\n",
       " '1000000 ',\n",
       " '129 ',\n",
       " ' ',\n",
       " '20 ',\n",
       " '152 ',\n",
       " '24 ',\n",
       " '16 ',\n",
       " '35 ',\n",
       " '17 ',\n",
       " '5 ',\n",
       " '18 ',\n",
       " '28 ',\n",
       " '7 ',\n",
       " '309 ',\n",
       " '3 ',\n",
       " '6 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '16 ',\n",
       " '13 ',\n",
       " '5 ',\n",
       " '6 ',\n",
       " '8 ',\n",
       " '2 ',\n",
       " '4 ',\n",
       " '148 ',\n",
       " '55 ',\n",
       " '17 ',\n",
       " '8 ',\n",
       " '42 ',\n",
       " '26 ',\n",
       " '50 ',\n",
       " '2 ',\n",
       " '281 ',\n",
       " '120 ',\n",
       " ' ',\n",
       " '6 ',\n",
       " '120432 ',\n",
       " '150000 ',\n",
       " '529 ',\n",
       " ' ',\n",
       " '16 ',\n",
       " '2500 ',\n",
       " '5 ',\n",
       " '68 ',\n",
       " '16 ',\n",
       " '100 ',\n",
       " '216 ',\n",
       " '23 ',\n",
       " '33 ',\n",
       " '140256 ',\n",
       " '19 ',\n",
       " '20 ',\n",
       " '20 ',\n",
       " '49 ',\n",
       " '12 ',\n",
       " '30 ',\n",
       " '5232 ',\n",
       " '20 ',\n",
       " '1 ',\n",
       " ' ',\n",
       " '61 ',\n",
       " '27 ',\n",
       " '53 ',\n",
       " '11 ',\n",
       " '25 ',\n",
       " '0 ',\n",
       " '20 ',\n",
       " '9 ',\n",
       " '3 ',\n",
       " '561 ',\n",
       " '82 ',\n",
       " '13 ',\n",
       " '16 ',\n",
       " '13 ',\n",
       " '28 ',\n",
       " '4 ',\n",
       " ' ',\n",
       " '24 ',\n",
       " '34 ',\n",
       " '8 ',\n",
       " '128 ',\n",
       " '15 ',\n",
       " '513 ',\n",
       " '7 ',\n",
       " '7 ',\n",
       " '5 ',\n",
       " '15 ',\n",
       " '480000 ',\n",
       " '11 ',\n",
       " '54 ',\n",
       " '561 ',\n",
       " '64 ',\n",
       " '6 ',\n",
       " '116 ',\n",
       " '19 ',\n",
       " ' ',\n",
       " '5812 ',\n",
       " '9 ',\n",
       " '32 ',\n",
       " '29 ',\n",
       " '67 ',\n",
       " ' ',\n",
       " '25 ',\n",
       " '6400 ',\n",
       " '10 ',\n",
       " '5 ',\n",
       " '13 ',\n",
       " '98 ',\n",
       " '36 ',\n",
       " '69 ',\n",
       " '2158859 ',\n",
       " '518 ',\n",
       " '15 ',\n",
       " '179 ',\n",
       " ' ',\n",
       " '12 ',\n",
       " '38 ',\n",
       " '65 ',\n",
       " '102 ',\n",
       " '86 ',\n",
       " '7 ',\n",
       " '53 ',\n",
       " '20 ',\n",
       " '1 ',\n",
       " '71 ',\n",
       " '29 ',\n",
       " '20531 ',\n",
       " '65 ',\n",
       " '3 ',\n",
       " '22 ',\n",
       " '38 ',\n",
       " '22 ',\n",
       " '4814 ',\n",
       " '698 ',\n",
       " '13 ',\n",
       " '10 ',\n",
       " '59 ',\n",
       " '56 ',\n",
       " '482 ',\n",
       " '171 ',\n",
       " '5 ',\n",
       " '500 ',\n",
       " '411 ',\n",
       " '8519 ',\n",
       " '21 ',\n",
       " '21 ',\n",
       " '171 ',\n",
       " '7 ',\n",
       " '49 ',\n",
       " '12 ',\n",
       " '3 ',\n",
       " '21 ',\n",
       " '9 ',\n",
       " '8 ',\n",
       " '7 ',\n",
       " '2 ',\n",
       " '11 ',\n",
       " '11 ',\n",
       " '173 ',\n",
       " '5 ',\n",
       " '15 ',\n",
       " '3 ',\n",
       " '105 ',\n",
       " '25000 ',\n",
       " ' ',\n",
       " '18 ',\n",
       " '21000 ',\n",
       " '115 ',\n",
       " '21 ',\n",
       " '206 ',\n",
       " '43680 ',\n",
       " '8 ',\n",
       " '10 ',\n",
       " '59 ',\n",
       " '10 ',\n",
       " '5 ',\n",
       " '5 ',\n",
       " '1000 ',\n",
       " '138 ',\n",
       " ' ',\n",
       " '16 ',\n",
       " '2 ',\n",
       " '10 ',\n",
       " ' ',\n",
       " '8 ',\n",
       " '6 ',\n",
       " '129 ',\n",
       " '81 ',\n",
       " '12 ',\n",
       " '6 ',\n",
       " '22 ',\n",
       " '18 ',\n",
       " '9 ',\n",
       " '754 ',\n",
       " '14 ',\n",
       " '5 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '18 ',\n",
       " '7 ',\n",
       " '7 ',\n",
       " ' ',\n",
       " '7 ',\n",
       " '5 ',\n",
       " '6 ',\n",
       " '4 ',\n",
       " '18 ',\n",
       " '11 ',\n",
       " '25 ',\n",
       " ' ',\n",
       " '20 ',\n",
       " '12 ',\n",
       " '46 ',\n",
       " '9 ',\n",
       " '8 ',\n",
       " '49 ',\n",
       " '11 ',\n",
       " '8 ',\n",
       " '54 ',\n",
       " '36 ',\n",
       " '3916 ',\n",
       " '710 ',\n",
       " '18 ',\n",
       " '8 ',\n",
       " '29 ',\n",
       " '7 ',\n",
       " '9 ',\n",
       " '37 ',\n",
       " '6 ',\n",
       " '1024 ',\n",
       " '1024 ',\n",
       " '14 ',\n",
       " '7 ',\n",
       " '8265 ',\n",
       " '29 ',\n",
       " '25 ',\n",
       " '3 ',\n",
       " '115 ',\n",
       " '1 ',\n",
       " '12 ',\n",
       " '13 ',\n",
       " '200000 ',\n",
       " '6 ',\n",
       " '21 ',\n",
       " '4 ',\n",
       " '2400 ',\n",
       " '175 ',\n",
       " '10 ',\n",
       " '4714 ',\n",
       " '23 ',\n",
       " '17 ',\n",
       " '2 ',\n",
       " '9 ',\n",
       " '8 ',\n",
       " '280 ',\n",
       " '49 ',\n",
       " '3 ',\n",
       " '14 ',\n",
       " '19 ',\n",
       " ' ',\n",
       " '54 ',\n",
       " '8 ',\n",
       " '1087 ',\n",
       " '12 ',\n",
       " '3 ',\n",
       " '17 ',\n",
       " '8 ',\n",
       " '9 ',\n",
       " '12 ',\n",
       " '1656 ',\n",
       " '6 ',\n",
       " '2 ',\n",
       " '11 ',\n",
       " '533 ',\n",
       " '14 ',\n",
       " '84 ',\n",
       " '22 ',\n",
       " '16 ',\n",
       " '52 ',\n",
       " '19 ',\n",
       " '3 ',\n",
       " '14 ',\n",
       " '321 ',\n",
       " '13 ',\n",
       " '13 ',\n",
       " '55 ',\n",
       " '39 ',\n",
       " '4 ',\n",
       " '7 ',\n",
       " '79 ',\n",
       " '1 ',\n",
       " ' ',\n",
       " '14 ',\n",
       " '96 ',\n",
       " '21 ',\n",
       " ' ',\n",
       " '5 ',\n",
       " '2 ',\n",
       " '69 ',\n",
       " '9 ',\n",
       " '124 ',\n",
       " '20 ',\n",
       " '25 ',\n",
       " '11 ',\n",
       " '19 ',\n",
       " '9 ',\n",
       " '9 ',\n",
       " '5 ',\n",
       " '7 ',\n",
       " '4006 ',\n",
       " '2 ',\n",
       " '19 ',\n",
       " '4 ',\n",
       " '13 ',\n",
       " '55 ',\n",
       " '1 ',\n",
       " '7842 ',\n",
       " '15 ',\n",
       " '15 ',\n",
       " '70 ',\n",
       " '7842 ',\n",
       " '4 ',\n",
       " '14 ',\n",
       " '17 ',\n",
       " '23 ',\n",
       " '321 ',\n",
       " '1068 ',\n",
       " '1068 ',\n",
       " '5 ']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Num_attribute\n",
    "d6=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]/p\")\n",
    "for i in d6[1:]:\n",
    "    try:\n",
    "        Num_attribute.append(i.text)\n",
    "    except:\n",
    "        Num_attribute.append(\"-\")\n",
    "Num_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Num_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1995 ',\n",
       " '1996 ',\n",
       " ' ',\n",
       " '1998 ',\n",
       " '1998 ',\n",
       " '1992 ',\n",
       " '1987 ',\n",
       " '1992 ',\n",
       " '1993 ',\n",
       " '1987 ',\n",
       " '1994 ',\n",
       " '1994 ',\n",
       " ' ',\n",
       " '1988 ',\n",
       " '1992 ',\n",
       " '1995 ',\n",
       " '1995 ',\n",
       " '1990 ',\n",
       " '1997 ',\n",
       " '1996 ',\n",
       " '1988 ',\n",
       " '1989 ',\n",
       " '1994 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1995 ',\n",
       " ' ',\n",
       " '1992 ',\n",
       " '1987 ',\n",
       " '1997 ',\n",
       " '1998 ',\n",
       " '1995 ',\n",
       " '1998 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1994 ',\n",
       " ' ',\n",
       " '1989 ',\n",
       " '1996 ',\n",
       " '1990 ',\n",
       " '1990 ',\n",
       " '1987 ',\n",
       " '1999 ',\n",
       " '1989 ',\n",
       " '1988 ',\n",
       " '1988 ',\n",
       " '1989 ',\n",
       " ' ',\n",
       " '1990 ',\n",
       " '1998 ',\n",
       " '1989 ',\n",
       " '1988 ',\n",
       " '1994 ',\n",
       " '1990 ',\n",
       " '1988 ',\n",
       " '1988 ',\n",
       " '1990 ',\n",
       " '1991 ',\n",
       " '1990 ',\n",
       " ' ',\n",
       " '1992 ',\n",
       " '1988 ',\n",
       " '1990 ',\n",
       " '1996 ',\n",
       " '1995 ',\n",
       " '1990 ',\n",
       " ' ',\n",
       " '1992 ',\n",
       " '1992 ',\n",
       " '1994 ',\n",
       " ' ',\n",
       " '1987 ',\n",
       " '1994 ',\n",
       " '1994 ',\n",
       " '1997 ',\n",
       " '1991 ',\n",
       " '1995 ',\n",
       " '1998 ',\n",
       " '1998 ',\n",
       " '1993 ',\n",
       " '1988 ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1992 ',\n",
       " '1993 ',\n",
       " '1988 ',\n",
       " '1989 ',\n",
       " '1988 ',\n",
       " '1987 ',\n",
       " '1993 ',\n",
       " '1988 ',\n",
       " '1999 ',\n",
       " '2001 ',\n",
       " '2001 ',\n",
       " ' ',\n",
       " '1992 ',\n",
       " '1993 ',\n",
       " '1997 ',\n",
       " '1991 ',\n",
       " '1987 ',\n",
       " '1994 ',\n",
       " '1988 ',\n",
       " '1987 ',\n",
       " '1993 ',\n",
       " '1988 ',\n",
       " '1988 ',\n",
       " '1991 ',\n",
       " '1996 ',\n",
       " '1990 ',\n",
       " ' ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " '2002 ',\n",
       " ' ',\n",
       " '2000 ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " '2001 ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " '2000 ',\n",
       " '1999 ',\n",
       " '2000 ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " ' ',\n",
       " '1998 ',\n",
       " '1999 ',\n",
       " '2001 ',\n",
       " '1999 ',\n",
       " ' ',\n",
       " '2003 ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " '1997 ',\n",
       " '1999 ',\n",
       " '1999 ',\n",
       " '1998 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1994 ',\n",
       " ' ',\n",
       " '1993 ',\n",
       " '1990 ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '1989 ',\n",
       " '2006 ',\n",
       " '2006 ',\n",
       " '2007 ',\n",
       " '2007 ',\n",
       " '2007 ',\n",
       " '2007 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2007 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2009 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2008 ',\n",
       " '2009 ',\n",
       " '2009 ',\n",
       " '2009 ',\n",
       " '2009 ',\n",
       " '2009 ',\n",
       " '2009 ',\n",
       " '2010 ',\n",
       " '2009 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2010 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2011 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2012 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2014 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2013 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2013 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2015 ',\n",
       " '2014 ',\n",
       " '2014 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2014 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2015 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2015 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2015 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2016 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2017 ',\n",
       " '2016 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2016 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2019 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2019 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2021 ',\n",
       " '2018 ',\n",
       " '2018 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2019 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2020 ',\n",
       " '2021 ',\n",
       " '2021 ',\n",
       " '2021 ']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting information for Year_info\n",
    "d7=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p\")\n",
    "for i in d7[1:]:\n",
    "    try:\n",
    "        Year_info.append(i.text)\n",
    "    except:\n",
    "        Year_info.append(\"-\")\n",
    "Year_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Year_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute type</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>452</td>\n",
       "      <td>279</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>in-vehicle coupon recommendation</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td></td>\n",
       "      <td>12684</td>\n",
       "      <td>23</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Gait Classification</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>48</td>\n",
       "      <td>321</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Wikipedia Math Essentials</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>731</td>\n",
       "      <td>1068</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Wikipedia Math Essentials</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>731</td>\n",
       "      <td>1068</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Synchronous Machine Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>557</td>\n",
       "      <td>5</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dataset Name      Data type                  Task  \\\n",
       "0                             Abalone  Multivariate        Classification    \n",
       "1                               Adult  Multivariate        Classification    \n",
       "2                           Annealing  Multivariate        Classification    \n",
       "3        Anonymous Microsoft Web Data                 Recommender-Systems    \n",
       "4                          Arrhythmia  Multivariate        Classification    \n",
       "..                                ...            ...                   ...   \n",
       "583  in-vehicle coupon recommendation  Multivariate        Classification    \n",
       "584               Gait Classification  Multivariate        Classification    \n",
       "585         Wikipedia Math Essentials   Time-Series            Regression    \n",
       "586         Wikipedia Math Essentials   Time-Series            Regression    \n",
       "587      Synchronous Machine Data Set  Multivariate            Regression    \n",
       "\n",
       "                  Attribute type No of instances No of attribute   Year  \n",
       "0    Categorical, Integer, Real            4177               8   1995   \n",
       "1          Categorical, Integer           48842              14   1996   \n",
       "2    Categorical, Integer, Real             798              38          \n",
       "3                   Categorical           37711             294   1998   \n",
       "4    Categorical, Integer, Real             452             279   1998   \n",
       "..                           ...             ...             ...    ...  \n",
       "583                                       12684              23   2020   \n",
       "584                        Real              48             321   2020   \n",
       "585                        Real             731            1068   2021   \n",
       "586                        Real             731            1068   2021   \n",
       "587                        Real             557               5   2021   \n",
       "\n",
       "[588 rows x 7 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving our answer in a DataFrame using Pandas\n",
    "UCI_ds=pd.DataFrame({})\n",
    "UCI_ds['Dataset Name']=Dataset_Name\n",
    "UCI_ds['Data type']=Data_type\n",
    "UCI_ds['Task']=Task_info\n",
    "UCI_ds['Attribute type']=Attribute_type\n",
    "UCI_ds['No of instances']=Num_instances\n",
    "UCI_ds['No of attribute']=Num_attribute\n",
    "UCI_ds['Year']=Year_info\n",
    "UCI_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************************************Completed*****************************************************************************************************************************************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ||Happy Ending||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
